var documenterSearchIndex = {"docs":
[{"location":"api.html#API","page":"API","title":"API","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"This is a reference for advanced users or those who wish to contribute.","category":"page"},{"location":"api.html","page":"API","title":"API","text":"","category":"page"},{"location":"api.html#CausalELM","page":"API","title":"CausalELM","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"CausalELM","category":"page"},{"location":"api.html#CausalELM","page":"API","title":"CausalELM","text":"Macros, functions, and structs for applying Extreme Learning Machines to causal inference tasks where the counterfactual is unavailable or biased and must be predicted. Provides  macros for event study designs, parametric G-computation, doubly robust machine learning, and  metalearners. Additionally, these tasks can be performed with or without L2 penalization and will automatically choose the best number of neurons and L2 penalty. \n\nFor more details on Extreme Learning Machines see:     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\n\n\n\n\n","category":"module"},{"location":"api.html#Activation-Functions","page":"API","title":"Activation Functions","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"CausalELM.ActivationFunctions\r\nCausalELM.ActivationFunctions.binarystep\r\nCausalELM.ActivationFunctions.σ\r\nCausalELM.ActivationFunctions.tanh\r\nCausalELM.ActivationFunctions.relu\r\nCausalELM.ActivationFunctions.leakyrelu\r\nCausalELM.ActivationFunctions.swish\r\nCausalELM.ActivationFunctions.softmax\r\nCausalELM.ActivationFunctions.softplus\r\nCausalELM.ActivationFunctions.gelu\r\nCausalELM.ActivationFunctions.gaussian\r\nCausalELM.ActivationFunctions.hardtanh\r\nCausalELM.ActivationFunctions.elish\r\nCausalELM.ActivationFunctions.fourier","category":"page"},{"location":"api.html#CausalELM.ActivationFunctions","page":"API","title":"CausalELM.ActivationFunctions","text":"Activation functions for Extreme Learning machines\n\n\n\n\n\n","category":"module"},{"location":"api.html#CausalELM.ActivationFunctions.binarystep","page":"API","title":"CausalELM.ActivationFunctions.binarystep","text":"binarystep(x)\n\nApply the binary step activation function to a real number.\n\nExamples\n\njulia> binarystep(1)\n1\n\n\n\n\n\nbinarystep(x)\n\nApply the binary step activation function to an array.\n\nExamples\n\njulia> binarystep([-1000, 100, 1, 0, -0.001, -3])\n[0, 1, 1, 1, 0, 0]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.σ","page":"API","title":"CausalELM.ActivationFunctions.σ","text":"σ(x)\n\nApply the sigmoid activation function to a real number.\n\nExamples\n\njulia> σ(1)\n0.7310585786300049\n\n\n\n\n\nσ(x)\n\nApply the sigmoid activation function to an array.\n\nExamples\n\njulia> σ([1, 0])\n[0.7310585786300049, 0.5]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.tanh","page":"API","title":"CausalELM.ActivationFunctions.tanh","text":"tanh(x)\n\nApply the tanh activation function to an array.\n\nThis is just a vectorized version of Base.tanh\n\nExamples\n\njulia> tanh([1, 0])\n[0.7615941559557649, 0.0]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.relu","page":"API","title":"CausalELM.ActivationFunctions.relu","text":"relu(x)\n\nApply the ReLU activation function to a real number.\n\nExamples\n\njulia> relu(1)\n1\n\n\n\n\n\nrelu(x)\n\nApply the ReLU activation function to an array.\n\nExamples\n\njulia> relu([1, 0, -1])\n[1, 0, 0]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.leakyrelu","page":"API","title":"CausalELM.ActivationFunctions.leakyrelu","text":"leakyrelu(x)\n\nApply the leaky ReLU activation function to a real number.\n\nExamples\n\njulia> leakyrelu(1)\n1\n\n\n\n\n\nleakyrelu(x)\n\nApply the leaky ReLU activation function to an array.\n\nExamples\n\njulia> leakyrelu([-0.01, 0, 1])\n[1, 0, 0]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.swish","page":"API","title":"CausalELM.ActivationFunctions.swish","text":"swish(x)\n\nApply the swish activation function to a real number.\n\nExamples\n\njulia> swish(1)\n0.7310585786300049\n\n\n\n\n\nswish(x)\n\nApply the swish activation function to an array.\n\nExamples\n\njulia> swish([1, 0, -1])\n[0.7310585786300049, 0, -0.2689414213699951]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.softmax","page":"API","title":"CausalELM.ActivationFunctions.softmax","text":"softmax(x)\n\nApply the softmax activation function to a real number.\n\nFor numbers that have large absolute values this function may become numerically unstable.\n\nExamples\n\njulia> softmax(1)\n2.718281828459045\n\n\n\n\n\nsoftmax(x)\n\nApply the softmax activation function to an array.\n\nFor numbers that have large absolute values this function might be numerically unstable.\n\nExamples\n\njulia> softmax([1, -1])\n[2.718281828459045, -0.36787944117144233]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.softplus","page":"API","title":"CausalELM.ActivationFunctions.softplus","text":"softplus(x)\n\nApply the softplus activation function to a real number.\n\nExamples\n\njulia> softplus(1)\n1.3132616875182228\n\n\n\n\n\nsoftplus(x)\n\nApply the softplus activation function to an array.\n\nExamples\n\njulia> softplus([1, -1])\n[1.3132616875182228, 0.31326168751822286]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.gelu","page":"API","title":"CausalELM.ActivationFunctions.gelu","text":"gelu(x)\n\nApply the GeLU activation function to a real number.\n\nExamples\n\njulia> gelu(1)\n0.8411919906082768\n\n\n\n\n\ngelu(x)\n\nApply the GeLU activation function to an array.\n\nExamples\n\njulia> gelu([-1, 0, 1])\n[-0.15880800939172324, 0, 0.8411919906082768]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.gaussian","page":"API","title":"CausalELM.ActivationFunctions.gaussian","text":"gaussian(x)\n\nApply the gaussian activation function to a real number.\n\nExamples\n\njulia> gaussian(1)\n0.11443511435028261\n\n\n\n\n\ngaussian(x)\n\nApply the gaussian activation function to an array.\n\nExamples\n\njulia> gaussian([1, -1])\n[0.36787944117144233, 0.36787944117144233]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.hardtanh","page":"API","title":"CausalELM.ActivationFunctions.hardtanh","text":"hardtanh(x)\n\nApply the hardtanh activation function to a real number.\n\nExamples\n\njulia> hardtanh(-2)\n-1\n\n\n\n\n\nhardtanh(x)\n\nApply the hardtanh activation function to an array.\n\nExamples\n\njulia> hardtanh([-2, 0, 2])\n[-1, 0, 1]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.elish","page":"API","title":"CausalELM.ActivationFunctions.elish","text":"elish(x)\n\nApply the ELiSH activation function to a real number.\n\nExamples\n\njulia> elish(1)\n0.7310585786300049\n\n\n\n\n\nelish(x)\n\nApply the ELiSH activation function to an array.\n\nExamples\n\njulia> elish([-1, 1])\n[-0.17000340156854793, 0.7310585786300049]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.ActivationFunctions.fourier","page":"API","title":"CausalELM.ActivationFunctions.fourier","text":"fourrier(x)\n\nApply the Fourier activation function to a real number.\n\nExamples\n\njulia> fourier(1)\n0.8414709848078965\n\n\n\n\n\nfourrier(x)\n\nApply the Fourier activation function to an array.\n\nExamples\n\njulia> fourier([-1, 1])\n[-0.8414709848078965, 0.8414709848078965]\n\n\n\n\n\n","category":"function"},{"location":"api.html#Cross-Valdiation","page":"API","title":"Cross Valdiation","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"CausalELM.CrossValidation\r\nCausalELM.CrossValidation.recode\r\nCausalELM.CrossValidation.traintest\r\nCausalELM.CrossValidation.validate\r\nCausalELM.CrossValidation.crossvalidate\r\nCausalELM.CrossValidation.bestsize","category":"page"},{"location":"api.html#CausalELM.CrossValidation","page":"API","title":"CausalELM.CrossValidation","text":"Methods to perform cross validation and find the optimum number of neurons.\n\nTo reduce computation time, the number of neurons is optimized by using cross validation to estimate the validation error on a small subset of the range of possible numbers of  neurons. Then, an Extreme Learning Machine is trained to predict validation loss from  the given cross validation sets. Finally, the number of neurons is selected that has the  smallest predicted loss or the highest classification metric.\n\n\n\n\n\n","category":"module"},{"location":"api.html#CausalELM.CrossValidation.recode","page":"API","title":"CausalELM.CrossValidation.recode","text":"recode(ŷ)\n\nRound predicted values to their predicted class for classification tasks.\n\nIf the smallest predicted label is 0, all labels are shifted up 1; if the smallest  label is -1, all labels are shifted up 2. Also labels cannot be smaller than -1.\n\nExamples\n\njulia> recode([-0.7, 0.2, 1.1])\n3-element Vector{Float64}\n1\n2\n3\njulia> recode([0.1, 0.2, 0.3])\n3-element Vector{Float64}\n1\n1\n1\njulia> recode([1.1, 1.51, 1.8])\n3-element Vector{Float64}\n1\n2\n2\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.CrossValidation.traintest","page":"API","title":"CausalELM.CrossValidation.traintest","text":"traintest(X, Y, folds)\n\nCreate a train-test split.\n\nIf an iteration is specified, the train test split will be treated as time series/panel data.\n\nExamples\n\njulia> xtrain, ytrain, xtest, ytest = traintest(zeros(20, 2), zeros(20), 5)\n\n\n\n\n\ntraintest(X, Y, folds, iteration)\n\nCreate a rolling train-test split for time series/panel data.\n\nAn iteration should not be specified for non-time series/panel data.\n\nExamples\n\njulia> xtrain, ytrain, xtest, ytest = traintest(zeros(20, 2), zeros(20), 5, 1)\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.CrossValidation.validate","page":"API","title":"CausalELM.CrossValidation.validate","text":"validate(X, Y, nodes, metric, iteration...; activation, regularized, folds)\n\nCalculate a validation metric for a single fold in k-fold cross validation.\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> validate(x, y, 5, accuracy, 3)\n0.0\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.CrossValidation.crossvalidate","page":"API","title":"CausalELM.CrossValidation.crossvalidate","text":"crossvalidate(X, Y, neurons, metric, activation, regularized, folds)\n\nCalculate a validation metric for k folds using a single set of hyperparameters.\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> crossvalidate(x, y, 5, accuracy)\n0.0257841765251021\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.CrossValidation.bestsize","page":"API","title":"CausalELM.CrossValidation.bestsize","text":"bestsize(X, Y, metric, task, activation, min_neurons, max_neurons, regularized, folds, temporal, \n    iterations, approximator_neurons)\n\nCompute the best number of neurons for an Extreme Learning Machine.\n\nThe procedure tests networks with numbers of neurons in a sequence whose length is given  by iterations on the interval [minneurons, maxneurons]. Then, it uses the networks  sizes and validation errors from the sequence to predict the validation error or metric  for every network size between minneurons and maxneurons using the function  approximation ability of an Extreme Learning Machine. Finally, it returns the network  size with the best predicted validation error or metric.\n\nExamples\n\njulia> bestsize(rand(100, 5), rand(100), mse, \"regression\")\n11\n\n\n\n\n\n","category":"function"},{"location":"api.html#ATE/ATE/ITT-Estimation","page":"API","title":"ATE/ATE/ITT Estimation","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"CausalELM.Estimators\r\nCausalELM.Estimators.CausalEstimator\r\nCausalELM.Estimators.EventStudy\r\nCausalELM.Estimators.GComputation\r\nCausalELM.Estimators.DoublyRobust\r\nCausalELM.Estimators.estimatecausaleffect!","category":"page"},{"location":"api.html#CausalELM.Estimators","page":"API","title":"CausalELM.Estimators","text":"Estimate causal effects with event study designs, G-computation, and doubly robust  estiamtion using Extreme Learning machines.\n\n\n\n\n\n","category":"module"},{"location":"api.html#CausalELM.Estimators.CausalEstimator","page":"API","title":"CausalELM.Estimators.CausalEstimator","text":"Abstract type for GComputation and DoublyRobust\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.Estimators.EventStudy","page":"API","title":"CausalELM.Estimators.EventStudy","text":"Container for the results of an event study\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.Estimators.GComputation","page":"API","title":"CausalELM.Estimators.GComputation","text":"Container for the results of G-Computation\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.Estimators.DoublyRobust","page":"API","title":"CausalELM.Estimators.DoublyRobust","text":"Container for the results of doubly robust estimation\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.estimatecausaleffect!","page":"API","title":"CausalELM.estimatecausaleffect!","text":"estimatecausaleffect!(study)\n\nEstimate the abnormal returns in an event study.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = EventStudy(X₀, Y₀, X₁, Y₁)\njulia> estimatecausaleffect!(m1)\n0.25714308\n\n\n\n\n\nestimatecausaleffect!(g)\n\nEstimate a causal effect of interest using G-Computation.\n\nIf treatents are administered at multiple time periods, the effect will be estimated as the average difference between the outcome of being treated in all periods and being treated in no periods. For example, given that individuals 1, 2, ..., i ∈ I recieved either a treatment or a placebo in p  different periods, the model would estimate the average treatment effect as  E[Yᵢ|T₁=1, T₂=1, ... Tₚ=1, Xₚ] - E[Yᵢ|T₁=0, T₂=0, ... Tₚ=0, Xₚ].\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = GComputation(X, Y, T)\njulia> estimatecausaleffect!(m1)\n0.31067439\n\n\n\n\n\nestimatecausaleffect!(DRE)\n\nEstimate a causal effect of interest using doubly robust estimation.\n\nUnlike other estimators, this method does not support time series or panel data. This method also  does not work as well with smaller datasets because it estimates separate outcome models for the  treatment and control groups.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = DoublyRobust(X, Y, T)\njulia> estimatecausaleffect!(m1)\n0.31067439\n\n\n\n\n\nestimatecausaleffect!(s)\n\nEstimate the CATE using an S-Learner.\n\nFor an overview of meatlearning, including S-Learners see:\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = SLearner(X, Y, T)\njulia> estimatecausaleffect!(m1)\n[0.20729633391630697, 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630697, 0.20729633391630697, 0.20729633391630703, \n0.20729633391630697, 0.20729633391630697  …  0.20729633391630703, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630703, 0.20729633391630697, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697]\n\n\n\n\n\nestimatecausaleffect!(t)\n\nEstimate the CATE using a T-Learner.\n\nFor an overview of meatlearning, including T-Learners see:\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = TLearner(X, Y, T)\njulia> estimatecausaleffect!(m1)\n[0.0493951571746305, 0.049395157174630444, 0.0493951571746305, 0.049395157174630444, \n0.04939515717463039, 0.04939515717463039, 0.04939515717463039, 0.04939515717463039, \n0.049395157174630444, 0.04939515717463061  …  0.0493951571746305, 0.04939515717463039, \n0.0493951571746305, 0.04939515717463039, 0.0493951571746305, 0.04939515717463039, \n0.04939515717463039, 0.049395157174630444, 0.04939515717463039, 0.049395157174630444]\n\n\n\n\n\nestimatecausaleffect!(x)\n\nEstimate the CATE using an X-Learner.\n\nFor an overview of meatlearning, including X-Learners see:\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = XLearner(X, Y, T)\njulia> estimatecausaleffect!(m1)\n[-0.025012644892878473, -0.024634294305967294, -0.022144246680543364, -0.023983138957276127, \n-0.024756239357838557, -0.019409519377053822, -0.02312807640357356, -0.016967113188439076, \n-0.020188871831409317, -0.02546526148141366  …  -0.019811641136866287, \n-0.020780821058711863, -0.013588359417922776, -0.020438648396328824, -0.016169487825519843, \n-0.024031422484491572, -0.01884713946778991, -0.021163590874553318, -0.014607310062509895, \n-0.022449034332142046]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CATE-Estimation","page":"API","title":"CATE Estimation","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"CausalELM.Metalearners\r\nCausalELM.Metalearners.Metalearner\r\nCausalELM.Metalearners.SLearner\r\nCausalELM.Metalearners.TLearner\r\nCausalELM.Metalearners.XLearner\r\nCausalELM.Metalearners.SLearner.estimatecausaleffect!\r\nCausalELM.Metalearners.TLearner.estimatecausaleffect!\r\nCausalELM.Metalearners.XLearner.estimatecausaleffect!","category":"page"},{"location":"api.html#CausalELM.Metalearners","page":"API","title":"CausalELM.Metalearners","text":"Metalearners to estimate the conditional average treatment effect (CATE).\n\n\n\n\n\n","category":"module"},{"location":"api.html#CausalELM.Metalearners.Metalearner","page":"API","title":"CausalELM.Metalearners.Metalearner","text":"Abstract type for metalearners\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.Metalearners.SLearner","page":"API","title":"CausalELM.Metalearners.SLearner","text":"S-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.Metalearners.TLearner","page":"API","title":"CausalELM.Metalearners.TLearner","text":"T-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.Metalearners.XLearner","page":"API","title":"CausalELM.Metalearners.XLearner","text":"X-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"api.html#Inference-and-Summarization","page":"API","title":"Inference and Summarization","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"CausalELM.Inference\r\nCausalELM.Inference.summarize\r\nCausalELM.Inference.quantitiesofinterest\r\nCausalELM.Inference.generatenulldistribution","category":"page"},{"location":"api.html#CausalELM.Inference","page":"API","title":"CausalELM.Inference","text":"Methods for summarization and inference from estimators and metalearners.\n\n\n\n\n\n","category":"module"},{"location":"api.html#CausalELM.summarize","page":"API","title":"CausalELM.summarize","text":"summarize(study, mean_effect)\n\nReturn a summary from an event study.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = EventStudy(X₀, Y₀, X₁, Y₁)\njulia> estimatetreatmenteffect!(m1)\n[0.25714308]\njulia> summarize(m1)\n{\"Task\" => \"Regression\", \"Regularized\" => true, \"Activation Function\" => relu, \n\"Validation Metric\" => \"mse\",\"Number of Neurons\" => 2, \n\"Number of Neurons in Approximator\" => 10, \"β\" => [0.25714308], \n\"Causal Effect\" => -3.9101138, \"Standard Error\" => 1.903434356, \"p-value\" = 0.00123356}\n\n\n\n\n\nsummarize(g)\n\nReturn a summary from a G-Computation estimator.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = GComputation(X, Y, T)\njulia> estimatetreatmenteffect!(m1)\n[0.3100468253]\njulia> summarize(m1)\n{\"Task\" => \"Regression\", \"Quantity of Interest\" => \"ATE\", Regularized\" => \"true\", \n\"Activation Function\" => \"relu\", \"Time Series/Panel Data\" => \"false\", \n\"Validation Metric\" => \"mse\",\"Number of Neurons\" => \"5\", \n\"Number of Neurons in Approximator\" => \"10\", \"β\" => \"[0.3100468253]\",\n\"Causal Effect: 0.00589761, \"Standard Error\" => 5.12900734, \"p-value\" => 0.479011245} \n\n\n\n\n\nsummarize(dre, n)\n\nReturn a summary from a doubly robust estimator.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = DoublyRobust(X, X, Y, T)\njulia> estimatetreatmenteffect!(m1)\n[0.5804032956]\njulia> summarize(m1)\n{\"Task\" => \"Regression\", \"Quantity of Interest\" => \"ATE\", Regularized\" => \"true\", \n\"Activation Function\" => \"relu\", \"Validation Metric\" => \"mse\", \"Number of Neurons\" => \"5\", \n\"Number of Neurons in Approximator\" => \"10\", \"Causal Effect\" = 0.5804032956, \n\"Standard Error\" => 2.129400324, \"p-value\" => 0.0008342356}\n\n\n\n\n\nsummarize(m, n)\n\nReturn a summary from a metalearner.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = SLearner(X, Y, T)\njulia> estimatecate!(m1)\n[0.20729633391630697, 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630697, 0.20729633391630697, 0.20729633391630703, \n0.20729633391630697, 0.20729633391630697  …  0.20729633391630703, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630703, 0.20729633391630697, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697]\njulia> summarise(m1)\n{\"Task\" => \"Regression\", Regularized\" => \"true\", \"Activation Function\" => \"relu\", \n\"Time Series/Panel Data\" => \"false\", \"Validation Metric\" => \"mse\", \n\"Number of Neurons\" => \"5\", \"Number of Neurons in Approximator\" => \"10\", \n\"β\" => \"[0.3100468253]\", \"Causal Effect: [0.20729633391630697, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630703, 0.20729633391630697, 0.20729633391630697  …  \n0.20729633391630703, 0.20729633391630697, 0.20729633391630692, 0.20729633391630703, \n0.20729633391630697, 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630697], \"Standard Error\" => 5.3121435085, \n\"p-value\" => 0.0632454855}\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Inference.quantitiesofinterest","page":"API","title":"CausalELM.Inference.quantitiesofinterest","text":"quantitiesofinterest(model, n)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from the generated distribution.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x, y, t = rand(100, 5), rand(1:100, 100, 1), [rand()<0.4 for i in 1:100]\njulia> g_computer = GComputation(x, y, t)\njulia> estimatecausaleffect!(g_computer)\njulia> quantitiesofinterest(g_computer, 1000)\n(0.114, 6.953133617011371)\n\n\n\n\n\nquantitiesofinterest(model, nsplits)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from  the generated distribution. Randomization for event studies is done by creating time splits  at even intervals and reestimating the causal effect.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> event_study = EventStudy(x₀, y₀, x₁, y₁)\njulia> estimatecausaleffect!(event_study)\njulia> quantitiesofinterest(event_study, 10)\n(0.0, 0.07703275541001667)\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Inference.generatenulldistribution","page":"API","title":"CausalELM.Inference.generatenulldistribution","text":"generatenulldistribution(e, n)\n\nGenerate a null distribution for the treatment effect of G-computation, doubly robust  estimation, or metalearning.\n\nThis method estimates the same model that is provided using random permutations of the  treatment assignment to generate a vector of estimated effects under different treatment regimes. When e is a metalearner the null statistic is the difference is the ATE.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nExamples\n\njulia> x, y, t = rand(100, 5), rand(1:100, 100, 1), [rand()<0.4 for i in 1:100]\njulia> g_computer = GComputation(x, y, t)\njulia> estimatecausaleffect!(g_computer)\njulia> generatenulldistribution(g_computer, 500)\n[0.016297180690693656, 0.0635928694685571, 0.20004144093635673, 0.505893866040335, \n0.5130594630907543, 0.5432486130493388, 0.6181727442724846, 0.61838399963459, \n0.7038981488009489, 0.7043407710415689  …  21.909186142780246, 21.960498059428854, \n21.988553083790023, 22.285403459215363, 22.613625375395973, 23.382102081355548, \n23.52056245175936, 24.739658523175912, 25.30523686137909, 28.07474553316176]\n\n\n\n\n\ngeneratenulldistribution(e, n, mean_effect)\n\nGenerate a null distribution for the treatment effect in an event study design. By default,  this method generates a null distribution of mean differences. To generate a null  distribution of cummulative differences, set the mean_effect argument to false.\n\nInstead of randomizing the assignment of units to the treamtent or control group, this  method generates the null distribution by reestimating the event study with the intervention set to n splits at even intervals within the total study duration.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> event_study = EventStudy(x₀, y₀, x₁, y₁)\njulia> estimatecausaleffect!(event_study)\njulia> generatenulldistribution(event_study, 10)\n[-0.5012456678829079, -0.33790650529972194, -0.2534340182760628, -0.21030239864895905, \n-0.11672915615117885, -0.08149441936166794, -0.0685134758182695, -0.06217013151235991, \n-0.05905529159312335, -0.04927743270606937]\n\n\n\n\n\n","category":"function"},{"location":"api.html#Validation-Metrics","page":"API","title":"Validation Metrics","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"CausalELM.Metrics\r\nCausalELM.Metrics.mse\r\nCausalELM.Metrics.mae\r\nCausalELM.Metrics.confusionmatrix\r\nCausalELM.Metrics.accuracy\r\nCausalELM.Metrics.precision\r\nCausalELM.Metrics.recall\r\nCausalELM.Metrics.F1","category":"page"},{"location":"api.html#CausalELM.Metrics","page":"API","title":"CausalELM.Metrics","text":"Metrics to evaluate the performance of an Extreme learning machine for regression and classification tasks.\n\n\n\n\n\n","category":"module"},{"location":"api.html#CausalELM.Metrics.mse","page":"API","title":"CausalELM.Metrics.mse","text":"mse(y, ŷ)\n\nCalculate the mean squared error\n\nSee also mae.\n\nExamples\n\njulia> mse([0.0, 0.0, 0.0], [0.0, 0.0, 0.0])\n0\njulia> mse([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n4\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Metrics.mae","page":"API","title":"CausalELM.Metrics.mae","text":"mae(y, ŷ)\n\nCalculate the mean absolute error\n\nSee also mse.\n\nExamples\n\njulia> mae([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n2\njulia> mae([1.0, 1.0, 1.0], [2.0, 2.0, 2.0])\n1\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Metrics.confusionmatrix","page":"API","title":"CausalELM.Metrics.confusionmatrix","text":"confusionmatrix(y, ŷ)\n\nGenerate a confusion matrix\n\nExamples\n\njulia> confusionmatrix([1, 1, 1, 1, 0], [1, 1, 1, 1, 0])\n2×2 Matrix{Int64}:\n 1  0\n 0 4\njulia> confusionmatrix([1, 1, 1, 1, 0, 2], [1, 1, 1, 1, 0, 2])\n3×3 Matrix{Int64}:\n 1  0 0\n 0 4 0\n 0 0 1\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Metrics.accuracy","page":"API","title":"CausalELM.Metrics.accuracy","text":"accuracy(y, ŷ)\n\nCalculate the accuracy for a classification task\n\nExamples\n\njulia> accuracy([1, 1, 1, 1], [0, 1, 1, 0])\n0.5\njulia> accuracy([1, 2, 3, 4], [1, 1, 1, 1])\n0.25\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Metrics.precision","page":"API","title":"CausalELM.Metrics.precision","text":"precision(y, ŷ)\n\nCalculate the precision for a classification task\n\nSee also recall.\n\nExamples\n\njulia> precision([0, 1, 0, 0], [0, 1, 1, 0])\n0.5\njulia> precision([0, 1, 0, 0], [0, 1, 0, 0])\n1\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Metrics.recall","page":"API","title":"CausalELM.Metrics.recall","text":"recall(y, ŷ)\n\nCalculate the recall for a classification task\n\nSee also precision.\n\nExamples\n\njulia> recall([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.5\njulia> recall([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])\n1\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Metrics.F1","page":"API","title":"CausalELM.Metrics.F1","text":"F1(y, ŷ)\n\nCalculate the F1 score for a classification task\n\nExamples\n\njulia> F1([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.4\njulia> F1([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])\n0.47058823529411764\n\n\n\n\n\n","category":"function"},{"location":"api.html#Base-Models","page":"API","title":"Base Models","text":"","category":"section"},{"location":"api.html","page":"API","title":"API","text":"CausalELM.Models\r\nCausalELM.Models.ExtremeLearningMachine\r\nCausalELM.Models.ExtremeLearner\r\nCausalELM.Models.RegularizedExtremeLearner\r\nCausalELM.Models.fit!\r\nCausalELM.Models.predict\r\nCausalELM.Models.predictcounterfactual!\r\nCausalELM.Models.placebotest","category":"page"},{"location":"api.html#CausalELM.Models","page":"API","title":"CausalELM.Models","text":"Base models to perform extreme learning with and without L2 penalization.\n\nFor details on Extreme learning machines see;     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nFor details on Extreme learning machines with an L2 penalty see:     Li, Guoqiang, and Peifeng Niu. \"An enhanced extreme learning machine based on ridge      regression for regression.\" Neural Computing and Applications 22, no. 3 (2013):      803-810.\n\n\n\n\n\n","category":"module"},{"location":"api.html#CausalELM.Models.ExtremeLearningMachine","page":"API","title":"CausalELM.Models.ExtremeLearningMachine","text":"Abstract type that includes vanilla and L2 regularized Extreme Learning Machines\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.Models.ExtremeLearner","page":"API","title":"CausalELM.Models.ExtremeLearner","text":"Struct to hold data for an Extreme Learning machine\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.Models.RegularizedExtremeLearner","page":"API","title":"CausalELM.Models.RegularizedExtremeLearner","text":"Struct to hold data for a regularized Extreme Learning Machine\n\n\n\n\n\n","category":"type"},{"location":"api.html#CausalELM.Models.fit!","page":"API","title":"CausalELM.Models.fit!","text":"fit!(model)\n\nMake predictions with an ExtremeLearner.\n\nFor more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit!(m1)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]\n\n\n\n\n\nfit!(model)\n\nFit a Regularized Extreme Learner.\n\nFor more details see:      Li, Guoqiang, and Peifeng Niu. \"An enhanced extreme learning machine based on ridge      regression for regression.\" Neural Computing and Applications 22, no. 3 (2013):      803-810.\n\nExamples julia-repl julia> m1 = RegularizedExtremeLearner(x, y, 10, σ)  Regularized Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit!(m1)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Models.predict","page":"API","title":"CausalELM.Models.predict","text":"predict(model, X)\n\nUse an ExtremeLearningMachine to make predictions.\n\nFor more details see:      Huang G-B, Zhu Q-Y, Siew C. Extreme learning machine: theory and applications.      Neurocomputing. 2006;70:489–501. https://doi.org/10.1016/j.neucom.2005.12.126\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit(m1, sigmoid)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]  julia> predict(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])  [9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Models.predictcounterfactual!","page":"API","title":"CausalELM.Models.predictcounterfactual!","text":"predictcounterfactual(model, X)\n\nUse an ExtremeLearningMachine to predict the counterfactual.\n\nThis should be run with the observed covariates. To use synthtic data for what-if      scenarios use predict.\n\nSee also predict.\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit(m1, sigmoid)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]  julia> predictcounterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])  [9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978]\n\n\n\n\n\n","category":"function"},{"location":"api.html#CausalELM.Models.placebotest","page":"API","title":"CausalELM.Models.placebotest","text":"placebotest(model)\n\nConduct a placebo test.\n\nThis method makes predictions for the post-event or post-treatment period using data  in the pre-event or pre-treatment period and the post-event or post-treament. If there is a statistically significant difference between these predictions the study design may be flawed. Due to the multitude of significance tests for time series data, this function returns the predictions but does not test for statistical significance.\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit(m1, sigmoid)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]  julia> predictcounterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])  [9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978]  julia> placebotest(m1)  ([9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978],  [0.5, 0.4, 0.3, 0.2])\n\n\n\n\n\n","category":"function"},{"location":"contributing.html#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"CausalELM welcomes and appreciates all contributions. Below are some guidlines to ensure the  contribution process is as smooth as possible.","category":"page"},{"location":"contributing.html#Reporting-a-Bug","page":"Contributing","title":"Reporting a Bug","text":"","category":"section"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"To report a bug, open an issue on the CausalELM.jl GitHub page. Please include all relevant  infomration, such as what methods were called, the operating system used, the verions of  CausalELM used, the verion or Julia used, any tracebakcs or error codes, and any other  relevant information.","category":"page"},{"location":"contributing.html#Requesting-New-Features","page":"Contributing","title":"Requesting New Features","text":"","category":"section"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"Before requesting a new feature, please check the issues page on GitHub to make sure someone else did not already request the same feature. If this is not the case, then please open an issue that explains what function or method you would like to be added and how you beleive  it should behave.","category":"page"},{"location":"contributing.html#Contributing-Code","page":"Contributing","title":"Contributing Code","text":"","category":"section"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"Before submitting a pull request, please open an issue explaining what the proposed code is and why you want to add it. When submitting a pull request, please reference the relevant issue(s). Please also ensure your code follows the following guidelines.","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"All abstract structs, structs, functions, methods, macros, and constants have docstrings ","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"that follow the same format as the other docstrings. These functions should also be included  in the relevant section of the API Manual.","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"There are no repeated code blocks. If there are repeated codeblocks, then they should be ","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"in a separate function.","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"Methods should generally include types and be type stable.If there is a strong reason to ","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"deviate from this point, there should be a comment in the code explaining why.","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"Minimize use of new constants and macros. If they must be included, the reason for their ","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"includsion should be obvious or included in the docstring.","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"When possible and relevant, code should call the @fastmath and @inbounds macros.","category":"page"},{"location":"contributing.html","page":"Contributing","title":"Contributing","text":"Use self-explanatory names for variables, methods, structs, constants, and macros.","category":"page"},{"location":"examples.html#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"Below are some small examples for estimating causal quantities of interest with CausalELM. Regardless of the estimator, the workflow is the same; get some data, initialize an  estimator, estimate the causal effect of interest, and get a summary of the model.","category":"page"},{"location":"examples.html#Event-Study-Estimation","page":"Examples","title":"Event Study Estimation","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"# Generate some data\r\nX₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\r\n\r\n# Initialize an event study estimator\r\nm1 = EventStudy(X₀, Y₀, X₁, Y₁)\r\n\r\n# Estimate the average treatment effect\r\n# We can also estimate the ATT of ITE\r\nestimatecausaleffect!(m1)\r\n\r\n# Get a summary\r\nsummarize(m1)","category":"page"},{"location":"examples.html#G-Computation","page":"Examples","title":"G-Computation","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"# Create some data with a binary treatment\r\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\r\n\r\n# Initialize a model\r\nm1 = GComputation(X, Y, T)\r\n\r\n# Estimate the ATE\r\n# Note that we could also estimate the ATT or ITE\r\nestimatecausaleffect!(m1)\r\n\r\n# Get a summary that includes a p-value and standard error via randomization inference\r\nsummarize(m1)","category":"page"},{"location":"examples.html#Doubly-Robust-Estimation","page":"Examples","title":"Doubly Robust Estimation","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"# Create some data with a binary treatment\r\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\r\n\r\n# Initialize a model\r\nm1 = DoublyRobust(X, Y, T)\r\n\r\n# Estimate the ATE\r\n# Note that we could also estimate the ATT or ITE\r\nestimatecausaleffect!(m1)\r\n\r\n# Get a summary with a p-value and standard error via randomization inference\r\nsummarize(m1)","category":"page"},{"location":"examples.html#S-Learning","page":"Examples","title":"S-Learning","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"# Generate data\r\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\r\n\r\n# Initialize an S-Learner\r\nm1 = SLearner(X, Y, T)\r\n\r\n# Estimate the CATE\r\nestimatecausaleffect!(m1)\r\n\r\n# Get a summary that includes a p-value and standard via randomization inference\r\nsummarize(m1)","category":"page"},{"location":"examples.html#T-Learning","page":"Examples","title":"T-Learning","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"# Generate data\r\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\r\n\r\n# Initialize an S-Learner\r\nm1 = TLearner(X, Y, T)\r\n\r\n# Estimate the CATE\r\nestimatecausaleffect!(m1)\r\n\r\n# Get a summary that includes a p-value and standard error via randomization inference\r\nsummarize(m1)","category":"page"},{"location":"examples.html#X-Learning","page":"Examples","title":"X-Learning","text":"","category":"section"},{"location":"examples.html","page":"Examples","title":"Examples","text":"# Generate data\r\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\r\n\r\n# Initialize an S-Learner\r\nm1 = XLearner(X, Y, T)\r\n\r\n# Estimate the CATE\r\nestimatecausaleffect!(m1)\r\n\r\n# Get a summary that includes a p-value and standard error via randomization inference\r\nsummarize(m1)","category":"page"},{"location":"index.html","page":"CausalELM","title":"CausalELM","text":"CurrentModule = CausalELM","category":"page"},{"location":"index.html#Overview","page":"CausalELM","title":"Overview","text":"","category":"section"},{"location":"index.html","page":"CausalELM","title":"CausalELM","text":"CausalELM enables Estimation of causal quantities of interest in research designs where a  counterfactual must be predicted and compared to the observed outcomes. More specifically,  CausalELM provides structs and methods to execute event study designs (interupted time  series analysis), G-Computation, and doubly robust estimation as well as estimation of the  CATE via S-Learning, T-Learning, and X-Learning. Once a causal model has beeen estimated,  CausalELM's summarize method provides basic information about the model as well as a p-value  and standard error estimated with approximate randomization inference. In all of these  implementations, CausalELM predicts the counterfactuals using an Extreme Learning Machine  that includes an L2 penalty by default. In this context, ELMs strike a good balance between  prediction accuracy, generalization, ease of implementation, speed, and interpretability. ","category":"page"},{"location":"index.html#Installation","page":"CausalELM","title":"Installation","text":"","category":"section"},{"location":"index.html","page":"CausalELM","title":"CausalELM","text":"using Pkg\nPkg.add(\"CausalELM\")","category":"page"},{"location":"index.html#Estimating-Causal-Effects","page":"CausalELM","title":"Estimating Causal Effects","text":"","category":"section"},{"location":"index.html","page":"CausalELM","title":"CausalELM","text":"\nusing CausalELM\n\n# 1000 data points with 5 features in pre-event period\nx0 = rand(1000, 5)\n\n# Pre-event outcome\ny0 = rand(1000)\n\n# 200 data points in the post-event period\nx1 = rand(200, 5)\n\n# Pose-event outcome\ny1 = rand(200)\n\n# Instantiate an EventStudy struct\nevent_study = EventStudy(x0, y0, x1, y1)\n\nestimatecausaleffect!(event_study)\n\n# Get information about the model including the p-value and standard error\nsummarize(event_study)","category":"page"}]
}
