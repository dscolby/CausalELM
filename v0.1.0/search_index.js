var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"This is a reference for advanced users or those who wish to contribute.","category":"page"},{"location":"api/","page":"API","title":"API","text":"","category":"page"},{"location":"api/#CausalELM","page":"API","title":"CausalELM","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM","category":"page"},{"location":"api/#CausalELM","page":"API","title":"CausalELM","text":"Macros, functions, and structs for applying Extreme Learning Machines to causal inference tasks where the counterfactual is unavailable or biased and must be predicted. Provides  macros for event study designs, parametric G-computation, doubly robust machine learning, and  metalearners. Additionally, these tasks can be performed with or without L2 penalization and will automatically choose the best number of neurons and L2 penalty. \n\nFor more details on Extreme Learning Machines see:     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\n\n\n\n\n","category":"module"},{"location":"api/#Activation-Functions","page":"API","title":"Activation Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.ActivationFunctions\nCausalELM.ActivationFunctions.binarystep\nCausalELM.ActivationFunctions.σ\nCausalELM.ActivationFunctions.tanh\nCausalELM.ActivationFunctions.relu\nCausalELM.ActivationFunctions.leakyrelu\nCausalELM.ActivationFunctions.swish\nCausalELM.ActivationFunctions.softmax\nCausalELM.ActivationFunctions.softplus\nCausalELM.ActivationFunctions.gelu\nCausalELM.ActivationFunctions.gaussian\nCausalELM.ActivationFunctions.hardtanh\nCausalELM.ActivationFunctions.elish\nCausalELM.ActivationFunctions.fourier","category":"page"},{"location":"api/#CausalELM.ActivationFunctions","page":"API","title":"CausalELM.ActivationFunctions","text":"Activation functions for Extreme Learning machines\n\n\n\n\n\n","category":"module"},{"location":"api/#CausalELM.ActivationFunctions.binarystep","page":"API","title":"CausalELM.ActivationFunctions.binarystep","text":"binarystep(x)\n\nApply the binary step activation function to a real number.\n\nExamples\n\njulia> binarystep(1)\n1\n\n\n\n\n\nbinarystep(x)\n\nApply the binary step activation function to an array.\n\nExamples\n\njulia> binarystep([-1000, 100, 1, 0, -0.001, -3])\n[0, 1, 1, 1, 0, 0]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.σ","page":"API","title":"CausalELM.ActivationFunctions.σ","text":"σ(x)\n\nApply the sigmoid activation function to a real number.\n\nExamples\n\njulia> σ(1)\n0.7310585786300049\n\n\n\n\n\nσ(x)\n\nApply the sigmoid activation function to an array.\n\nExamples\n\njulia> σ([1, 0])\n[0.7310585786300049, 0.5]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.tanh","page":"API","title":"CausalELM.ActivationFunctions.tanh","text":"tanh(x)\n\nApply the tanh activation function to an array.\n\nThis is just a vectorized version of Base.tanh\n\nExamples\n\njulia> tanh([1, 0])\n[0.7615941559557649, 0.0]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.relu","page":"API","title":"CausalELM.ActivationFunctions.relu","text":"relu(x)\n\nApply the ReLU activation function to a real number.\n\nExamples\n\njulia> relu(1)\n1\n\n\n\n\n\nrelu(x)\n\nApply the ReLU activation function to an array.\n\nExamples\n\njulia> relu([1, 0, -1])\n[1, 0, 0]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.leakyrelu","page":"API","title":"CausalELM.ActivationFunctions.leakyrelu","text":"leakyrelu(x)\n\nApply the leaky ReLU activation function to a real number.\n\nExamples\n\njulia> leakyrelu(1)\n1\n\n\n\n\n\nleakyrelu(x)\n\nApply the leaky ReLU activation function to an array.\n\nExamples\n\njulia> leakyrelu([-0.01, 0, 1])\n[1, 0, 0]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.swish","page":"API","title":"CausalELM.ActivationFunctions.swish","text":"swish(x)\n\nApply the swish activation function to a real number.\n\nExamples\n\njulia> swish(1)\n0.7310585786300049\n\n\n\n\n\nswish(x)\n\nApply the swish activation function to an array.\n\nExamples\n\njulia> swish([1, 0, -1])\n[0.7310585786300049, 0, -0.2689414213699951]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.softmax","page":"API","title":"CausalELM.ActivationFunctions.softmax","text":"softmax(x)\n\nApply the softmax activation function to a real number.\n\nFor numbers that have large absolute values this function may become numerically unstable.\n\nExamples\n\njulia> softmax(1)\n2.718281828459045\n\n\n\n\n\nsoftmax(x)\n\nApply the softmax activation function to an array.\n\nFor numbers that have large absolute values this function might be numerically unstable.\n\nExamples\n\njulia> softmax([1, -1])\n[2.718281828459045, -0.36787944117144233]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.softplus","page":"API","title":"CausalELM.ActivationFunctions.softplus","text":"softplus(x)\n\nApply the softplus activation function to a real number.\n\nExamples\n\njulia> softplus(1)\n1.3132616875182228\n\n\n\n\n\nsoftplus(x)\n\nApply the softplus activation function to an array.\n\nExamples\n\njulia> softplus([1, -1])\n[1.3132616875182228, 0.31326168751822286]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.gelu","page":"API","title":"CausalELM.ActivationFunctions.gelu","text":"gelu(x)\n\nApply the GeLU activation function to a real number.\n\nExamples\n\njulia> gelu(1)\n0.8411919906082768\n\n\n\n\n\ngelu(x)\n\nApply the GeLU activation function to an array.\n\nExamples\n\njulia> gelu([-1, 0, 1])\n[-0.15880800939172324, 0, 0.8411919906082768]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.gaussian","page":"API","title":"CausalELM.ActivationFunctions.gaussian","text":"gaussian(x)\n\nApply the gaussian activation function to a real number.\n\nExamples\n\njulia> gaussian(1)\n0.11443511435028261\n\n\n\n\n\ngaussian(x)\n\nApply the gaussian activation function to an array.\n\nExamples\n\njulia> gaussian([1, -1])\n[0.36787944117144233, 0.36787944117144233]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.hardtanh","page":"API","title":"CausalELM.ActivationFunctions.hardtanh","text":"hardtanh(x)\n\nApply the hardtanh activation function to a real number.\n\nExamples\n\njulia> hardtanh(-2)\n-1\n\n\n\n\n\nhardtanh(x)\n\nApply the hardtanh activation function to an array.\n\nExamples\n\njulia> hardtanh([-2, 0, 2])\n[-1, 0, 1]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.elish","page":"API","title":"CausalELM.ActivationFunctions.elish","text":"elish(x)\n\nApply the ELiSH activation function to a real number.\n\nExamples\n\njulia> elish(1)\n0.7310585786300049\n\n\n\n\n\nelish(x)\n\nApply the ELiSH activation function to an array.\n\nExamples\n\njulia> elish([-1, 1])\n[-0.17000340156854793, 0.7310585786300049]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ActivationFunctions.fourier","page":"API","title":"CausalELM.ActivationFunctions.fourier","text":"fourrier(x)\n\nApply the Fourier activation function to a real number.\n\nExamples\n\njulia> fourier(1)\n0.8414709848078965\n\n\n\n\n\nfourrier(x)\n\nApply the Fourier activation function to an array.\n\nExamples\n\njulia> fourier([-1, 1])\n[-0.8414709848078965, 0.8414709848078965]\n\n\n\n\n\n","category":"function"},{"location":"api/#Cross-Valdiation","page":"API","title":"Cross Valdiation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.CrossValidation\nCausalELM.CrossValidation.recode\nCausalELM.CrossValidation.traintest\nCausalELM.CrossValidation.validate\nCausalELM.CrossValidation.crossvalidate\nCausalELM.CrossValidation.bestsize","category":"page"},{"location":"api/#CausalELM.CrossValidation","page":"API","title":"CausalELM.CrossValidation","text":"Methods to perform cross validation and find the optimum number of neurons.\n\nTo reduce computation time, the number of neurons is optimized by using cross validation to estimate the validation error on a small subset of the range of possible numbers of  neurons. Then, an Extreme Learning Machine is trained to predict validation loss from  the given cross validation sets. Finally, the number of neurons is selected that has the  smallest predicted loss or the highest classification metric.\n\n\n\n\n\n","category":"module"},{"location":"api/#CausalELM.CrossValidation.recode","page":"API","title":"CausalELM.CrossValidation.recode","text":"recode(ŷ)\n\nRound predicted values to their predicted class for classification tasks.\n\nIf the smallest predicted label is 0, all labels are shifted up 1; if the smallest  label is -1, all labels are shifted up 2. Also labels cannot be smaller than -1.\n\nExamples\n\njulia> recode([-0.7, 0.2, 1.1])\n3-element Vector{Float64}\n1\n2\n3\njulia> recode([0.1, 0.2, 0.3])\n3-element Vector{Float64}\n1\n1\n1\njulia> recode([1.1, 1.51, 1.8])\n3-element Vector{Float64}\n1\n2\n2\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.CrossValidation.traintest","page":"API","title":"CausalELM.CrossValidation.traintest","text":"traintest(X, Y, folds)\n\nCreate a train-test split.\n\nIf an iteration is specified, the train test split will be treated as time series/panel data.\n\nExamples\n\njulia> xtrain, ytrain, xtest, ytest = traintest(zeros(20, 2), zeros(20), 5)\n\n\n\n\n\ntraintest(X, Y, folds, iteration)\n\nCreate a rolling train-test split for time series/panel data.\n\nAn iteration should not be specified for non-time series/panel data.\n\nExamples\n\njulia> xtrain, ytrain, xtest, ytest = traintest(zeros(20, 2), zeros(20), 5, 1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.CrossValidation.validate","page":"API","title":"CausalELM.CrossValidation.validate","text":"validate(X, Y, nodes, metric, iteration...; activation, regularized, folds)\n\nCalculate a validation metric for a single fold in k-fold cross validation.\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> validate(x, y, 5, accuracy, 3)\n0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.CrossValidation.crossvalidate","page":"API","title":"CausalELM.CrossValidation.crossvalidate","text":"crossvalidate(X, Y, neurons, metric, activation, regularized, folds)\n\nCalculate a validation metric for k folds using a single set of hyperparameters.\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> crossvalidate(x, y, 5, accuracy)\n0.0257841765251021\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.CrossValidation.bestsize","page":"API","title":"CausalELM.CrossValidation.bestsize","text":"bestsize(X, Y, metric, task, activation, min_neurons, max_neurons, regularized, folds, temporal, \n    iterations, approximator_neurons)\n\nCompute the best number of neurons for an Extreme Learning Machine.\n\nThe procedure tests networks with numbers of neurons in a sequence whose length is given  by iterations on the interval [minneurons, maxneurons]. Then, it uses the networks  sizes and validation errors from the sequence to predict the validation error or metric  for every network size between minneurons and maxneurons using the function  approximation ability of an Extreme Learning Machine. Finally, it returns the network  size with the best predicted validation error or metric.\n\nExamples\n\njulia> bestsize(rand(100, 5), rand(100), mse, \"regression\")\n11\n\n\n\n\n\n","category":"function"},{"location":"api/#ATE/ATE/ITT-Estimation","page":"API","title":"ATE/ATE/ITT Estimation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.Estimators\nCausalELM.Estimators.EventStudy\nCausalELM.Estimators.GComputation\nCausalELM.Estimators.DoublyRobust\nCausalELM.Estimators.estimatecausaleffect!\nCausalELM.Estimators.summarize","category":"page"},{"location":"api/#CausalELM.Estimators","page":"API","title":"CausalELM.Estimators","text":"Estimate causal effects with event study designs, G-computation, and doubly robust  estiamtion using Extreme Learning machines.\n\n\n\n\n\n","category":"module"},{"location":"api/#CausalELM.Estimators.EventStudy","page":"API","title":"CausalELM.Estimators.EventStudy","text":"Container for the results of an event study\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Estimators.GComputation","page":"API","title":"CausalELM.Estimators.GComputation","text":"Container for the results of G-Computation\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Estimators.DoublyRobust","page":"API","title":"CausalELM.Estimators.DoublyRobust","text":"Container for the results of doubly robust estimation\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Estimators.estimatecausaleffect!","page":"API","title":"CausalELM.Estimators.estimatecausaleffect!","text":"estimatecausaleffect!(study)\n\nEstimate the abnormal returns in an event study.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = EventStudy(X₀, Y₀, X₁, Y₁)\njulia> estimatecausaleffect!(m1)\n0.25714308\n\n\n\n\n\nestimatecausaleffect!(g)\n\nEstimate a causal effect of interest using G-Computation.\n\nIf treatents are administered at multiple time periods, the effect will be estimated as the average difference between the outcome of being treated in all periods and being treated in no periods. For example, given that individuals 1, 2, ..., i ∈ I recieved either a treatment or a placebo in p  different periods, the model would estimate the average treatment effect as  E[Yᵢ|T₁=1, T₂=1, ... Tₚ=1, Xₚ] - E[Yᵢ|T₁=0, T₂=0, ... Tₚ=0, Xₚ].\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = GComputation(X, Y, T)\njulia> estimatecausaleffect!(m1)\n0.31067439\n\n\n\n\n\nestimatecausaleffect!(DRE)\n\nEstimate a causal effect of interest using doubly robust estimation.\n\nUnlike other estimators, this method does not support time series or panel data. This method also  does not work as well with smaller datasets because it estimates separate outcome models for the  treatment and control groups.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = DoublyRobust(X, Y, T)\njulia> estimatecausaleffect!(m1)\n0.31067439\n\n\n\n\n\nestimatecausaleffect!(s)\n\nEstimate the CATE using an S-Learner.\n\nFor an overview of meatlearning, including S-Learners see:\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = SLearner(X, Y, T)\njulia> estimatecausaleffect!(m1)\n[0.20729633391630697, 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630697, 0.20729633391630697, 0.20729633391630703, \n0.20729633391630697, 0.20729633391630697  …  0.20729633391630703, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630703, 0.20729633391630697, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697]\n\n\n\n\n\nestimatecausaleffect!(t)\n\nEstimate the CATE using a T-Learner.\n\nFor an overview of meatlearning, including T-Learners see:\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = TLearner(X, Y, T)\njulia> estimatecausaleffect!(m1)\n[0.0493951571746305, 0.049395157174630444, 0.0493951571746305, 0.049395157174630444, \n0.04939515717463039, 0.04939515717463039, 0.04939515717463039, 0.04939515717463039, \n0.049395157174630444, 0.04939515717463061  …  0.0493951571746305, 0.04939515717463039, \n0.0493951571746305, 0.04939515717463039, 0.0493951571746305, 0.04939515717463039, \n0.04939515717463039, 0.049395157174630444, 0.04939515717463039, 0.049395157174630444]\n\n\n\n\n\nestimatecausaleffect!(x)\n\nEstimate the CATE using an X-Learner.\n\nFor an overview of meatlearning, including X-Learners see:\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = XLearner(X, Y, T)\njulia> estimatecausaleffect!(m1)\n[-0.025012644892878473, -0.024634294305967294, -0.022144246680543364, -0.023983138957276127, \n-0.024756239357838557, -0.019409519377053822, -0.02312807640357356, -0.016967113188439076, \n-0.020188871831409317, -0.02546526148141366  …  -0.019811641136866287, \n-0.020780821058711863, -0.013588359417922776, -0.020438648396328824, -0.016169487825519843, \n-0.024031422484491572, -0.01884713946778991, -0.021163590874553318, -0.014607310062509895, \n-0.022449034332142046]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.Estimators.summarize","page":"API","title":"CausalELM.Estimators.summarize","text":"summarize(study)\n\nReturn a summary from an event study.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = EventStudy(X₀, Y₀, X₁, Y₁)\njulia> estimatetreatmenteffect!(m1)\n[0.25714308]\njulia> summarize(m1)\n{\"Task\" => \"Regression\", \"Regularized\" => \"true\", \"Activation Function\" => \"relu\", \n\"Validation Metric\" => \"mse\",\"Number of Neurons\" => \"2\", \"Number of Neurons in Approximator\" => \"10\", \n\"β\" => \"[0.25714308]\"}\n\n\n\n\n\nsummarize(study)\n\nReturn a summary from an event study.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = GComputation(X, Y, T)\njulia> estimatetreatmenteffect!(m1)\n[0.3100468253]\njulia> summarize(m1)\n{\"Task\" => \"Regression\", \"Quantity of Interest\" => \"ATE\", Regularized\" => \"true\", \n\"Activation Function\" => \"relu\", \"Time Series/Panel Data\" => \"false\", \"Validation Metric\" => \"mse\",\n\"Number of Neurons\" => \"5\", \"Number of Neurons in Approximator\" => \"10\", \"β\" => \"[0.3100468253]\",\n\"Causal Effect: 0.00589761} \n\n\n\n\n\nsummarize(dre)\n\nReturn a summary from a doubly robust estimator.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = DoublyRobust(X, X, Y, T)\njulia> estimatetreatmenteffect!(m1)\n[0.5804032956]\njulia> summarize(m1)\n{\"Task\" => \"Regression\", \"Quantity of Interest\" => \"ATE\", Regularized\" => \"true\", \n\"Activation Function\" => \"relu\", \"Validation Metric\" => \"mse\", \"Number of Neurons\" => \"5\", \n\"Number of Neurons in Approximator\" => \"10\", \"Causal Effect\" = 0.5804032956}\n\n\n\n\n\nsummarise(m)\n\nReturn a summary from a metalearner.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = SLearner(X, Y, T)\njulia> estimatecate!(m1)\n[0.20729633391630697, 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630697, 0.20729633391630697, 0.20729633391630703, \n0.20729633391630697, 0.20729633391630697  …  0.20729633391630703, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630703, 0.20729633391630697, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697]\njulia> summarise(m1)\n{\"Task\" => \"Regression\", Regularized\" => \"true\", \"Activation Function\" => \"relu\", \n\"Time Series/Panel Data\" => \"false\", \"Validation Metric\" => \"mse\", \n\"Number of Neurons\" => \"5\", \"Number of Neurons in Approximator\" => \"10\", \n\"β\" => \"[0.3100468253]\", \"Causal Effect: [0.20729633391630697, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630703, 0.20729633391630697, 0.20729633391630697  …  \n0.20729633391630703, 0.20729633391630697, 0.20729633391630692, 0.20729633391630703, \n0.20729633391630697, 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630697]}\n\n\n\n\n\n","category":"function"},{"location":"api/#CATE-Estimation","page":"API","title":"CATE Estimation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.Metalearners\nCausalELM.Metalearners.SLearner\nCausalELM.Metalearners.TLearner\nCausalELM.Metalearners.XLearner","category":"page"},{"location":"api/#CausalELM.Metalearners","page":"API","title":"CausalELM.Metalearners","text":"Metalearners to estimate the conditional average treatment effect (CATE).\n\n\n\n\n\n","category":"module"},{"location":"api/#CausalELM.Metalearners.SLearner","page":"API","title":"CausalELM.Metalearners.SLearner","text":"S-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Metalearners.TLearner","page":"API","title":"CausalELM.Metalearners.TLearner","text":"T-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Metalearners.XLearner","page":"API","title":"CausalELM.Metalearners.XLearner","text":"X-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"api/#Validation-Metrics","page":"API","title":"Validation Metrics","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.Metrics\nCausalELM.Metrics.mse\nCausalELM.Metrics.mae\nCausalELM.Metrics.accuracy\nCausalELM.Metrics.precision\nCausalELM.Metrics.recall\nCausalELM.Metrics.F1","category":"page"},{"location":"api/#CausalELM.Metrics","page":"API","title":"CausalELM.Metrics","text":"Metrics to evaluate the performance of an Extreme learning machine for regression and classification tasks.\n\n\n\n\n\n","category":"module"},{"location":"api/#CausalELM.Metrics.mse","page":"API","title":"CausalELM.Metrics.mse","text":"mse(y, ŷ)\n\nCalculate the mean squared error\n\nSee also 'mae'.\n\nExamples\n\njulia> mse([0.0, 0.0, 0.0], [0.0, 0.0, 0.0])\n0\njulia> mse([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n4\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.Metrics.mae","page":"API","title":"CausalELM.Metrics.mae","text":"mae(y, ŷ)\n\nCalculate the mean absolute error\n\nSee also 'mse'.\n\nExamples\n\njulia> mae([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n2\njulia> mae([1.0, 1.0, 1.0], [2.0, 2.0, 2.0])\n1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.Metrics.accuracy","page":"API","title":"CausalELM.Metrics.accuracy","text":"accuracy(y, ŷ)\n\nCalculate the accuracy for a classification task\n\nExamples\n\njulia> accuracy([1, 1, 1, 1], [0, 1, 1, 0])\n0.5\njulia> accuracy([1, 2, 3, 4], [1, 1, 1, 1])\n0.25\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.Metrics.precision","page":"API","title":"CausalELM.Metrics.precision","text":"precision(y, ŷ)\n\nCalculate the precision for a classification task\n\nSee also 'recall'.\n\nExamples\n\njulia> precision([0, 1, 0, 0], [0, 1, 1, 0])\n0.5\njulia> precision([0, 1, 0, 0], [0, 1, 0, 0])\n1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.Metrics.recall","page":"API","title":"CausalELM.Metrics.recall","text":"recall(y, ŷ)\n\nCalculate the recall for a classification task\n\nSee also 'precision'.\n\nExamples\n\njulia> recall([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.5\njulia> recall([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])\n1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.Metrics.F1","page":"API","title":"CausalELM.Metrics.F1","text":"F1(y, ŷ)\n\nCalculate the F1 score for a classification task\n\nExamples\n\njulia> F1([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.4\njulia> F1([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])\n0.47058823529411764\n\n\n\n\n\n","category":"function"},{"location":"api/#Base-Models","page":"API","title":"Base Models","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.Models\nCausalELM.Models.ExtremeLearningMachine\nCausalELM.Models.ExtremeLearner\nCausalELM.Models.RegularizedExtremeLearner\nCausalELM.Models.fit!\nCausalELM.Models.predict\nCausalELM.Models.predictcounterfactual!\nCausalELM.Models.placebotest","category":"page"},{"location":"api/#CausalELM.Models","page":"API","title":"CausalELM.Models","text":"Base models to perform extreme learning with and without L2 penalization.\n\nFor details on Extreme learning machines see;     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nFor details on Extreme learning machines with an L2 penalty see:     Li, Guoqiang, and Peifeng Niu. \"An enhanced extreme learning machine based on ridge      regression for regression.\" Neural Computing and Applications 22, no. 3 (2013):      803-810.\n\n\n\n\n\n","category":"module"},{"location":"api/#CausalELM.Models.ExtremeLearningMachine","page":"API","title":"CausalELM.Models.ExtremeLearningMachine","text":"Abstract type that includes vanilla and L2 regularized Extreme Learning Machines\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Models.ExtremeLearner","page":"API","title":"CausalELM.Models.ExtremeLearner","text":"Struct to hold data for an Extreme Learning machine\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Models.RegularizedExtremeLearner","page":"API","title":"CausalELM.Models.RegularizedExtremeLearner","text":"Struct to hold data for a regularized Extreme Learning Machine\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Models.fit!","page":"API","title":"CausalELM.Models.fit!","text":"fit!(model)\n\nMake predictions with an ExtremeLearner.\n\nFor more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit!(m1)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]\n\n\n\n\n\nfit!(model)\n\nFit a Regularized Extreme Learner.\n\nFor more details see:      Li, Guoqiang, and Peifeng Niu. \"An enhanced extreme learning machine based on ridge      regression for regression.\" Neural Computing and Applications 22, no. 3 (2013):      803-810.\n\nExamples julia-repl julia> m1 = RegularizedExtremeLearner(x, y, 10, σ)  Regularized Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit!(m1)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.Models.predict","page":"API","title":"CausalELM.Models.predict","text":"predict(model, X)\n\nUse an ExtremeLearningMachine to make predictions.\n\nFor more details see:      Huang G-B, Zhu Q-Y, Siew C. Extreme learning machine: theory and applications.      Neurocomputing. 2006;70:489–501. https://doi.org/10.1016/j.neucom.2005.12.126\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit(m1, sigmoid)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]  julia> predict(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])  [9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.Models.predictcounterfactual!","page":"API","title":"CausalELM.Models.predictcounterfactual!","text":"predictcounterfactual(model, X)\n\nUse an ExtremeLearningMachine to predict the counterfactual.\n\nThis should be run with the observed covariates. To use synthtic data for what-if      scenarios use predict.\n\nSee also 'predict'.\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit(m1, sigmoid)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]  julia> predictcounterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])  [9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.Models.placebotest","page":"API","title":"CausalELM.Models.placebotest","text":"placebotest(model)\n\nConduct a placebo test.\n\nThis method makes predictions for the post-event or post-treatment period using data  in the pre-event or pre-treatment period and the post-event or post-treament. If there is a statistically significant difference between these predictions the study design may be flawed. Due to the multitude of significance tests for time series data, this function returns the predictions but does not test for statistical significance.\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit(m1, sigmoid)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]  julia> predictcounterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])  [9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978]  julia> placebotest(m1)  ([9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978],  [0.5, 0.4, 0.3, 0.2])\n\n\n\n\n\n","category":"function"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"Below are some small examples for estimating causal quantities of interest with CausalELM. Regardless of the estimator, the workflow is the same; get some data, initialize an  estimator, estimate the causal effect of interest, and get a summary of the model.","category":"page"},{"location":"examples/#Event-Study-Estimation","page":"Examples","title":"Event Study Estimation","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Generate some data\nX₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\n\n# Initialize an event study estimator\nm1 = EventStudy(X₀, Y₀, X₁, Y₁)\n\n# Estimate the average treatment effect\n# We can also estimate the ATT of ITE\nestimatecausaleffect!(m1)\n\n# Get a summary\nsummarize(m1)","category":"page"},{"location":"examples/#G-Computation","page":"Examples","title":"G-Computation","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Create some data with a binary treatment\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\n\n# Initialize a model\nm1 = GComputation(X, Y, T)\n\n# Estimate the ATE\n# Note that we could also estimate the ATT or ITE\nestimatecausaleffect!(m1)\n\n# Get a summary\nsummarize(m1)","category":"page"},{"location":"examples/#Doubly-Robust-Estimation","page":"Examples","title":"Doubly Robust Estimation","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Create some data with a binary treatment\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\n\n# Initialize a model\nm1 = DoublyRobust(X, Y, T)\n\n# Estimate the ATE\n# Note that we could also estimate the ATT or ITE\nestimatecausaleffect!(m1)\n\n# Get a summary\nsummarize(m1)","category":"page"},{"location":"examples/#S-Learning","page":"Examples","title":"S-Learning","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Generate data\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\n\n# Initialize an S-Learner\nm1 = SLearner(X, Y, T)\n\n# Estimate the CATE\nestimatecausaleffect!(m1)\n\n# Get a summary\nsummarize(m1)","category":"page"},{"location":"examples/#T-Learning","page":"Examples","title":"T-Learning","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Generate data\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\n\n# Initialize an S-Learner\nm1 = TLearner(X, Y, T)\n\n# Estimate the CATE\nestimatecausaleffect!(m1)\n\n# Get a summary\nsummarize(m1)","category":"page"},{"location":"examples/#X-Learning","page":"Examples","title":"X-Learning","text":"","category":"section"},{"location":"examples/","page":"Examples","title":"Examples","text":"# Generate data\nX, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\n\n# Initialize an S-Learner\nm1 = XLearner(X, Y, T)\n\n# Estimate the CATE\nestimatecausaleffect!(m1)\n\n# Get a summary\nsummarize(m1)","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CurrentModule = CausalELM","category":"page"},{"location":"#Overview","page":"CausalELM","title":"Overview","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM enables Estimation of causal quantities of interest in research designs where a  counterfactual must be predicted and compared to the observed outcomes. More specifically,  CausalELM provides structs and methods to execute event study designs (interupted time  series analysis), G-Computation, and doubly robust estimation as well as estimation of the  CATE via S-Learning, T-Learning, and X-Learning. In all of these implementations, CausalELM  predicts the counterfactuals using an Extreme Learning Machine. In this context, ELMs strike a good balance between prediction accuracy, generalization, ease of implementation, speed,  and interpretability. In addition, CausalELM provides the ability to incorporate an L2  penalty.","category":"page"},{"location":"#Installation","page":"CausalELM","title":"Installation","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Pkg.add(\"CausalELM\")","category":"page"},{"location":"#Estimating-Causal-Effects","page":"CausalELM","title":"Estimating Causal Effects","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"\nusing CausalELM\n\n# 1000 data points with 5 features in pre-event period\nx0 = rand(1000, 5)\n\n# Pre-event outcome\ny0 = rand(1000)\n\n# 200 data points in the post-event period\nx1 = rand(200, 5)\n\n# Pose-event outcome\ny1 = rand(200)\n\n# Instantiate an EventStudy struct\nevent_study = EventStudy(x0, y0, x1, y1)\n\nestimatecausaleffect!(event_study)\n\nsummarize(event_study)","category":"page"}]
}
