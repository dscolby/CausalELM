<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · CausalELM</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://dscolby.github.io/CausalELM.jl/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.jpg" alt="CausalELM logo"/></a><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">CausalELM</a></li><li><span class="tocitem">Getting Started</span><ul><li><a class="tocitem" href="../guide/estimatorselection/">Deciding Which Estimator to Use</a></li><li><a class="tocitem" href="../guide/its/">Interrupted Time Series Estimation</a></li><li><a class="tocitem" href="../guide/gcomputation/">G-computation</a></li><li><a class="tocitem" href="../guide/doublemachinelearning/">Double Machine Learning</a></li><li><a class="tocitem" href="../guide/metalearners/">Metalearners</a></li><li><a class="tocitem" href="../guide/doublyrobust/">Doubly Robust Estimation</a></li></ul></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Types"><span>Types</span></a></li><li><a class="tocitem" href="#Activation-Functions"><span>Activation Functions</span></a></li><li><a class="tocitem" href="#Cross-Validation"><span>Cross Validation</span></a></li><li><a class="tocitem" href="#Average-Causal-Effect-Estimators"><span>Average Causal Effect Estimators</span></a></li><li><a class="tocitem" href="#Metalearners"><span>Metalearners</span></a></li><li><a class="tocitem" href="#Common-Methods"><span>Common Methods</span></a></li><li><a class="tocitem" href="#Inference"><span>Inference</span></a></li><li><a class="tocitem" href="#Model-Validation"><span>Model Validation</span></a></li><li><a class="tocitem" href="#Validation-Metrics"><span>Validation Metrics</span></a></li><li><a class="tocitem" href="#Extreme-Learning-Machines"><span>Extreme Learning Machines</span></a></li><li><a class="tocitem" href="#Utility-Functions"><span>Utility Functions</span></a></li></ul></li><li><a class="tocitem" href="../contributing/">Contributing</a></li><li><a class="tocitem" href="../release_notes/">Release Notes</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/dscolby/CausalELM.jl/blob/main/docs/src/api.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="CausalELM"><a class="docs-heading-anchor" href="#CausalELM">CausalELM</a><a id="CausalELM-1"></a><a class="docs-heading-anchor-permalink" href="#CausalELM" title="Permalink"></a></h1><p>Most of the methods and structs here are private, not exported, should not be called by the  user, and are documented for the purpose of developing CausalELM or to facilitate  understanding of the implementation.</p><h2 id="Types"><a class="docs-heading-anchor" href="#Types">Types</a><a id="Types-1"></a><a class="docs-heading-anchor-permalink" href="#Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.InterruptedTimeSeries" href="#CausalELM.InterruptedTimeSeries"><code>CausalELM.InterruptedTimeSeries</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InterruptedTimeSeries(X₀, Y₀, X₁, Y₁; kwargs...)</code></pre><p>Initialize an interrupted time series estimator. </p><p><strong>Arguments</strong></p><ul><li><code>X₀::Any</code>: an array or DataFrame of covariates from the pre-treatment period.</li><li><code>Y₁::Any</code>: an array or DataFrame of outcomes from the pre-treatment period.</li><li><code>X₁::Any</code>: an array or DataFrame of covariates from the post-treatment period.</li><li><code>Y₁::Any</code>: an array or DataFrame of outcomes from the post-treatment period.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li></ul><p><strong>Keywords</strong></p><ul><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Real</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Real</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Real</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Real</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Real</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For a simple linear regression-based tutorial on interrupted time series analysis see:     Bernal, James Lopez, Steven Cummins, and Antonio Gasparrini. &quot;Interrupted time series      regression for the evaluation of public health interventions: a tutorial.&quot; International      journal of epidemiology 46, no. 1 (2017): 348-355.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)
julia&gt; m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)
julia&gt; m2 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁; regularized=false)
julia&gt; x₀_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100))
julia&gt; y₀_df = DataFrame(y=rand(100))
julia&gt; x₁_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100)) 
julia&gt; y₁_df = DataFrame(y=rand(100))
julia&gt; m3 = InterruptedTimeSeries(x₀_df, y₀_df, x₁_df, y₁_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/estimators.jl#L4-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.GComputation" href="#CausalELM.GComputation"><code>CausalELM.GComputation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GComputation(X, T, Y; kwargs...)</code></pre><p>Initialize a G-Computation estimator.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>quantity_of_interest::String</code>: ATE for average treatment effect or ATT for average    treatment effect on the treated.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross    validation.</li><li>`min_neurons::Real: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Real</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Real</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Real</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Real</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For a good overview of G-Computation see:     Chatton, Arthur, Florent Le Borgne, Clémence Leyrat, Florence Gillaizeau, Chloé      Rousseau, Laetitia Barbin, David Laplaud, Maxime Léger, Bruno Giraudeau, and Yohann      Foucher. &quot;G-computation, propensity score-based methods, and targeted maximum likelihood      estimator for causal inference with different covariates sets: a comparative simulation      study.&quot; Scientific reports 10, no. 1 (2020): 9219.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), rand(100), [rand()&lt;0.4 for i in 1:100]
julia&gt; m1 = GComputation(X, T, Y)
julia&gt; m2 = GComputation(X, T, Y; task=&quot;regression&quot;)
julia&gt; m3 = GComputation(X, T, Y; task=&quot;regression&quot;, quantity_of_interest=&quot;ATE)

julia&gt; x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
julia&gt; t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100)) 
julia&gt; m5 = GComputation(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/estimators.jl#L107-L162">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.DoubleMachineLearning" href="#CausalELM.DoubleMachineLearning"><code>CausalELM.DoubleMachineLearning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DoubleMachineLearning(X, T, Y; kwargs...)</code></pre><p>Initialize a double machine learning estimator with cross fitting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates of interest.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>W::Any</code>: an array or dataframe of all possible confounders.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Real</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Real</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Real</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Real</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Real</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p>Unlike other estimators, this method does not support time series or panel data. This method  also does not work as well with smaller datasets because it estimates separate outcome  models for the treatment and control groups.</p><p><strong>References</strong></p><p>For more information see:     Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,      Whitney Newey, and James Robins. &quot;Double/debiased machine learning for treatment and      structural parameters.&quot; (2016): C1-C68.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = DoubleMachineLearning(X, T, Y)
julia&gt; m2 = DoubleMachineLearning(X, T, Y; task=&quot;regression&quot;)

julia&gt; x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
julia&gt; t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
julia&gt; m3 = DoubleMachineLearning(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/estimators.jl#L213-L267">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.SLearner" href="#CausalELM.SLearner"><code>CausalELM.SLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SLearner(X, T, Y; kwargs...)</code></pre><p>Initialize a S-Learner.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Real</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Real</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Real</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Real</code>: the number of iterations to perform cross validation between </li></ul><p>min<em>neurons and max</em>neurons.</p><ul><li><code>approximator_neurons::Real</code>: the number of nuerons in the validation loss approximator </li></ul><p>network.</p><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as  in the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For an overview of S-Learners and other metalearners see: Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for  estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of  the national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p>For details and a derivation of the generalized cross validation estimator see: Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a  method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979):  215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = SLearner(X, T, Y)
julia&gt; m2 = SLearner(X, T, Y; task=&quot;regression&quot;)
julia&gt; m3 = SLearner(X, T, Y; task=&quot;regression&quot;, regularized=true)

julia&gt; x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
julia&gt; t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
julia&gt; m4 = SLearner(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L4-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.TLearner" href="#CausalELM.TLearner"><code>CausalELM.TLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TLearner(X, T, Y; kwargs...)</code></pre><p>Initialize a T-Learner.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross </li></ul><p>validation.</p><ul><li><code>min_neurons::Real</code>: the minimum number of neurons to consider for the extreme </li></ul><p>learner.</p><ul><li><code>max_neurons::Real</code>: the maximum number of neurons to consider for the extreme </li></ul><p>learner.</p><ul><li><code>folds::Real</code>: the number of cross validation folds to find the best number of </li></ul><p>neurons.</p><ul><li><code>iterations::Real</code>: the number of iterations to perform cross validation between </li></ul><p>min<em>neurons and max</em>neurons.</p><ul><li><code>approximator_neurons::Real</code>: the number of nuerons in the validation loss approximator </li></ul><p>network.</p><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as  in the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For an overview of T-Learners and other metalearners see: Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for  estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of  the national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p>For details and a derivation of the generalized cross validation estimator see: Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a  method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979):  215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = TLearner(X, T, Y)
julia&gt; m2 = TLearner(X, T, Y; task=&quot;regression&quot;)
julia&gt; m3 = TLearner(X, T, Y; task=&quot;regression&quot;, regularized=true)

julia&gt; x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
julia&gt; t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
julia&gt; m4 = TLearner(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L100-L154">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.RLearner" href="#CausalELM.RLearner"><code>CausalELM.RLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RLearner(X, T, Y; kwargs...)</code></pre><p>Initialize an R-Learner.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates of interest.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>W::Any</code> : an array of all possible confounders.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Real</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Real</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Real</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Real</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Real</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For an explanation of R-Learner estimation see:     Nie, Xinkun, and Stefan Wager. &quot;Quasi-oracle estimation of heterogeneous treatment      effects.&quot; Biometrika 108, no. 2 (2021): 299-319.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = RLearner(X, T, Y)
julia&gt; m2 = RLearner(X, T, Y; t_cat=true)

julia&gt; x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
julia&gt; t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
julia&gt; m4 = RLearner(x_df, t_df, y_df)

julia&gt; w = rand(100, 6)
julia&gt; m5 = RLearner(X, T, Y, W=w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L303-L354">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.CausalEstimator" href="#CausalELM.CausalEstimator"><code>CausalELM.CausalEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Abstract type for GComputation and DoubleMachineLearning</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/estimators.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Metalearner" href="#CausalELM.Metalearner"><code>CausalELM.Metalearner</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Abstract type for metalearners</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.ExtremeLearningMachine" href="#CausalELM.ExtremeLearningMachine"><code>CausalELM.ExtremeLearningMachine</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Abstract type that includes vanilla and L2 regularized Extreme Learning Machines</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L3">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.ExtremeLearner" href="#CausalELM.ExtremeLearner"><code>CausalELM.ExtremeLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ExtremeLearner(X, Y, hidden_neurons, activation)</code></pre><p>Construct an ExtremeLearner for fitting and prediction.</p><p><strong>Notes</strong></p><p>While it is possible to use an ExtremeLearner for regression, it is recommended to use  RegularizedExtremeLearner, which imposes an L2 penalty, to reduce multicollinearity.</p><p><strong>References</strong></p><p>For more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. &quot;Extreme learning machine: theory      and applications.&quot; Neurocomputing 70, no. 1-3 (2006): 489-501.</p><p>See also <a href="@ref">&#39;RegularizedExtremeLearner&#39;</a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
julia&gt; m1 = ExtremeLearner(x, y, 10, σ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L6-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.RegularizedExtremeLearner" href="#CausalELM.RegularizedExtremeLearner"><code>CausalELM.RegularizedExtremeLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RegularizedExtremeLearner(X, Y, hidden_neurons, activation)</code></pre><p>Construct a RegularizedExtremeLearner for fitting and prediction.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
julia&gt; m1 = RegularizedExtremeLearner(x, y, 10, σ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L47-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Nonbinary" href="#CausalELM.Nonbinary"><code>CausalELM.Nonbinary</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Abstract type used to dispatch risk_ratio on nonbinary treatments</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Binary" href="#CausalELM.Binary"><code>CausalELM.Binary</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Type used to dispatch risk_ratio on binary treatments</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Count" href="#CausalELM.Count"><code>CausalELM.Count</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Type used to dispatch risk_ratio on count treatments</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Continuous" href="#CausalELM.Continuous"><code>CausalELM.Continuous</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Type used to dispatch risk_ratio on continuous treatments</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L10">source</a></section></article><h2 id="Activation-Functions"><a class="docs-heading-anchor" href="#Activation-Functions">Activation Functions</a><a id="Activation-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Activation-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.binary_step" href="#CausalELM.binary_step"><code>CausalELM.binary_step</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">binary_step(x)</code></pre><p>Apply the binary step activation function.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; binary_step(1)
1

julia&gt; binary_step([-1000, 100, 1, 0, -0.001, -3])
6-element Vector{Int64}:
 0
 1
 1
 1
 0
 0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.σ" href="#CausalELM.σ"><code>CausalELM.σ</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">σ(x)</code></pre><p>Apply the sigmoid activation function.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; σ(1)
0.7310585786300049

julia&gt; σ([1.0, 0.0])
2-element Vector{Float64}:
 0.7310585786300049
 0.5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L25-L40">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>tanh</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="CausalELM.relu" href="#CausalELM.relu"><code>CausalELM.relu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">relu(x)</code></pre><p>Apply the ReLU activation function.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; relu(1)
1

julia&gt; relu([1.0, 0.0, -1.0])
3-element Vector{Float64}:
 1.0
 0.0
 0.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L63-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.leaky_relu" href="#CausalELM.leaky_relu"><code>CausalELM.leaky_relu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">leaky_relu(x)</code></pre><p>Apply the leaky ReLU activation function to a number.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; leaky_relu(1)
1

julia&gt; leaky_relu([-1.0, 0.0, 1.0])
3-element Vector{Float64}:
 -0.01
  0.0
  1.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L84-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.swish" href="#CausalELM.swish"><code>CausalELM.swish</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">swish(x)</code></pre><p>Apply the swish activation function to a number.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; swish(1)
0.7310585786300049

julia&gt; swish([1.0, -1.0])
2-element Vector{Float64}:
  0.7310585786300049
 -0.2689414213699951</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L105-L120">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.softmax" href="#CausalELM.softmax"><code>CausalELM.softmax</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">softmax(x)</code></pre><p>Apply the softmax activation function to a number.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; softmax(1)
1.0

julia&gt; softmax([1.0, 2.0, 3.0])
3-element Vector{Float64}:
 0.09003057317038045
 0.24472847105479764
 0.6652409557748219

julia&gt; softmax([1.0 2.0 3.0; 4.0 5.0 6.0])
2×3 Matrix{Float64}:
 0.0900306  0.244728  0.665241
 0.0900306  0.244728  0.665241</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L125-L146">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.softplus" href="#CausalELM.softplus"><code>CausalELM.softplus</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">softplus(x)</code></pre><p>Apply the softplus activation function to a number.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; softplus(1)
1.3132616875182228

julia&gt; softplus([1.0, -1.0])
2-element Vector{Float64}:
 1.3132616875182228
 0.3132616875182228</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L153-L168">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.gelu" href="#CausalELM.gelu"><code>CausalELM.gelu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gelu(x)</code></pre><p>Apply the GeLU activation function to a number.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; gelu(1)
0.8411919906082768

julia&gt; gelu([-1.0, 0.0])
2-element Vector{Float64}:
 -0.15880800939172324
  0.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L173-L188">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.gaussian" href="#CausalELM.gaussian"><code>CausalELM.gaussian</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gaussian(x)</code></pre><p>Apply the gaussian activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; gaussian(1)
0.36787944117144233

julia&gt; gaussian([1.0, -1.0])
2-element Vector{Float64}:
 0.3678794411714423
 0.3678794411714423</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L193-L208">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.hard_tanh" href="#CausalELM.hard_tanh"><code>CausalELM.hard_tanh</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">hard_tanh(x)</code></pre><p>Apply the hard_tanh activation function to a number.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; hard_tanh(-2)
-1

julia&gt; hard_tanh([-2.0, 0.0, 2.0])
3-element Vector{Real}:
 -1
  0.0
  1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L213-L229">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.elish" href="#CausalELM.elish"><code>CausalELM.elish</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">elish(x)</code></pre><p>Apply the ELiSH activation function to a number.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; elish(1)
0.7310585786300049

julia&gt; elish([-1.0, 1.0])
2-element Vector{Float64}:
 -0.17000340156854793
  0.7310585786300049</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L242-L257">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.fourier" href="#CausalELM.fourier"><code>CausalELM.fourier</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fourrier(x)</code></pre><p>Apply the Fourier activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; fourier(1)
0.8414709848078965

julia&gt; fourier([-1.0, 1.0])
2-element Vector{Float64}:
 -0.8414709848078965
  0.8414709848078965</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/activation.jl#L262-L277">source</a></section></article><h2 id="Cross-Validation"><a class="docs-heading-anchor" href="#Cross-Validation">Cross Validation</a><a id="Cross-Validation-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-Validation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.generate_folds" href="#CausalELM.generate_folds"><code>CausalELM.generate_folds</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generate_folds(X, Y, folds)</code></pre><p>Create folds for cross validation.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; xfolds, y_folds = CausalELM.generate_folds(zeros(4, 2), zeros(4), 2)
([[0.0 0.0], [0.0 0.0; 0.0 0.0; 0.0 0.0]], [[0.0], [0.0, 0.0, 0.0]])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/crossval.jl#L3-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.generate_temporal_folds" href="#CausalELM.generate_temporal_folds"><code>CausalELM.generate_temporal_folds</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generate_temporal_folds(X, Y, folds)</code></pre><p>Create rolling folds for cross validation of time series data.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; xfolds, yfolds = CausalELM.generate_temporal_folds([1 1; 1 1; 0 0; 0 0], zeros(4), 2)
([[1 1; 1 1], [1 1; 1 1; 0 0; 0 0]], [[0.0, 0.0], [0.0, 0.0, 0.0, 0.0]])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/crossval.jl#L38-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.validation_loss" href="#CausalELM.validation_loss"><code>CausalELM.validation_loss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">validation_loss(xtrain, ytrain, xtest, ytest, nodes, metric; kwargs...)</code></pre><p>Calculate a validation metric for a single fold in k-fold cross validation.</p><p><strong>Arguments</strong></p><ul><li><code>xtrain::Any</code>: an array of features to train on.</li><li><code>ytrain::Any</code>: an array of training labels.</li><li><code>xtest::Any</code>: an array of features to test on.</li><li><code>ytrain::Any</code>: an array of testing labels.</li><li><code>nodes::Int</code>: the number of neurons in the extreme learning machine.</li><li><code>metric::Function</code>: the validation metric to calculate.</li></ul><p><strong>Keywords</strong></p><ul><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x = rand(100, 5); y = Float64.(rand(100) .&gt; 0.5)
julia&gt; validation_loss(x, y, 5, accuracy, 3)
0.5402532843396273</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/crossval.jl#L66-L89">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.cross_validate" href="#CausalELM.cross_validate"><code>CausalELM.cross_validate</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">cross_validate(X, Y, neurons, metric, activation, regularized, folds, temporal)</code></pre><p>Calculate a validation metric for k folds using a single set of hyperparameters.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array</code>: array of features to train on.</li><li><code>Y::Vector</code>: vector of labels to train on.</li><li><code>neurons::Int</code>: number of neurons to use in the extreme learning machine.</li><li><code>metric::Function</code>: validation metric to calculate.</li><li><code>activation::Function=relu</code>: activation function to use.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>folds::Int</code>: number of folds to use for cross validation.</li><li><code>temporal::Function=true</code>: whether the data is of a time series or panel nature.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x = rand(100, 5); y = Float64.(rand(100) .&gt; 0.5)
julia&gt; cross_validate(x, y, 5, accuracy)
0.8891028047100136</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/crossval.jl#L105-L126">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.best_size" href="#CausalELM.best_size"><code>CausalELM.best_size</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">best_size(m)</code></pre><p>Compute the best number of neurons for an estimator.</p><p><strong>Notes</strong></p><p>The procedure tests networks with numbers of neurons in a sequence whose length is given  by iterations on the interval [min<em>neurons, max</em>neurons]. Then, it uses the networks  sizes and validation errors from the sequence to predict the validation error or metric  for every network size between min<em>neurons and max</em>neurons using the function  approximation ability of an Extreme Learning Machine. Finally, it returns the network  size with the best predicted validation error or metric.</p><p><strong>Arguments</strong></p><ul><li><code>m::Any</code>: estimator to find the best number of neurons for.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y = rand(100, 5), rand(0:1, 100), rand(100)
julia&gt; m1 = GComputation(X, T, y)
julia&gt; best_size(m1)
8</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/crossval.jl#L159-L182">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.shuffle_data" href="#CausalELM.shuffle_data"><code>CausalELM.shuffle_data</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">shuffle_data(X, Y)</code></pre><p>Shuffles covariates and outcome vector for cross validation.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; shuffle_data([1 1; 2 2; 3 3; 4 4], collect(1:4))
([4 4; 2 2; 1 1; 3 3], [4, 2, 1, 3])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/crossval.jl#L211-L221">source</a></section></article><h2 id="Average-Causal-Effect-Estimators"><a class="docs-heading-anchor" href="#Average-Causal-Effect-Estimators">Average Causal Effect Estimators</a><a id="Average-Causal-Effect-Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Average-Causal-Effect-Estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.XLearner" href="#CausalELM.XLearner"><code>CausalELM.XLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">XLearner(X, T, Y; kwargs...)</code></pre><p>Initialize an X-Learner.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross </li></ul><p>validation.</p><ul><li><code>min_neurons::Real</code>: the minimum number of neurons to consider for the extreme </li></ul><p>learner.</p><ul><li><code>max_neurons::Real</code>: the maximum number of neurons to consider for the extreme </li></ul><p>learner.</p><ul><li><code>folds::Real</code>: the number of cross validation folds to find the best number of </li></ul><p>neurons.</p><ul><li><code>iterations::Real</code>: the number of iterations to perform cross validation between </li></ul><p>min<em>neurons and max</em>neurons.</p><ul><li><code>approximator_neurons::Real</code>: the number of nuerons in the validation loss </li></ul><p>approximator network.</p><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as  in the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For an overview of X-Learners and other metalearners see: Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for  estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of  the national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p>For details and a derivation of the generalized cross validation estimator see: Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a  method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979):  215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = XLearner(X, T, Y)
julia&gt; m2 = XLearner(X, T, Y; task=&quot;regression&quot;)
julia&gt; m3 = XLearner(X, T, Y; task=&quot;regression&quot;, regularized=true)

julia&gt; x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
julia&gt; t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
julia&gt; m4 = XLearner(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L201-L255">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.estimate_effect!</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="CausalELM.predict_residuals" href="#CausalELM.predict_residuals"><code>CausalELM.predict_residuals</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predict_residuals(D, x_train, x_test, y_train, y_test, t_train, t_test)</code></pre><p>Predict treatment and outcome residuals for double machine learning or R-learning.</p><p><strong>Notes</strong></p><p>This method should not be called directly.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; x_train, x_test = X[1:80, :], X[81:end, :]
julia&gt; y_train, y_test = Y[1:80], Y[81:end]
julia&gt; t_train, t_test = T[1:80], T[81:100]
julia&gt; m1 = DoubleMachineLearning(X, T, Y)
julia&gt; predict_residuals(m1, x_train, x_test, y_train, y_test, t_train, t_test)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/estimators.jl#L455-L472">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.moving_average" href="#CausalELM.moving_average"><code>CausalELM.moving_average</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">moving_average(x)</code></pre><p>Calculates a cumulative moving average.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; moving_average([1, 2, 3])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/estimators.jl#L520-L529">source</a></section></article><h2 id="Metalearners"><a class="docs-heading-anchor" href="#Metalearners">Metalearners</a><a id="Metalearners-1"></a><a class="docs-heading-anchor-permalink" href="#Metalearners" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.stage1!" href="#CausalELM.stage1!"><code>CausalELM.stage1!</code></a> — <span class="docstring-category">Function</span></header><section><div><p>stage1!(x)</p><p>Estimate the first stage models for an X-learner.</p><p>This method should not be called by the user.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = XLearner(X, T, Y)
julia&gt; stage1!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L733-L746">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.stage2!" href="#CausalELM.stage2!"><code>CausalELM.stage2!</code></a> — <span class="docstring-category">Function</span></header><section><div><p>stage2!(x)</p><p>Estimate the second stage models for an X-learner.</p><p>This method should not be called by the user.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = XLearner(X, T, Y)
julia&gt; stage1!(m1)
julia&gt; stage2!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L775-L789">source</a></section></article><h2 id="Common-Methods"><a class="docs-heading-anchor" href="#Common-Methods">Common Methods</a><a id="Common-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Common-Methods" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.estimate_causal_effect!" href="#CausalELM.estimate_causal_effect!"><code>CausalELM.estimate_causal_effect!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">estimate_causal_effect!(its)</code></pre><p>Estimate the effect of an event relative to a predicted counterfactual.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)
julia&gt; m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)
julia&gt; estimate_causal_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/estimators.jl#L314-L325">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(DML)</code></pre><p>Estimate a causal effect of interest using double machine learning.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = DoubleMachineLearning(X, T, Y)
julia&gt; estimate_causal_effect!(m1)

julia&gt; W = rand(100, 6)
julia&gt; m2 = DoubleMachineLearning(X, T, Y, W=W)
julia&gt; estimate_causal_effect!(m2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/estimators.jl#L393-L408">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(s)</code></pre><p>Estimate the CATE using an S-learner.</p><p>For an overview of S-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for      estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m4 = SLearner(X, T, Y)
julia&gt; estimate_causal_effect!(m4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L497-L513">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(t)</code></pre><p>Estimate the CATE using an T-learner.</p><p>For an overview of T-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for      estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m5 = TLearner(X, T, Y)
julia&gt; estimate_causal_effect!(m5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L519-L535">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(x)</code></pre><p>Estimate the CATE using an X-learner.</p><p>For an overview of X-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for      estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = XLearner(X, T, Y)
julia&gt; estimate_causal_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L560-L576">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(R)</code></pre><p>Estimate the CATE using an R-learner.</p><p>For an overview of R-learning see:     Nie, Xinkun, and Stefan Wager. &quot;Quasi-oracle estimation of heterogeneous treatment      effects.&quot; Biometrika 108, no. 2 (2021): 299-319.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = RLearner(X, T, Y)
julia&gt; estimate_causal_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L593-L608">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(DRE)</code></pre><p>Estimate the CATE using a doubly robust learner.</p><p>For details on how this method estimates the CATE see:     Kennedy, Edward H. &quot;Towards optimal doubly robust estimation of heterogeneous causal      effects.&quot; Electronic Journal of Statistics 17, no. 2 (2023): 3008-3049.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = DoublyRobustLearner(X, T, Y)
julia&gt; estimate_causal_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metalearners.jl#L644-L659">source</a></section></article><h2 id="Inference"><a class="docs-heading-anchor" href="#Inference">Inference</a><a id="Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Inference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.summarize" href="#CausalELM.summarize"><code>CausalELM.summarize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">summarize(mod, n)</code></pre><p>Get a summary from a CausalEstimator or Metalearner.</p><p><strong>Arguments</strong></p><ul><li><code>mod::Union{CausalEstimator, Metalearner}</code>: a model to summarize.</li><li><code>n::Int=100</code>: the number of iterations to generate the numll distribution for    randomization inference.</li></ul><p><strong>Notes</strong></p><p>p-values and standard errors are estimated using approximate randomization inference.</p><p><strong>References</strong></p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X, T, Y = rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
julia&gt; m1 = GComputation(X, T, Y)
julia&gt; estimate_causal_effect!(m1)
julia&gt; summarize(m1)

julia&gt; m2 = RLearner(X, T, Y)
julia&gt; estimate_causal_effect(m2)
julia&gt; summarize(m2)

julia&gt; m3 = SLearner(X, T, Y)
julia&gt; estimate_causal_effect!(m3)
julia&gt; summarise(m3)  # British spelling works too!</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/inference.jl#L3-L35">source</a></section><section><div><pre><code class="nohighlight hljs">summarize(its, n, mean_effect)</code></pre><p>Get a summary from an interrupted time series estimator.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTimeSeries</code>: interrupted time series estimator</li><li><code>n::Int=100</code>: number of iterations to generate the numll distribution for randomization    inference.</li><li><code>mean_effect::Bool=true</code>: whether to estimate the mean or cumulative effect for an    interrupted time series estimator.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)
julia&gt; m4 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)
julia&gt; estimate_causal_effect!(m4)
julia&gt; summarize(m4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/inference.jl#L81-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.generate_null_distribution" href="#CausalELM.generate_null_distribution"><code>CausalELM.generate_null_distribution</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generate_null_distribution(mod, n)</code></pre><p>Generate a null distribution for the treatment effect of G-computation, double machine  learning, or metalearning.</p><p><strong>Arguments</strong></p><ul><li><code>mod::Any</code>: model to summarize.</li><li><code>n::Int=100</code>: number of iterations to generate the null distribution for randomization    inference.</li></ul><p><strong>Notes</strong></p><p>This method estimates the same model that is provided using random permutations of the  treatment assignment to generate a vector of estimated effects under different treatment regimes. When mod is a metalearner the null statistic is the difference is the ATE.</p><p>Note that lowering the number of iterations increases the probability of failing to reject the null hypothesis.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, t, y = rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(1:100, 100, 1)
julia&gt; g_computer = GComputation(x, t, y)
julia&gt; estimate_causal_effect!(g_computer)
julia&gt; generate_null_distribution(g_computer, 500)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/inference.jl#L142-L168">source</a></section><section><div><pre><code class="nohighlight hljs">generate_null_distribution(its, n, mean_effect)</code></pre><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTimeSeries</code>: interrupted time series estimator</li><li><code>n::Int=100</code>: number of iterations to generate the numll distribution for randomization    inference.</li><li><code>mean_effect::Bool=true</code>: whether to estimate the mean or cumulative effect for an    interrupted time series estimator.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)
julia&gt; its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
julia&gt; estimate_causal_effect!(its)
julia&gt; generate_null_distribution(its, 10)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/inference.jl#L190-L207">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.quantities_of_interest" href="#CausalELM.quantities_of_interest"><code>CausalELM.quantities_of_interest</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">quantities_of_interest(mod, n)</code></pre><p>Generate a p-value and standard error through randomization inference</p><p>This method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from the generated distribution.</p><p>Note that lowering the number of iterations increases the probability of failing to reject the null hypothesis.</p><p>For a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, t, y = rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(1:100, 100, 1)
julia&gt; g_computer = GComputation(x, t, y)
julia&gt; estimate_causal_effect!(g_computer)
julia&gt; quantities_of_interest(g_computer, 1000)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/inference.jl#L236-L258">source</a></section><section><div><pre><code class="nohighlight hljs">quantities_of_interest(mod, n)</code></pre><p>Generate a p-value and standard error through randomization inference</p><p>This method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from  the generated distribution. Randomization for event studies is done by creating time splits  at even intervals and reestimating the causal effect.</p><p>Note that lowering the number of iterations increases the probability of failing to reject the null hypothesis.</p><p>For a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)
julia&gt; its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
julia&gt; estimate_causal_effect!(its)
julia&gt; quantities_of_interest(its, 10)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/inference.jl#L271-L294">source</a></section></article><h2 id="Model-Validation"><a class="docs-heading-anchor" href="#Model-Validation">Model Validation</a><a id="Model-Validation-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Validation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.validate" href="#CausalELM.validate"><code>CausalELM.validate</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">validate(its; kwargs...)</code></pre><p>Test the validity of an estimated interrupted time series analysis.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTimeSeries</code>: an interrupted time seiries estimator.</li></ul><p><strong>Keywords</strong></p><ul><li><code>n::Int</code>: number of times to simulate a confounder.</li><li><code>low::Float64</code>=0.15: minimum proportion of data points to include before or after the    tested break in the Wald supremum test.</li><li><code>high::Float64=0.85</code>: maximum proportion of data points to include before or after the    tested break in the Wald supremum test.</li></ul><p><strong>Notes</strong></p><p>This method coducts a Chow Test, a Wald supremeum test, and tests the model&#39;s sensitivity to  confounders. The Chow Test tests for structural breaks in the covariates between the time  before and after the event. p-values represent the proportion of times the magnitude of the  break in a covariate would have been greater due to chance. Lower p-values suggest a higher  probability the event effected the covariates and they cannot provide unbiased  counterfactual predictions. The Wald supremum test finds the structural break with the  highest Wald statistic. If this is not the same as the hypothesized break, it could indicate  an anticipation effect, a confounding event, or that the intervention or policy took place  in multiple phases. p-values represent the proportion of times we would see a larger Wald  statistic if the data points were randomly allocated to pre and post-event periods for the  predicted structural break. Ideally, the hypothesized break will be the same as the  predicted break and it will also have a low p-value. The omitted predictors test adds  normal random variables with uniform noise as predictors. If the included covariates are  good predictors of the counterfactual outcome, adding irrelevant predictors should not have  a large effect on the predicted counterfactual outcomes or the estimated effect.</p><p>This method does not implement the second test in Baicker and Svoronos because the estimator  in this package models the relationship between covariates and the outcome and uses an  extreme learning machine instead of linear regression, so variance in the outcome across  different bins is not much of an issue.</p><p><strong>References</strong></p><p>For more details on the assumptions and validity of interrupted time series designs, see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.</p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; X₀, Y₀, X₁, Y₁ = rand(100, 5), rand(100), rand(10, 5), rand(10)
julia&gt; m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)
julia&gt; estimate_causal_effect!(m1)
julia&gt; validate(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L35-L87">source</a></section><section><div><pre><code class="nohighlight hljs">validate(m; kwargs)</code></pre><p><strong>Arguments</strong></p><ul><li><code>m::Union{CausalEstimator, Metalearner}</code>: model to validate/test the assumptions of.</li></ul><p><strong>Keywords</strong></p><ul><li><code>devs=::Any</code>: iterable of deviations from which to generate noise to simulate violations    of the counterfactual consistency assumption.</li><li><code>num_iterations=10::Int: number of times to simulate a violation of the counterfactual    consistency assumption.</code></li><li><code>min::Float64</code>=1.0e-6: minimum probability of treatment for the positivity assumption.</li><li><code>high::Float64=1-min</code>: maximum probability of treatment for the positivity assumption.</li></ul><p><strong>Notes</strong></p><p>This method tests the counterfactual consistency, exchangeability, and positivity  assumptions required for causal inference. It should be noted that consistency and  exchangeability are not directly testable, so instead, these tests do not provide definitive  evidence of a violation of these assumptions. To probe the counterfactual consistency  assumption, we simulate counterfactual outcomes that are different from the observed  outcomes, estimate models with the simulated counterfactual outcomes, and take the averages. If the outcome is continuous, the noise for the simulated counterfactuals is drawn from  N(0, dev) for each element in devs, otherwise the default is 0.25, 0.5, 0.75, and 1.0  standard deviations from the mean outcome. For discrete variables, each outcome is replaced  with a different value in the range of outcomes with probability ϵ for each ϵ in devs,  otherwise the default is 0.025, 0.05, 0.075, 0.1. If the average estimate for a given level  of violation differs greatly from the effect estimated on the actual data, then the model is  very sensitive to violations of the counterfactual consistency assumption for that level of  violation. Next, this methods tests the model&#39;s sensitivity to a violation of the  exchangeability assumption by calculating the E-value, which is the minimum strength of  association, on the risk ratio scale, that an unobserved confounder would need to have with  the treatment and outcome variable to fully explain away the estimated effect. Thus, higher  E-values imply the model is more robust to a violation of the exchangeability assumption.  Finally, this method tests the positivity assumption by estimating propensity scores. Rows  in the matrix are levels of covariates that have a zero probability of treatment. If the  matrix is empty, none of the observations have an estimated zero probability of treatment,  which implies the positivity assumption is satisfied.</p><p><strong>References</strong></p><p>For a thorough review of casual inference assumptions see:     Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and      Francis, 2024. </p><p>For more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. &quot;Sensitivity analysis in observational research:      introducing the E-value.&quot; Annals of internal medicine 167, no. 4 (2017): 268-274.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, t, y = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100]), vec(rand(1:100, 100, 1)) 
julia&gt; g_computer = GComputation(x, t, y, temporal=false)
julia&gt; estimate_causal_effect!(g_computer)
julia&gt; validate(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L98-L152">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.covariate_independence" href="#CausalELM.covariate_independence"><code>CausalELM.covariate_independence</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">covariate_independence(its; kwargs..)</code></pre><p>Test for independence between covariates and the event or intervention.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTImeSeries</code>: an interrupted time seiries estimator.</li></ul><p><strong>Keywords</strong></p><ul><li><code>n::Int</code>: number of permutations for assigning observations to the pre and        post-treatment periods.</li></ul><p>This is a Chow Test for covariates with p-values estimated via randomization inference,  which does not assume a distribution for the outcome variable. The p-values are the  proportion of times randomly assigning observations to the pre or post-intervention period  would have a larger estimated effect on the the slope of the covariates. The lower the  p-values, the more likely it is that the event/intervention effected the covariates and  they cannot provide an unbiased prediction of the counterfactual outcomes.</p><p>For more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.</p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), 
       randn(10))
julia&gt; its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
julia&gt; estimate_causal_effect!(its)
julia&gt; covariate_independence(its)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L172-L206">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.omitted_predictor" href="#CausalELM.omitted_predictor"><code>CausalELM.omitted_predictor</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">omitted_predictor(its; kwargs...)</code></pre><p>See how an omitted predictor/variable could change the results of an interrupted time series  analysis.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTImeSeries</code>: interrupted time seiries estimator.</li></ul><p><strong>Keywords</strong></p><ul><li><code>n::Int</code>: number of times to simulate a confounder.</li></ul><p><strong>Notes</strong></p><p>This method reestimates interrupted time series models with uniform random variables. If the  included covariates are good predictors of the counterfactual outcome, adding a random  variable as a covariate should not have a large effect on the predicted counterfactual  outcomes and therefore the estimated average effect.</p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), randn(10))
julia&gt; its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
julia&gt; estimate_causal_effect!(its)
julia&gt; omitted_predictor(its)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L224-L252">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.sup_wald" href="#CausalELM.sup_wald"><code>CausalELM.sup_wald</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sup_wald(its; kwargs)</code></pre><p>Check if the predicted structural break is the hypothesized structural break.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTimeSeries</code>: interrupted time seiries estimator.</li></ul><p><strong>Keywords</strong></p><ul><li><code>n::Int</code>: number of times to simulate a confounder.</li><li><code>low::Float64</code>=0.15: minimum proportion of data points to include before or after the        tested break in the Wald supremum test.</li><li><code>high::Float64=0.85</code>: maximum proportion of data points to include before or after the        tested break in the Wald supremum test.</li></ul><p><strong>Notes</strong></p><p>This method conducts Wald tests and identifies the structural break with the highest Wald  statistic. If this break is not the same as the hypothesized break, it could indicate an  anticipation effect, confounding by some other event or intervention, or that the  intervention or policy took place in multiple phases. p-values are estimated using  approximate randomization inference and represent the proportion of times we would see a  larger Wald statistic if the data points were randomly allocated to pre and post-event  periods for the predicted structural break.</p><p><strong>References</strong></p><p>For more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.</p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), 
       randn(10))
julia&gt; its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
julia&gt; estimate_causal_effect!(its)
julia&gt; sup_wald(its)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L282-L322">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.p_val" href="#CausalELM.p_val"><code>CausalELM.p_val</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">p_val(x, y, β; kwargs...)</code></pre><p>Estimate the p-value for the hypothesis that an event had a statistically significant effect  on the slope of a covariate using randomization inference.</p><p><strong>Arguments</strong></p><ul><li><code>x::Array{&lt;:Real}</code>: covariates.</li><li><code>y::Array{&lt;:Real}</code>: outcome.</li><li><code>β::Array{&lt;:Real}</code>=0.15: fitted weights.</li></ul><p><strong>Keywords</strong></p><ul><li><code>two_sided::Bool=false</code>: whether to conduct a one-sided hypothesis test.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, y, β = reduce(hcat, (float(rand(0:1, 10)), ones(10))), rand(10), 0.5
julia&gt; p_val(x, y, β)
julia&gt; p_val(x, y, β; n=100, two_sided=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L352-L372">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.counterfactual_consistency" href="#CausalELM.counterfactual_consistency"><code>CausalELM.counterfactual_consistency</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">counterfactual_consistency(m; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>m::Union{CausalEstimator, Metalearner}</code>: model to validate/test the assumptions of.</li></ul><p><strong>Keywords</strong></p><ul><li><code>num_devs=(0.25, 0.5, 0.75, 1.0)::Tuple</code>: number of standard deviations from which to    generate noise from a normal distribution to simulate violations of the counterfactual    consistency assumption.</li><li><code>num_iterations=10::Int: number of times to simulate a violation of the counterfactual    consistency assumption.</code></li></ul><p><strong>Notes</strong></p><p>Examine the counterfactual consistency assumption. First, this function simulates  counterfactual outcomes that are offset from the outcomes in the dataset by random scalars drawn from a N(0, num<em>std</em>dev). Then, the procedure is repeated num<em>iterations times and  averaged. If the model is a metalearner, then the estimated individual treatment effects  are averaged and the mean CATE is averaged over all the iterations, otherwise the estimated  treatment effect is averaged over the iterations. The previous steps are repeated for each  element in num</em>devs.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, t = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100]
julia&gt; y = vec(rand(1:100, 100, 1)))
julia&gt; g_computer = GComputation(x, t, y, temporal=false)
julia&gt; estimate_causal_effect!(g_computer)
julia&gt; counterfactual_consistency(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L394-L424">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.exchangeability" href="#CausalELM.exchangeability"><code>CausalELM.exchangeability</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">exchangeability(model)</code></pre><p>Test the sensitivity of a G-computation or doubly robust estimator or metalearner to a  violation of the exchangeability assumption.</p><p><strong>References</strong></p><p>For more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. &quot;Sensitivity analysis in observational research:      introducing the E-value.&quot; Annals of internal medicine 167, no. 4 (2017): 268-274.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, t = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100]
julia&gt; y = vec(rand(1:100, 100, 1)))
julia&gt; g_computer = GComputation(x, t, y, temporal=false)
julia&gt; estimate_causal_effect!(g_computer)
julia&gt; e_value(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L476-L495">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.e_value" href="#CausalELM.e_value"><code>CausalELM.e_value</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">e_value(model)</code></pre><p>Test the sensitivity of an estimator to a violation of the exchangeability assumption.</p><p><strong>References</strong></p><p>For more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. &quot;Sensitivity analysis in observational research:      introducing the E-value.&quot; Annals of internal medicine 167, no. 4 (2017): 268-274.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, t = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100]
julia&gt; y = vec(rand(1:100, 100, 1)))
julia&gt; g_computer = GComputation(x, t, y, temporal=false)
julia&gt; estimate_causal_effect!(g_computer)
julia&gt; e_value(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L498-L516">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.binarize" href="#CausalELM.binarize"><code>CausalELM.binarize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">binarize(x, cutoff)</code></pre><p>Convert a vector of counts or a continuous vector to a binary vector.</p><p><strong>Arguments</strong></p><ul><li><code>x::Any</code>: interable of numbers to binarize.</li><li><code>x::Any</code>: threshold after which numbers are converted to 1 and befrore which are converted    to 0.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; CausalELM.binarize([1, 2, 3], 2)
3-element Vector{Int64}:
 0
 0
 1</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L527-L545">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.risk_ratio" href="#CausalELM.risk_ratio"><code>CausalELM.risk_ratio</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">risk_ratio(model)</code></pre><p>Calculate the risk ratio for an estimated model.</p><p><strong>Notes</strong></p><p>If the treatment variable is not binary and the outcome variable is not continuous then the  treatment variable will be binarized.</p><p><strong>References</strong></p><p>For more information on how other quantities of interest are converted to risk ratios see:     VanderWeele, Tyler J., and Peng Ding. &quot;Sensitivity analysis in observational research:      introducing the E-value.&quot; Annals of internal medicine 167, no. 4 (2017): 268-274.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, t = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100]
julia&gt; y = vec(rand(1:100, 100, 1)))
julia&gt; g_computer = GComputation(x, t, y, temporal=false)
julia&gt; estimate_causal_effect!(g_computer)
julia&gt; risk_ratio(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L556-L578">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.positivity" href="#CausalELM.positivity"><code>CausalELM.positivity</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">positivity(model, [,min], [,max])</code></pre><p>Find likely violations of the positivity assumption.</p><p><strong>Notes</strong></p><p>This method uses an extreme learning machine or regularized extreme learning machine to  estimate probabilities of treatment. The returned matrix, which may be empty, are the  covariates that have a (near) zero probability of treatment or near zero probability of  being assigned to the control group, whith their entry in the last column being their  estimated treatment probability. In other words, they likely violate the positivity  assumption.</p><p><strong>Arguments</strong></p><ul><li><code>model::Union{CausalEstimator, Metalearner}</code>: a model to validate/test the assumptions of.</li><li><code>min::Float64</code>=1.0e-6: minimum probability of treatment for the positivity assumption.</li><li><code>high::Float64=1-min</code>: the maximum probability of treatment for the positivity assumption.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, t = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100]
julia&gt; y = vec(rand(1:100, 100, 1)))
julia&gt; g_computer = GComputation(x, t, y, temporal=false)
julia&gt; estimate_causal_effect!(g_computer)
julia&gt; positivity(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L661-L687">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.sums_of_squares</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.class_pointers</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.backtrack_to_find_breaks</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.variance</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.best_splits</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.group_by_class</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.jenks_breaks</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.fake_treatments</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.sdam</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.scdm</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.gvf</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="CausalELM.var_type" href="#CausalELM.var_type"><code>CausalELM.var_type</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">var_type(x)</code></pre><p>Determine the type of variable held by a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; CausalELM.var_type([1, 2, 3, 2, 3, 1, 1, 3, 2])
CausalELM.Count()</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/model_validation.jl#L13-L23">source</a></section></article><h2 id="Validation-Metrics"><a class="docs-heading-anchor" href="#Validation-Metrics">Validation Metrics</a><a id="Validation-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Validation-Metrics" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.mse" href="#CausalELM.mse"><code>CausalELM.mse</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mse(y, ŷ)</code></pre><p>Calculate the mean squared error</p><p>See also <a href="#CausalELM.mae"><code>mae</code></a>.</p><p>Examples</p><pre><code class="language-julia-repl hljs">julia&gt; mse([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])
4.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metrics.jl#L3-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.mae" href="#CausalELM.mae"><code>CausalELM.mae</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mae(y, ŷ)</code></pre><p>Calculate the mean absolute error</p><p>See also <a href="#CausalELM.mse"><code>mse</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; mae([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])
2.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metrics.jl#L24-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.accuracy" href="#CausalELM.accuracy"><code>CausalELM.accuracy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">accuracy(y, ŷ)</code></pre><p>Calculate the accuracy for a classification task</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; accuracy([1, 1, 1, 1], [0, 1, 1, 0])
0.5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metrics.jl#L45-L55">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>precision</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="CausalELM.recall" href="#CausalELM.recall"><code>CausalELM.recall</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">recall(y, ŷ)</code></pre><p>Calculate the recall for a classification task</p><p>See also <a href="@ref"><code>precision</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; recall([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])
0.5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metrics.jl#L97-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.F1" href="#CausalELM.F1"><code>CausalELM.F1</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">F1(y, ŷ)</code></pre><p>Calculate the F1 score for a classification task</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; F1([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])
0.4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metrics.jl#L124-L134">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.confusion_matrix" href="#CausalELM.confusion_matrix"><code>CausalELM.confusion_matrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">confusion_matrix(y, ŷ)</code></pre><p>Generate a confusion matrix</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; CausalELM.confusion_matrix([1, 1, 1, 1, 0], [1, 1, 1, 1, 0])
2×2 Matrix{Int64}:
 1  0
 0  4</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/metrics.jl#L140-L152">source</a></section></article><h2 id="Extreme-Learning-Machines"><a class="docs-heading-anchor" href="#Extreme-Learning-Machines">Extreme Learning Machines</a><a id="Extreme-Learning-Machines-1"></a><a class="docs-heading-anchor-permalink" href="#Extreme-Learning-Machines" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.fit!" href="#CausalELM.fit!"><code>CausalELM.fit!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit!(model)</code></pre><p>Make predictions with an ExtremeLearner.</p><p><strong>References</strong></p><p>For more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. &quot;Extreme learning machine: theory      and applications.&quot; Neurocomputing 70, no. 1-3 (2006): 489-501.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
julia&gt; m1 = ExtremeLearner(x, y, 10, σ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L78-L93">source</a></section><section><div><pre><code class="nohighlight hljs">fit!(model)</code></pre><p>Fit a Regularized Extreme Learner.</p><p><strong>References</strong></p><p>For more details see:      Li, Guoqiang, and Peifeng Niu. &quot;An enhanced extreme learning machine based on ridge      regression for regression.&quot; Neural Computing and Applications 22, no. 3 (2013):      803-810.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
julia&gt; m1 = RegularizedExtremeLearner(x, y, 10, σ)
julia&gt; f1 = fit!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L102-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.predict" href="#CausalELM.predict"><code>CausalELM.predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predict(model, X)</code></pre><p>Use an ExtremeLearningMachine to make predictions.</p><p><strong>References</strong></p><p>For more details see:      Huang G-B, Zhu Q-Y, Siew C. Extreme learning machine: theory and applications.      Neurocomputing. 2006;70:489–501. https://doi.org/10.1016/j.neucom.2005.12.126</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
julia&gt; m1 = ExtremeLearner(x, y, 10, σ)
julia&gt; f1 = fit(m1, sigmoid)
julia&gt; predict(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L133-L150">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.predict_counterfactual!" href="#CausalELM.predict_counterfactual!"><code>CausalELM.predict_counterfactual!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predict_counterfactual!(model, X)</code></pre><p>Use an ExtremeLearningMachine to predict the counterfactual.</p><p><strong>Notes</strong></p><p>This should be run with the observed covariates. To use synthtic data for what-if scenarios  use predict.</p><p>See also <a href="#CausalELM.predict"><code>predict</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
julia&gt; m1 = ExtremeLearner(x, y, 10, σ)
julia&gt; f1 = fit(m1, sigmoid)
julia&gt; predict_counterfactual!(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L159-L177">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.placebo_test" href="#CausalELM.placebo_test"><code>CausalELM.placebo_test</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">placebo_test(model)</code></pre><p>Conduct a placebo test.</p><p><strong>Notes</strong></p><p>This method makes predictions for the post-event or post-treatment period using data  in the pre-event or pre-treatment period and the post-event or post-treament. If there is a statistically significant difference between these predictions the study design may be flawed. Due to the multitude of significance tests for time series data, this function returns the predictions but does not test for statistical significance.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
julia&gt; m1 = ExtremeLearner(x, y, 10, σ)
julia&gt; f1 = fit(m1, sigmoid)
julia&gt; predict_counterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])
julia&gt; placebo_test(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L184-L204">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.ridge_constant" href="#CausalELM.ridge_constant"><code>CausalELM.ridge_constant</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ridge_constant(model, [,iterations])</code></pre><p>Calculate the L2 penalty for a regularized extreme learning machine using generalized cross  validation with successive halving.</p><p><strong>Arguments</strong></p><ul><li><code>model::RegularizedExtremeLearner</code>: regularized extreme learning machine.</li><li><code>iterations::Int</code>: number of iterations to perform for successive halving.</li></ul><p><strong>References</strong></p><p>For more information see:      Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; m1 = RegularizedExtremeLearner(x, y, 10, σ)
julia&gt; ridge_constant(m1)
julia&gt; ridge_constant(m1, iterations=20)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L213-L234">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.set_weights_biases" href="#CausalELM.set_weights_biases"><code>CausalELM.set_weights_biases</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">set_weights_biases(model)</code></pre><p>Calculate the weights and biases for an extreme learning machine or regularized extreme  learning machine.</p><p><strong>Notes</strong></p><p>Initialization is done using uniform Xavier initialization.</p><p><strong>References</strong></p><p>For details see;     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. &quot;Extreme learning machine: theory      and applications.&quot; Neurocomputing 70, no. 1-3 (2006): 489-501.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; m1 = RegularizedExtremeLearner(x, y, 10, σ)
julia&gt; set_weights_biases(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/models.jl#L263-L282">source</a></section></article><h2 id="Utility-Functions"><a class="docs-heading-anchor" href="#Utility-Functions">Utility Functions</a><a id="Utility-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.mean" href="#CausalELM.mean"><code>CausalELM.mean</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mean(x)</code></pre><p>Calculate the mean of a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; CausalELM.mean([1, 2, 3, 4])
2.5</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/utilities.jl#L1-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.var" href="#CausalELM.var"><code>CausalELM.var</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">var(x)</code></pre><p>Calculate the (sample) mean of a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; CausalELM.var([1, 2, 3, 4])
1.6666666666666667</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/utilities.jl#L14-L24">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>CausalELM.consecutive</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="CausalELM.one_hot_encode" href="#CausalELM.one_hot_encode"><code>CausalELM.one_hot_encode</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">one_hot_encode(x)</code></pre><p>One hot encode a categorical vector for multiclass classification.</p><p><strong>Examples</strong></p><pre><code class="language-julia-repl hljs">julia&gt; CausalELM.one_hot_encode([1, 2, 3, 4, 5])
5×5 Matrix{Float64}:
 1.0  0.0  0.0  0.0  0.0
 0.0  1.0  0.0  0.0  0.0
 0.0  0.0  1.0  0.0  0.0
 0.0  0.0  0.0  1.0  0.0
 0.0  0.0  0.0  0.0  1.0</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/3c04f9cf0af4fc6117b7a00752526aed7455ff89/src/utilities.jl#L27-L42">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../guide/doublyrobust/">« Doubly Robust Estimation</a><a class="docs-footer-nextpage" href="../contributing/">Contributing »</a><div class="flexbox-break"></div><p class="footer-message">© 2024 Darren Colby</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Monday 17 June 2024 17:28">Monday 17 June 2024</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
