<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API · CausalELM</title><script data-outdated-warner src="../assets/warner.js"></script><link rel="canonical" href="https://dscolby.github.io/CausalELM.jl/api/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.jpg" alt="CausalELM logo"/></a><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">CausalELM</a></li><li><span class="tocitem">Getting Started</span><ul><li><a class="tocitem" href="../guide/estimatorselection/">Deciding Which Estimator to Use</a></li><li><a class="tocitem" href="../guide/its/">Interrupted Time Series Estimation</a></li><li><a class="tocitem" href="../guide/gcomputation/">G-computation</a></li><li><a class="tocitem" href="../guide/doublemachinelearning/">Double Machine Learning</a></li><li><a class="tocitem" href="../guide/metalearners/">Metalearners</a></li><li><a class="tocitem" href="../guide/doublyrobust/">Doubly Robust Estimation</a></li></ul></li><li class="is-active"><a class="tocitem" href>API</a><ul class="internal"><li><a class="tocitem" href="#Types"><span>Types</span></a></li><li><a class="tocitem" href="#Activation-Functions"><span>Activation Functions</span></a></li><li><a class="tocitem" href="#Cross-Validation"><span>Cross Validation</span></a></li><li><a class="tocitem" href="#Average-Causal-Effect-Estimators"><span>Average Causal Effect Estimators</span></a></li><li><a class="tocitem" href="#Metalearners"><span>Metalearners</span></a></li><li><a class="tocitem" href="#Common-Methods"><span>Common Methods</span></a></li><li><a class="tocitem" href="#Inference"><span>Inference</span></a></li><li><a class="tocitem" href="#Model-Validation"><span>Model Validation</span></a></li><li><a class="tocitem" href="#Validation-Metrics"><span>Validation Metrics</span></a></li><li><a class="tocitem" href="#Extreme-Learning-Machines"><span>Extreme Learning Machines</span></a></li><li><a class="tocitem" href="#Utility-Functions"><span>Utility Functions</span></a></li></ul></li><li><a class="tocitem" href="../contributing/">Contributing</a></li><li><a class="tocitem" href="../release_notes/">Release Notes</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/dscolby/CausalELM.jl/blob/main/docs/src/api.md#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="CausalELM"><a class="docs-heading-anchor" href="#CausalELM">CausalELM</a><a id="CausalELM-1"></a><a class="docs-heading-anchor-permalink" href="#CausalELM" title="Permalink"></a></h1><p>Most of the methods and structs here are private, not exported, should not be called by the  user, and are documented for the purpose of developing CausalELM or to facilitate  understanding of the implementation.</p><h2 id="Types"><a class="docs-heading-anchor" href="#Types">Types</a><a id="Types-1"></a><a class="docs-heading-anchor-permalink" href="#Types" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.InterruptedTimeSeries" href="#CausalELM.InterruptedTimeSeries"><code>CausalELM.InterruptedTimeSeries</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">InterruptedTimeSeries(X₀, Y₀, X₁, Y₁; kwargs...)</code></pre><p>Initialize an interrupted time series estimator. </p><p><strong>Arguments</strong></p><ul><li><code>X₀::Any</code>: an array or DataFrame of covariates from the pre-treatment period.</li><li><code>Y₁::Any</code>: an array or DataFrame of outcomes from the pre-treatment period.</li><li><code>X₁::Any</code>: an array or DataFrame of covariates from the post-treatment period.</li><li><code>Y₁::Any</code>: an array or DataFrame of outcomes from the post-treatment period.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li></ul><p><strong>Keywords</strong></p><ul><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Int</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Int</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Int</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Int</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Int</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For a simple linear regression-based tutorial on interrupted time series analysis see:     Bernal, James Lopez, Steven Cummins, and Antonio Gasparrini. &quot;Interrupted time series      regression for the evaluation of public health interventions: a tutorial.&quot; International      journal of epidemiology 46, no. 1 (2017): 348-355.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)
m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)
m2 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁; regularized=false)
x₀_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100))
y₀_df = DataFrame(y=rand(100))
x₁_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100)) 
y₁_df = DataFrame(y=rand(100))
m3 = InterruptedTimeSeries(x₀_df, y₀_df, x₁_df, y₁_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L4-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.GComputation" href="#CausalELM.GComputation"><code>CausalELM.GComputation</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GComputation(X, T, Y; kwargs...)</code></pre><p>Initialize a G-Computation estimator.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>task::String</code>: either regression or classification.</li><li><code>quantity_of_interest::String</code>: ATE for average treatment effect or CTE for cummulative    treatment effect.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Int</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Int</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Int</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Int</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Int</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For a good overview of G-Computation see:     Chatton, Arthur, Florent Le Borgne, Clémence Leyrat, Florence Gillaizeau, Chloé      Rousseau, Laetitia Barbin, David Laplaud, Maxime Léger, Bruno Giraudeau, and Yohann      Foucher. &quot;G-computation, propensity score-based methods, and targeted maximum likelihood      estimator for causal inference with different covariates sets: a comparative simulation      study.&quot; Scientific reports 10, no. 1 (2020): 9219.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), rand(100), [rand()&lt;0.4 for i in 1:100]
m1 = GComputation(X, T, Y)
m2 = GComputation(X, T, Y; task=&quot;regression&quot;)
m3 = GComputation(X, T, Y; task=&quot;regression&quot;, quantity_of_interest=&quot;ATE)
m4 = GComputation(X, T, Y; task=&quot;regression&quot;, quantity_of_interest=&quot;ATE&quot;, regularized=true)

x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100)) 
m5 = GComputation(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L106-L162">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.DoubleMachineLearning" href="#CausalELM.DoubleMachineLearning"><code>CausalELM.DoubleMachineLearning</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DoubleMachineLearning(X, T, Y; kwargs...)</code></pre><p>Initialize a double machine learning estimator with cross fitting.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates of interest.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>W::Any</code>: an array or dataframe of all possible confounders.</li><li><code>task::String</code>: either regression or classification.</li><li><code>quantity_of_interest::String</code>: ATE for average treatment effect or CTE for cummulative    treatment effect.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Int</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Int</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Int</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Int</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Int</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p>Unlike other estimators, this method does not support time series or panel data. This method  also does not work as well with smaller datasets because it estimates separate outcome  models for the treatment and control groups.</p><p><strong>References</strong></p><p>For more information see:     Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,      Whitney Newey, and James Robins. &quot;Double/debiased machine learning for treatment and      structural parameters.&quot; (2016): C1-C68.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = DoubleMachineLearning(X, T, Y)
m2 = DoubleMachineLearning(X, T, Y; task=&quot;regression&quot;)

x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
m3 = DoubleMachineLearning(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L217-L274">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.SLearner" href="#CausalELM.SLearner"><code>CausalELM.SLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SLearner(X, T, Y; kwargs...)</code></pre><p>Initialize a S-Learner.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>task::String</code>: either regression or classification.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Int</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Int</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Int</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Int</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Int</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For an overview of S-Learners and other metalearners see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for      estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = SLearner(X, T, Y)
m2 = SLearner(X, T, Y; task=&quot;regression&quot;)
m3 = SLearner(X, T, Y; task=&quot;regression&quot;, regularized=true)

x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
m4 = SLearner(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L4-L54">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.TLearner" href="#CausalELM.TLearner"><code>CausalELM.TLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">TLearner(X, T, Y; kwargs...)</code></pre><p>Initialize a T-Learner.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>task::String</code>: either regression or classification.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Int</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Int</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Int</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Int</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Int</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For an overview of T-Learners and other metalearners see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for      estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = TLearner(X, T, Y)
m2 = TLearner(X, T, Y; task=&quot;regression&quot;)
m3 = TLearner(X, T, Y; task=&quot;regression&quot;, regularized=true)

x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
m4 = TLearner(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L72-L122">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.RLearner" href="#CausalELM.RLearner"><code>CausalELM.RLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RLearner(X, T, Y; kwargs...)</code></pre><p>Initialize an R-Learner.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates of interest.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>W::Any</code> : an array of all possible confounders.</li><li><code>task::String</code>: either regression or classification.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Int</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Int</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Int</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Int</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Int</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For an explanation of R-Learner estimation see:     Nie, Xinkun, and Stefan Wager. &quot;Quasi-oracle estimation of heterogeneous treatment      effects.&quot; Biometrika 108, no. 2 (2021): 299-319.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = RLearner(X, T, Y)
m2 = RLearner(X, T, Y; t_cat=true)

x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
m4 = RLearner(x_df, t_df, y_df)

w = rand(100, 6)
m5 = RLearner(X, T, Y, W=w)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L274-L326">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.CausalEstimator" href="#CausalELM.CausalEstimator"><code>CausalELM.CausalEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Abstract type for GComputation and DoubleMachineLearning</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Metalearner" href="#CausalELM.Metalearner"><code>CausalELM.Metalearner</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Abstract type for metalearners</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.ExtremeLearningMachine" href="#CausalELM.ExtremeLearningMachine"><code>CausalELM.ExtremeLearningMachine</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Abstract type that includes vanilla and L2 regularized Extreme Learning Machines</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L3">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.ExtremeLearner" href="#CausalELM.ExtremeLearner"><code>CausalELM.ExtremeLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ExtremeLearner(X, Y, hidden_neurons, activation)</code></pre><p>Construct an ExtremeLearner for fitting and prediction.</p><p><strong>Notes</strong></p><p>While it is possible to use an ExtremeLearner for regression, it is recommended to use  RegularizedExtremeLearner, which imposes an L2 penalty, to reduce multicollinearity.</p><p><strong>References</strong></p><p>For more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. &quot;Extreme learning machine: theory      and applications.&quot; Neurocomputing 70, no. 1-3 (2006): 489-501.</p><p>See also <a href="@ref">&#39;RegularizedExtremeLearner&#39;</a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
m1 = ExtremeLearner(x, y, 10, σ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L6-L27">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.RegularizedExtremeLearner" href="#CausalELM.RegularizedExtremeLearner"><code>CausalELM.RegularizedExtremeLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RegularizedExtremeLearner(X, Y, hidden_neurons, activation)</code></pre><p>Construct a RegularizedExtremeLearner for fitting and prediction.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
m1 = RegularizedExtremeLearner(x, y, 10, σ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L47-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Nonbinary" href="#CausalELM.Nonbinary"><code>CausalELM.Nonbinary</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Abstract type used to dispatch risk_ratio on nonbinary treatments</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L1">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Binary" href="#CausalELM.Binary"><code>CausalELM.Binary</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Type used to dispatch risk_ratio on binary treatments</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L4">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Count" href="#CausalELM.Count"><code>CausalELM.Count</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Type used to dispatch risk_ratio on count treatments</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.Continuous" href="#CausalELM.Continuous"><code>CausalELM.Continuous</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Type used to dispatch risk_ratio on continuous treatments</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L10">source</a></section></article><h2 id="Activation-Functions"><a class="docs-heading-anchor" href="#Activation-Functions">Activation Functions</a><a id="Activation-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Activation-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.binary_step" href="#CausalELM.binary_step"><code>CausalELM.binary_step</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">binary_step(x)</code></pre><p>Apply the binary step activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">binary_step(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L1-L10">source</a></section><section><div><pre><code class="nohighlight hljs">binary_step(x)</code></pre><p>Apply the binary step activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">binary_step([-1000, 100, 1, 0, -0.001, -3])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L13-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.σ" href="#CausalELM.σ"><code>CausalELM.σ</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">σ(x)</code></pre><p>Apply the sigmoid activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">σ(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L25-L34">source</a></section><section><div><pre><code class="nohighlight hljs">σ(x)</code></pre><p>Apply the sigmoid activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">σ([1, 0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L40-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.tanh" href="#Base.tanh"><code>Base.tanh</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">tanh(x)</code></pre><p>Apply the tanh activation function to an array.</p><p><strong>Notes</strong></p><p>This is just a vectorized version of Base.tanh</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">tanh([1, 0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L52-L64">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.relu" href="#CausalELM.relu"><code>CausalELM.relu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">relu(x)</code></pre><p>Apply the ReLU activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">relu(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L67-L76">source</a></section><section><div><pre><code class="nohighlight hljs">relu(x)</code></pre><p>Apply the ReLU activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">relu([1, 0, -1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L79-L88">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.leaky_relu" href="#CausalELM.leaky_relu"><code>CausalELM.leaky_relu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">leaky_relu(x)</code></pre><p>Apply the leaky ReLU activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">leaky_relu(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L91-L100">source</a></section><section><div><pre><code class="nohighlight hljs">leaky_relu(x)</code></pre><p>Apply the leaky ReLU activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">leaky_relu([-0.01, 0, 1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L103-L112">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.swish" href="#CausalELM.swish"><code>CausalELM.swish</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">swish(x)</code></pre><p>Apply the swish activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">swish(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L115-L124">source</a></section><section><div><pre><code class="nohighlight hljs">swish(x)</code></pre><p>Apply the swish activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">swish([1, 0, -1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L127-L136">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.softmax" href="#CausalELM.softmax"><code>CausalELM.softmax</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">softmax(x)</code></pre><p>Apply the softmax activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">softmax(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L139-L148">source</a></section><section><div><pre><code class="nohighlight hljs">softmax(x)</code></pre><p>Apply the softmax activation function to a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">softmax([1, 2, 3])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L151-L160">source</a></section><section><div><pre><code class="nohighlight hljs">softmax(x)</code></pre><p>Apply the softmax activation function to the rows of an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">julia&gt; x = rand(5, 3)
softmax(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L163-L173">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.softplus" href="#CausalELM.softplus"><code>CausalELM.softplus</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">softplus(x)</code></pre><p>Apply the softplus activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">softplus(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L176-L185">source</a></section><section><div><pre><code class="nohighlight hljs">softplus(x)</code></pre><p>Apply the softplus activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">softplus([1, -1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L188-L197">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.gelu" href="#CausalELM.gelu"><code>CausalELM.gelu</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gelu(x)</code></pre><p>Apply the GeLU activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">gelu(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L200-L209">source</a></section><section><div><pre><code class="nohighlight hljs">gelu(x)</code></pre><p>Apply the GeLU activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">gelu([-1, 0, 1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L212-L221">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.gaussian" href="#CausalELM.gaussian"><code>CausalELM.gaussian</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gaussian(x)</code></pre><p>Apply the gaussian activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">gaussian(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L224-L233">source</a></section><section><div><pre><code class="nohighlight hljs">gaussian(x)</code></pre><p>Apply the gaussian activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">gaussian([1, -1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L236-L245">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.hard_tanh" href="#CausalELM.hard_tanh"><code>CausalELM.hard_tanh</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">hard_tanh(x)</code></pre><p>Apply the hard_tanh activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">hard_tanh(-2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L248-L257">source</a></section><section><div><pre><code class="nohighlight hljs">hard_tanh(x)</code></pre><p>Apply the hard_tanh activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">hard_tanh([-2, 0, 2])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L268-L277">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.elish" href="#CausalELM.elish"><code>CausalELM.elish</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">elish(x)</code></pre><p>Apply the ELiSH activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">elish(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L280-L289">source</a></section><section><div><pre><code class="nohighlight hljs">elish(x)</code></pre><p>Apply the ELiSH activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">elish([-1, 1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L292-L301">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.fourier" href="#CausalELM.fourier"><code>CausalELM.fourier</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fourrier(x)</code></pre><p>Apply the Fourier activation function to a real number.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">fourier(1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L304-L313">source</a></section><section><div><pre><code class="nohighlight hljs">fourrier(x)</code></pre><p>Apply the Fourier activation function to an array.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">fourier([-1, 1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/activation.jl#L316-L325">source</a></section></article><h2 id="Cross-Validation"><a class="docs-heading-anchor" href="#Cross-Validation">Cross Validation</a><a id="Cross-Validation-1"></a><a class="docs-heading-anchor-permalink" href="#Cross-Validation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.generate_folds" href="#CausalELM.generate_folds"><code>CausalELM.generate_folds</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generate_folds(X, Y, folds)</code></pre><p>Create folds for cross validation.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">xfolds, y_folds = generate_folds(zeros(20, 2), zeros(20), 5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/crossval.jl#L3-L12">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.generate_temporal_folds" href="#CausalELM.generate_temporal_folds"><code>CausalELM.generate_temporal_folds</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generate_folds(X, Y, folds)</code></pre><p>Create rolling folds for cross validation of time series data.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">xfolds, y_folds = generate_temporal_folds(zeros(20, 2), zeros(20), 5, temporal=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/crossval.jl#L36-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.validation_loss" href="#CausalELM.validation_loss"><code>CausalELM.validation_loss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">validation_loss(xtrain, ytrain, xtest, ytest, nodes, metric; kwargs...)</code></pre><p>Calculate a validation metric for a single fold in k-fold cross validation.</p><p><strong>Arguments</strong></p><ul><li><code>xtrain::Any</code>: an array of features to train on.</li><li><code>ytrain::Any</code>: an array of training labels.</li><li><code>xtest::Any</code>: an array of features to test on.</li><li><code>ytrain::Any</code>: an array of testing labels.</li><li><code>nodes::Int</code>: the number of neurons in the extreme learning machine.</li><li><code>metric::Function</code>: the validation metric to calculate.</li></ul><p><strong>Keywords</strong></p><ul><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(100, 5); y = Float64.(rand(100) .&gt; 0.5)
validation_loss(x, y, 5, accuracy, 3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/crossval.jl#L61-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.cross_validate" href="#CausalELM.cross_validate"><code>CausalELM.cross_validate</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">cross_validate(X, Y, neurons, metric, activation, regularized, folds, temporal)</code></pre><p>Calculate a validation metric for k folds using a single set of hyperparameters.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array</code>: an array of features to train on.</li><li><code>Y::Vector</code>: a vector of labels to train on.</li><li><code>neurons::Int</code>: the number of neurons to use in the extreme learning machine.</li><li><code>metric::Function</code>: the validation metric to calculate.</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>folds::Int</code>: the number of folds to use for cross validation.</li><li><code>temporal::Function=true</code>: whether the data is of a time series or panel nature.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x = rand(100, 5); y = Float64.(rand(100) .&gt; 0.5)
cross_validate(x, y, 5, accuracy)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/crossval.jl#L99-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.best_size" href="#CausalELM.best_size"><code>CausalELM.best_size</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">best_size(X, Y, metric, task, activation, min_neurons, max_neurons, regularized, folds, 
          temporal, iterations, elm_size)</code></pre><p>Compute the best number of neurons for an Extreme Learning Machine.</p><p><strong>Notes</strong></p><p>The procedure tests networks with numbers of neurons in a sequence whose length is given  by iterations on the interval [min<em>neurons, max</em>neurons]. Then, it uses the networks  sizes and validation errors from the sequence to predict the validation error or metric  for every network size between min<em>neurons and max</em>neurons using the function  approximation ability of an Extreme Learning Machine. Finally, it returns the network  size with the best predicted validation error or metric.</p><p><strong>Arguments</strong></p><ul><li><code>X::Array</code>: an array of features to train on.</li><li><code>Y::Vector</code>: a vector of labels to train on.</li><li><code>metric::Function</code>: the validation metric to calculate.</li><li><code>task::String</code>: either regression or classification.</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>min_neurons::Int</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Int</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>folds::Int</code>: the number of folds to use for cross validation.</li><li><code>temporal::Function=true</code>: whether the data is of a time series or panel nature.</li><li><code>iterations::Int</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>elm_size::Int</code>: the number of nuerons in the validation loss approximator network.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">best_size(rand(100, 5), rand(100), mse, &quot;regression&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/crossval.jl#L144-L177">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.shuffle_data" href="#CausalELM.shuffle_data"><code>CausalELM.shuffle_data</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">shuffle_data(X, Y)</code></pre><p>Shuffles covariates and outcome vector for cross validation.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, y, t = rand(100, 5), rand(100), [rand()&lt;0.4 for i in 1:100]
shuffle_data(x, y, t)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/crossval.jl#L200-L210">source</a></section></article><h2 id="Average-Causal-Effect-Estimators"><a class="docs-heading-anchor" href="#Average-Causal-Effect-Estimators">Average Causal Effect Estimators</a><a id="Average-Causal-Effect-Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Average-Causal-Effect-Estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.XLearner" href="#CausalELM.XLearner"><code>CausalELM.XLearner</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">XLearner(X, T, Y; kwargs...)</code></pre><p>Initialize an X-Learner.</p><p><strong>Arguments</strong></p><ul><li><code>X::Any</code>: an array or DataFrame of covariates.</li><li><code>T::Any</code>: an vector or DataFrame of treatment statuses.</li><li><code>Y::Any</code>: an array or DataFrame of outcomes.</li></ul><p><strong>Keywords</strong></p><ul><li><code>task::String</code>: either regression or classification.</li><li><code>regularized::Function=true</code>: whether to use L2 regularization</li><li><code>activation::Function=relu</code>: the activation function to use.</li><li><code>validation_metric::Function</code>: the validation metric to calculate during cross validation.</li><li><code>min_neurons::Int</code>: the minimum number of neurons to consider for the extreme learner.</li><li><code>max_neurons::Int</code>: the maximum number of neurons to consider for the extreme learner.</li><li><code>folds::Int</code>: the number of cross validation folds to find the best number of neurons.</li><li><code>iterations::Int</code>: the number of iterations to perform cross validation between    min<em>neurons and max</em>neurons.</li><li><code>approximator_neurons::Int</code>: the number of nuerons in the validation loss approximator    network.</li></ul><p><strong>Notes</strong></p><p>If regularized is set to true then the ridge penalty will be estimated using generalized  cross validation where the maximum number of iterations is 2 * folds for the successive  halving procedure. However, if the penalty in on iteration is approximately the same as in  the previous penalty, then the procedure will stop early.</p><p><strong>References</strong></p><p>For an overview of X-Learners and other metalearners see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for      estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p>For details and a derivation of the generalized cross validation estimator see:     Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = XLearner(X, T, Y)
m2 = XLearner(X, T, Y; task=&quot;regression&quot;)
m3 = XLearner(X, T, Y; task=&quot;regression&quot;, regularized=true)

x_df = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100))
t_df, y_df = DataFrame(t=rand(0:1, 100)), DataFrame(y=rand(100))
m4 = XLearner(x_df, t_df, y_df)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L172-L222">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.estimate_effect!" href="#CausalELM.estimate_effect!"><code>CausalELM.estimate_effect!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">estimate_effect!(DML, [,cate])</code></pre><p>Estimate a treatment effect using double machine learning.</p><p><strong>Notes</strong></p><p>This method should not be called directly.</p><p><strong>Arguments</strong></p><ul><li><code>DML::DoubleMachineLearning</code>: the DoubleMachineLearning struct to estimate the effect for.</li><li><code>cate::Bool=false</code>: whether to estimate the cate.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = DoubleMachineLearning(X, T, Y)
estimate_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L433-L451">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_effect!(DRE, X, T, Y, Z)</code></pre><p>Estimate the CATE for a single cross fitting iteration via doubly robust estimation.</p><p>This method should not be called directly.</p><p><strong>Arguments</strong></p><ul><li><code>DRE::DoublyRobustLearner</code>: the DoubleMachineLearning struct to estimate the effect for.</li><li><code>X</code>: a vector of three covariate folds.</li><li><code>T</code>: a vector of three treatment folds.</li><li><code>Y</code>: a vector of three outcome folds.</li><li><code>Z</code> : a vector of three confounder folds and covariate folds.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y, W =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100), rand(6, 100)
m1 = DoublyRobustLearner(X, T, Y, W=W)

X, T, W, Y = make_folds(m1)
Z = m1.W == m1.X ? X : [reduce(hcat, (z)) for z in zip(X, W)]
estimate_effect!(m1, X, T, Y, Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L627-L650">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.predict_residuals" href="#CausalELM.predict_residuals"><code>CausalELM.predict_residuals</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predict_residuals(DML, x_train, x_test, y_train, y_test, t_train, t_test)</code></pre><p>Predict treatment and outcome residuals for doubl machine learning.</p><p><strong>Notes</strong></p><p>This method should not be called directly.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
x_train, x_test = X[1:80, :], X[81:end, :]
y_train, y_test = Y[1:80], Y[81:end]
t_train, t_test = T[1:80], T[81:100]
m1 = DoubleMachineLearning(X, T, Y)
predict_residuals(m1, x_train, x_test, y_train, y_test, t_train, t_test)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L479-L496">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.moving_average" href="#CausalELM.moving_average"><code>CausalELM.moving_average</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">moving_average(x)</code></pre><p>Calculates a cumulative moving average.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">moving_average([1, 2, 3])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L542-L551">source</a></section></article><h2 id="Metalearners"><a class="docs-heading-anchor" href="#Metalearners">Metalearners</a><a id="Metalearners-1"></a><a class="docs-heading-anchor-permalink" href="#Metalearners" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.stage1!" href="#CausalELM.stage1!"><code>CausalELM.stage1!</code></a> — <span class="docstring-category">Function</span></header><section><div><p>stage1!(x)</p><p>Estimate the first stage models for an X-learner.</p><p>This method should not be called by the user.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = XLearner(X, T, Y)
stage1!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L678-L691">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.stage2!" href="#CausalELM.stage2!"><code>CausalELM.stage2!</code></a> — <span class="docstring-category">Function</span></header><section><div><p>stage2!(x)</p><p>Estimate the second stage models for an X-learner.</p><p>This method should not be called by the user.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = XLearner(X, T, Y)
stage1!(m1)
stage2!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L713-L727">source</a></section></article><h2 id="Common-Methods"><a class="docs-heading-anchor" href="#Common-Methods">Common Methods</a><a id="Common-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Common-Methods" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.estimate_causal_effect!" href="#CausalELM.estimate_causal_effect!"><code>CausalELM.estimate_causal_effect!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">estimate_causal_effect!(its)</code></pre><p>Estimate the effect of an event relative to a predicted counterfactual.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)
m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)
estimate_causal_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L319-L330">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(g)</code></pre><p>Estimate a causal effect of interest using G-Computation.</p><p><strong>Notes</strong></p><p>If treatents are administered at multiple time periods, the effect will be estimated as the  average difference between the outcome of being treated in all periods and being treated in  no periods. For example, given that ividuals 1, 2, ..., i ∈ I recieved either a treatment  or a placebo in p different periods, the model would estimate the average treatment effect  as E[Yᵢ|T₁=1, T₂=1, ... Tₚ=1, Xₚ] - E[Yᵢ|T₁=0, T₂=0, ... Tₚ=0, Xₚ].</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = GComputation(X, T, Y)
estimate_causal_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L354-L372">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(DML)</code></pre><p>Estimate a causal effect of interest using double machine learning.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = DoubleMachineLearning(X, T, Y)
estimate_causal_effect!(m1)

W = rand(100, 6)
m2 = DoubleMachineLearning(X, T, Y, W=W)
estimate_causal_effect!(m2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/estimators.jl#L401-L416">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(s)</code></pre><p>Estimate the CATE using an S-learner.</p><p>For an overview of S-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for      estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m4 = SLearner(X, T, Y)
estimate_causal_effect!(m4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L439-L455">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(t)</code></pre><p>Estimate the CATE using an T-learner.</p><p>For an overview of T-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for      estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m5 = TLearner(X, T, Y)
estimate_causal_effect!(m5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L474-L490">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(x)</code></pre><p>Estimate the CATE using an X-learner.</p><p>For an overview of X-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. &quot;Metalearners for      estimating heterogeneous treatment effects using machine learning.&quot; Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = XLearner(X, T, Y)
estimate_causal_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L518-L534">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(R)</code></pre><p>Estimate the CATE using an R-learner.</p><p>For an overview of R-learning see:     Nie, Xinkun, and Stefan Wager. &quot;Quasi-oracle estimation of heterogeneous treatment      effects.&quot; Biometrika 108, no. 2 (2021): 299-319.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = RLearner(X, T, Y)
estimate_causal_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L553-L568">source</a></section><section><div><pre><code class="nohighlight hljs">estimate_causal_effect!(DRE)</code></pre><p>Estimate the CATE using a doubly robust learner.</p><p>For details on how this method estimates the CATE see:     Kennedy, Edward H. &quot;Towards optimal doubly robust estimation of heterogeneous causal      effects.&quot; Electronic Journal of Statistics 17, no. 2 (2023): 3008-3049.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = DoublyRobustLearner(X, T, Y)
estimate_causal_effect!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metalearners.jl#L587-L602">source</a></section></article><h2 id="Inference"><a class="docs-heading-anchor" href="#Inference">Inference</a><a id="Inference-1"></a><a class="docs-heading-anchor-permalink" href="#Inference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.summarize" href="#CausalELM.summarize"><code>CausalELM.summarize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">summarize(mod, n)</code></pre><p>Get a summary from a CausalEstimator or Metalearner.</p><p><strong>Arguments</strong></p><ul><li><code>mod::Union{CausalEstimator, Metalearner}</code>: a model to summarize.</li><li><code>n::Int=100</code>: the number of iterations to generate the numll distribution for    randomization inference.</li></ul><p><strong>Notes</strong></p><p>p-values and standard errors are estimated using approximate randomization inference.</p><p><strong>References</strong></p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X, T, Y =  rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(100)
m1 = GComputation(X, T, Y)
estimate_causal_effect!(m1)
summarize(m1)

m2 = RLearner(X, T, Y)
estimate_causal_effect(m2)
julia&gt; summarize(m2)

m3 = SLearner(X, T, Y)
estimate_causal_effect!(m3)
summarise(m3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/inference.jl#L3-L35">source</a></section><section><div><pre><code class="nohighlight hljs">summarize(its, n, mean_effect)</code></pre><p>Get a summary from an interrupted time series estimator.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTimeSeries</code>: an interrupted time series estimator</li><li><code>n::Int=100</code>: the number of iterations to generate the numll distribution for    randomization inference.</li><li><code>mean_effect::Bool=true</code>: whether to estimate the mean or cumulative effect for an    interrupted time series estimator.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)
m4 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)
estimate_causal_effect!(m4)
julia&gt; summarize(m4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/inference.jl#L62-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.generate_null_distribution" href="#CausalELM.generate_null_distribution"><code>CausalELM.generate_null_distribution</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">generate_null_distribution(mod, n)</code></pre><p>Generate a null distribution for the treatment effect of G-computation, double machine  learning, or metalearning.</p><p><strong>Arguments</strong></p><ul><li><code>mod::Union{CausalEstimator, Metalearner}</code>: a model to summarize.</li><li><code>n::Int=100</code>: the number of iterations to generate the numll distribution for    randomization inference.</li></ul><p><strong>Notes</strong></p><p>This method estimates the same model that is provided using random permutations of the  treatment assignment to generate a vector of estimated effects under different treatment regimes. When mod is a metalearner the null statistic is the difference is the ATE.</p><p>Note that lowering the number of iterations increases the probability of failing to reject the null hypothesis.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, t, y = rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(1:100, 100, 1)
g_computer = GComputation(x, t, y)
estimate_causal_effect!(g_computer)
generate_null_distribution(g_computer, 500)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/inference.jl#L110-L136">source</a></section><section><div><pre><code class="nohighlight hljs">generate_null_distribution(its, n, mean_effect)</code></pre><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTimeSeries</code>: an interrupted time series estimator</li><li><code>n::Int=100</code>: the number of iterations to generate the numll distribution for        randomization inference.</li><li><code>mean_effect::Bool=true</code>: whether to estimate the mean or cumulative effect for an        interrupted time series estimator.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)
its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
estimate_causale_ffect!(its)
generate_null_distribution(its, 10)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/inference.jl#L158-L175">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.quantities_of_interest" href="#CausalELM.quantities_of_interest"><code>CausalELM.quantities_of_interest</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">quantities_of_interest(mod, n)</code></pre><p>Generate a p-value and standard error through randomization inference</p><p>This method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from the generated distribution.</p><p>Note that lowering the number of iterations increases the probability of failing to reject the null hypothesis.</p><p>For a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, t, y = rand(100, 5), [rand()&lt;0.4 for i in 1:100], rand(1:100, 100, 1)
g_computer = GComputation(x, t, y)
estimate_causal_effect!(g_computer)
quantities_of_interest(g_computer, 1000)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/inference.jl#L199-L221">source</a></section><section><div><pre><code class="nohighlight hljs">quantities_of_interest(mod, n)</code></pre><p>Generate a p-value and standard error through randomization inference</p><p>This method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from  the generated distribution. Randomization for event studies is done by creating time splits  at even intervals and reestimating the causal effect.</p><p>Note that lowering the number of iterations increases the probability of failing to reject the null hypothesis.</p><p>For a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)
its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
estimate_causal_effect!(its)
quantities_of_interest(its, 10)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/inference.jl#L234-L257">source</a></section></article><h2 id="Model-Validation"><a class="docs-heading-anchor" href="#Model-Validation">Model Validation</a><a id="Model-Validation-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Validation" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.validate" href="#CausalELM.validate"><code>CausalELM.validate</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">validate(its; kwargs...)</code></pre><p>Test the validity of an estimated interrupted time series analysis.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTimeSeries</code>: an interrupted time seiries estimator.</li></ul><p><strong>Keywords</strong></p><ul><li><code>n::Int</code>: the number of times to simulate a confounder.</li><li><code>low::Float64</code>=0.15: the minimum proportion of data points to include before or after the    tested break in the Wald supremum test.</li><li><code>high::Float64=0.85</code>: the maximum proportion of data points to include before or after the    tested break in the Wald supremum test.</li></ul><p><strong>Notes</strong></p><p>This method coducts a Chow Test, a Wald supremeum test, and tests the model&#39;s sensitivity to  confounders. The Chow Test tests for structural breaks in the covariates between the time  before and after the event. p-values represent the proportion of times the magnitude of the  break in a covariate would have been greater due to chance. Lower p-values suggest a higher  probability the event effected the covariates and they cannot provide unbiased  counterfactual predictions. The Wald supremum test finds the structural break with the  highest Wald statistic. If this is not the same as the hypothesized break, it could indicate  an anticipation effect, a confounding event, or that the intervention or policy took place  in multiple phases. p-values represent the proportion of times we would see a larger Wald  statistic if the data points were randomly allocated to pre and post-event periods for the  predicted structural break. Ideally, the hypothesized break will be the same as the  predicted break and it will also have a low p-value. The omitted predictors test adds  normal random variables with uniform noise as predictors. If the included covariates are  good predictors of the counterfactual outcome, adding irrelevant predictors should not have  a large effect on the predicted counterfactual outcomes or the estimated effect.</p><p>This method does not implement the second test in Baicker and Svoronos because the estimator  in this package models the relationship between covariates and the outcome and uses an  extreme learning machine instead of linear regression, so variance in the outcome across  different bins is not much of an issue.</p><p><strong>References</strong></p><p>For more details on the assumptions and validity of interrupted time series designs, see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.</p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)
m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)
stimate_causal_effect!(m1)
julia&gt; validate(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L34-L86">source</a></section><section><div><pre><code class="nohighlight hljs">validate(m; kwargs)</code></pre><p><strong>Arguments</strong></p><ul><li><code>m::Union{CausalEstimator, Metalearner}</code>: a model to validate/test the assumptions of.</li></ul><p><strong>Keywords</strong></p><ul><li><code>num_treatments=5::Int</code>: the maximum number of treatments to use when testing the        plausability of the counterfactual consistency assumption.</li><li><code>min::Float64</code>=1.0e-6: minimum probability of treatment for the positivity assumption.</li><li><code>high::Float64=1-min</code>: the maximum probability of treatment for the positivity assumption.</li></ul><p><strong>Notes</strong></p><p>This method tests the counterfactual consistency, exchangeability, and positivity  assumptions required for causal inference. It should be noted that consistency and  exchangeability are not directly testable, so instead, these tests do not provide definitive  evidence of a violation of these assumptions. To probe the counterfactual consistency  assumption, we assume there were multiple levels of treatments and find them by binning the dependent vairable for treated observations using Jenks breaks. The optimal number of breaks  between 2 and num_treatments is found using the elbow method. Using these hypothesized  treatment assignemnts, this method compares the MSE of linear regressions using the observed  and hypothesized treatments. If the counterfactual consistency assumption holds then the  difference between the MSE with hypothesized treatments and the observed treatments should  be positive because the hypothesized treatments should not provide useful information. If  it is negative, that indicates there was more useful information provided by the  hypothesized treatments than the observed treatments or that there is an unobserved  confounder. Next, this methods tests the model&#39;s sensitivity to a violation of the  exchangeability assumption by calculating the E-value, which is the minimum strength of  association, on the risk ratio scale, that an unobserved confounder would need to have with  the treatment and outcome variable to fully explain away the estimated effect. Thus, higher  E-values imply the model is more robust to a violation of the exchangeability assumption.  Finally, this method tests the positivity assumption by estimating propensity scores. Rows in the matrix are levels of covariates that have a zero probability of treatment. If the  matrix is empty, none of the observations have an estimated zero probability of treatment,  which implies the positivity assumption is satisfied.</p><p><strong>References</strong></p><p>For a thorough review of casual inference assumptions see:     Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and      Francis, 2024. </p><p>For more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. &quot;Sensitivity analysis in observational research:      introducing the E-value.&quot; Annals of internal medicine 167, no. 4 (2017): 268-274.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, t, y = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100]), vec(rand(1:100, 100, 1)) 
g_computer = GComputation(x, t, y, temporal=false)
estimate_causal_effect!(g_computer)
validate(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L96-L148">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.covariate_independence" href="#CausalELM.covariate_independence"><code>CausalELM.covariate_independence</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">covariate_independence(its; kwargs..)</code></pre><p>Test for independence between covariates and the event or intervention.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTImeSeries</code>: an interrupted time seiries estimator.</li></ul><p><strong>Keywords</strong></p><ul><li><code>n::Int</code>: the number of permutations for assigning observations to the pre and        post-treatment periods.</li></ul><p>This is a Chow Test for covariates with p-values estimated via randomization inference. The  p-values are the proportion of times randomly assigning observations to the pre or  post-intervention period would have a larger estimated effect on the the slope of the  covariates. The lower the p-values, the more likely it is that the event/intervention  effected the covariates and they cannot provide an unbiased prediction of the counterfactual  outcomes.</p><p>For more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.</p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), randn(10))
its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
estimate_causal_effect!(its)
covariate_independence(its)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L171-L204">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.omitted_predictor" href="#CausalELM.omitted_predictor"><code>CausalELM.omitted_predictor</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">omitted_predictor(its; kwargs...)</code></pre><p>See how an omitted predictor/variable could change the results of an interrupted time series  analysis.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTImeSeries</code>: an interrupted time seiries estimator.</li></ul><p><strong>Keywords</strong></p><ul><li><code>n::Int</code>: the number of times to simulate a confounder.</li></ul><p><strong>Notes</strong></p><p>This method reestimates interrupted time series models with uniform random variables. If the  included covariates are good predictors of the counterfactual outcome, adding a random  variable as a covariate should not have a large effect on the predicted counterfactual  outcomes and therefore the estimated average effect.</p><p><strong>References</strong></p><p>For more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.</p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), randn(10))
its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
estimate_causal_effect!(its)
omitted_predictor(its)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L223-L256">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.sup_wald" href="#CausalELM.sup_wald"><code>CausalELM.sup_wald</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sup_wald(its; kwargs)</code></pre><p>Check if the predicted structural break is the hypothesized structural break.</p><p><strong>Arguments</strong></p><ul><li><code>its::InterruptedTimeSeries</code>: an interrupted time seiries estimator.</li></ul><p><strong>Keywords</strong></p><ul><li><code>n::Int</code>: the number of times to simulate a confounder.</li><li><code>low::Float64</code>=0.15: the minimum proportion of data points to include before or after the        tested break in the Wald supremum test.</li><li><code>high::Float64=0.85</code>: the maximum proportion of data points to include before or after the        tested break in the Wald supremum test.</li></ul><p><strong>Notes</strong></p><p>This method conducts Wald tests and identifies the structural break with the highest Wald  statistic. If this break is not the same as the hypothesized break, it could indicate an  anticipation effect, confounding by some other event or intervention, or that the  intervention or policy took place in multiple phases. p-values are estimated using  approximate randomization inference and represent the proportion of times we would see a  larger Wald statistic if the data points were randomly allocated to pre and post-event  periods for the predicted structural break.</p><p><strong>References</strong></p><p>For more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.</p><p>For a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), randn(10))
its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)
estimate_causal_effect!(its)
sup_wald(its)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L283-L322">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.p_val" href="#CausalELM.p_val"><code>CausalELM.p_val</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">p_val(x, y, β; kwargs...)</code></pre><p>Estimate the p-value for the hypothesis that an event had a statistically significant effect  on the slope of a covariate using randomization inference.</p><p><strong>Arguments</strong></p><ul><li><code>x::Array{&lt;:Real}</code>: covariates.</li><li><code>y::Array{&lt;:Real}</code>: the outcome.</li><li><code>β::Array{&lt;:Real}</code>=0.15: the fitted weights.</li></ul><p><strong>Keywords</strong></p><ul><li><code>two_sided::Bool=false</code>: whether to conduct a one-sided hypothesis test.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, y, β = reduce(hcat, (float(rand(0:1, 10)), ones(10))), rand(10), 0.5
p_val(x, y, β)
p_val(x, y, β; n=100, two_sided=true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L349-L369">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.counterfactual_consistency" href="#CausalELM.counterfactual_consistency"><code>CausalELM.counterfactual_consistency</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">counterfactual_consistency(m; kwargs...)</code></pre><p><strong>Arguments</strong></p><ul><li><code>m::Union{CausalEstimator, Metalearner}</code>: a model to validate/test the assumptions of.</li></ul><p><strong>Keywords</strong></p><ul><li><code>num_treatments=5::Int</code>: the maximum number of treatments to use when testing the        plausability of the counterfactual consistency assumption.</li></ul><p><strong>Notes</strong></p><p>Examine the counterfactual consistency assumption. First, this function generates Jenks  breaks based on outcome values for the treatment group. Then, it replaces treatment statuses  with the numbers corresponding to each group. Next, it runs two models, one for the  treatment group, one with and one without the fake treatment assignemnts generated by  the Jenks breaks. Finally, it subtracts the mean squared error from the regression with real  data from the mean squared error from the regression with the fake treatment statuses. If  this number is negative, it might indicate a violation of the counterfactual consistency  assumption or omitted variable bias.</p><p><strong>References</strong></p><p>For a primer on G-computation and its assumptions see:     Naimi, Ashley I., Stephen R. Cole, and Edward H. Kennedy. &quot;An introduction to g      methods.&quot; International journal of epidemiology 46, no. 2 (2017): 756-762.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, t, y = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100], vec(rand(1:100, 100, 1)))
g_computer = GComputation(x, t, y, temporal=false)
estimate_causal_effect!(g_computer)
counterfactual_consistency(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L395-L427">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.exchangeability" href="#CausalELM.exchangeability"><code>CausalELM.exchangeability</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">exchangeability(model)</code></pre><p>Test the sensitivity of a G-computation or doubly robust estimator or metalearner to a  violation of the exchangeability assumption.</p><p><strong>References</strong></p><p>For more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. &quot;Sensitivity analysis in observational research:      introducing the E-value.&quot; Annals of internal medicine 167, no. 4 (2017): 268-274.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, t, y = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100], vec(rand(1:100, 100, 1)))
g_computer = GComputation(x, t, y, temporal=false)
estimate_causal_effect!(g_computer)
e_value(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L441-L459">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.e_value" href="#CausalELM.e_value"><code>CausalELM.e_value</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">e_value(model)</code></pre><p>Test the sensitivity of an estimator to a violation of the exchangeability assumption.</p><p><strong>References</strong></p><p>For more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. &quot;Sensitivity analysis in observational research:      introducing the E-value.&quot; Annals of internal medicine 167, no. 4 (2017): 268-274.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, t, y = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100], vec(rand(1:100, 100, 1)))
g_computer = GComputation(x, t, y, temporal=false)
estimate_causal_effect!(g_computer)
e_value(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L462-L479">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.binarize" href="#CausalELM.binarize"><code>CausalELM.binarize</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">binarize(x, cutoff)</code></pre><p>Convert a vector of counts or a continuous vector to a binary vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">binarize([1, 2, 3], 2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L490-L499">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.risk_ratio" href="#CausalELM.risk_ratio"><code>CausalELM.risk_ratio</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">risk_ratio(model)</code></pre><p>Calculate the risk ratio for an estimated model.</p><p><strong>Notes</strong></p><p>If the treatment variable is not binary and the outcome variable is not continuous then the  treatment variable will be binarized.</p><p><strong>References</strong></p><p>For more information on how other quantities of interest are converted to risk ratios see:     VanderWeele, Tyler J., and Peng Ding. &quot;Sensitivity analysis in observational research:      introducing the E-value.&quot; Annals of internal medicine 167, no. 4 (2017): 268-274.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, t, y = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100], vec(rand(1:100, 100, 1)))
g_computer = GComputation(x, t, y, temporal=false)
estimate_causal_effect!(g_computer)
risk_ratio(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L510-L531">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.positivity" href="#CausalELM.positivity"><code>CausalELM.positivity</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">positivity(model, [,min], [,max])</code></pre><p>Find likely violations of the positivity assumption.</p><p><strong>Notes</strong></p><p>This method uses an extreme learning machine or regularized extreme learning machine to  estimate probabilities of treatment. The returned matrix, which may be empty, are the  covariates that have a (near) zero probability of treatment or near zero probability of  being assigned to the control group, whith their entry in the last column being their  estimated treatment probability. In other words, they likely violate the positivity  assumption.</p><p><strong>Arguments</strong></p><ul><li><code>model::Union{CausalEstimator, Metalearner}</code>: a model to validate/test the assumptions of.</li><li><code>min::Float64</code>=1.0e-6: minimum probability of treatment for the positivity assumption.</li><li><code>high::Float64=1-min</code>: the maximum probability of treatment for the positivity assumption.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, t, y = rand(100, 5), Float64.([rand()&lt;0.4 for i in 1:100], vec(rand(1:100, 100, 1)))
g_computer = GComputation(x, t, y, temporal=false)
estimate_causal_effect!(g_computer)
positivity(g_computer)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L610-L635">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.sums_of_squares" href="#CausalELM.sums_of_squares"><code>CausalELM.sums_of_squares</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sums_of_squares(data, num_classes)</code></pre><p>Calculate the minimum sum of squares for each data point and class for the Jenks breaks  algorithm.</p><p><strong>Notes</strong></p><p>This should not be called by the user.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">sums_of_squares([1, 2, 3, 4, 5], 2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L682-L695">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.class_pointers" href="#CausalELM.class_pointers"><code>CausalELM.class_pointers</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">class_pointers(data, num_classes, sums_of_sqs)</code></pre><p>Compute class pointers that minimize the sum of squares for Jenks breaks.</p><p><strong>Notes</strong></p><p>This should not be callled by the user.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">sums_squares = sums_of_squares([1, 2, 3, 4, 5], 2)
class_pointers([1, 2, 3, 4, 5], 2, sums_squares)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L716-L729">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.backtrack_to_find_breaks" href="#CausalELM.backtrack_to_find_breaks"><code>CausalELM.backtrack_to_find_breaks</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">backtrack_to_find_breaks(data, num_classes, sums_of_sqs)</code></pre><p>Determine break points from class assignments.</p><p><strong>Notes</strong></p><p>This should not be called by the user.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">data = [1, 2, 3, 4, 5]
ptr = class_pointers([1, 2, 3, 4, 5], 2, sums_of_squares([1, 2, 3, 4, 5], 2))
backtrack_to_find_breaks([1, 2, 3, 4, 5], ptr)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L750-L764">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.variance" href="#CausalELM.variance"><code>CausalELM.variance</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">variance(data)</code></pre><p>Calculate the variance of some numbers.</p><p><strong>Notes</strong></p><p>This function does not use Besel&#39;s correction.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">variance([1, 2, 3, 4, 5])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L786-L798">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.best_splits" href="#CausalELM.best_splits"><code>CausalELM.best_splits</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">best_splits(data, num_classes)</code></pre><p>Find the best number of splits for Jenks breaks.</p><p><strong>Notes</strong></p><p>This function finds the best number of splits by finding the number of splits that results  in the greatest decrease in the slope of the line between itself and its GVF and the next  higher number of splits and its GVF. This is the same thing as the elbow method.</p><p>This should nto be called by the user.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">best_splits(collect(1:10), 5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L806-L822">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.group_by_class" href="#CausalELM.group_by_class"><code>CausalELM.group_by_class</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">group_by_class(data, classes)</code></pre><p>Group data points into vectors such that data points assigned to the same class are in the  same vector.</p><p><strong>Notes</strong></p><p>This should nto be called by the user.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">group_by_class([1, 2, 3, 4, 5], [1, 1, 1, 2, 3])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L836-L849">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.jenks_breaks" href="#CausalELM.jenks_breaks"><code>CausalELM.jenks_breaks</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">jenks_breaks(data, num_classes)</code></pre><p>Generate Jenks breaks for a vector of real numbers.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">jenks_breaks([1, 2, 3, 4, 5], 3)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L867-L876">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.fake_treatments" href="#CausalELM.fake_treatments"><code>CausalELM.fake_treatments</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fake_treatments(data, num_classes)</code></pre><p>Generate fake treatment statuses corresponding to the classes assigned by the Jenks breaks  algorithm.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">fake_treatments([1, 2, 3, 4, 5], 4)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L886-L896">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.sdam" href="#CausalELM.sdam"><code>CausalELM.sdam</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sdam(x)</code></pre><p>Calculate the sum of squared deviations for array mean for a set of sub arrays.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">sdam([5, 4, 9, 10])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L919-L928">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.scdm" href="#CausalELM.scdm"><code>CausalELM.scdm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sdcm(x)</code></pre><p>Calculate the sum of squared deviations for class means for a set of sub arrays.</p><p><strong>Examples</strong></p><pre><code class="language-juliascdm hljs"></code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L935-L943">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.gvf" href="#CausalELM.gvf"><code>CausalELM.gvf</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">gvf(x)</code></pre><p>Calculate the goodness of variance fit for a set of sub vectors.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">gvf([[4, 5], [9, 10]])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L946-L955">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.var_type" href="#CausalELM.var_type"><code>CausalELM.var_type</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">var_type(x)</code></pre><p>Determine the type of variable held by a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">var_type([1, 2, 3, 2, 3, 1, 1, 3, 2])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/model_validation.jl#L13-L22">source</a></section></article><h2 id="Validation-Metrics"><a class="docs-heading-anchor" href="#Validation-Metrics">Validation Metrics</a><a id="Validation-Metrics-1"></a><a class="docs-heading-anchor-permalink" href="#Validation-Metrics" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.mse" href="#CausalELM.mse"><code>CausalELM.mse</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mse(y, ŷ)</code></pre><p>Calculate the mean squared error</p><p>See also <a href="#CausalELM.mae"><code>mae</code></a>.</p><p>Examples</p><pre><code class="language-julia hljs">mse([0.0, 0.0, 0.0], [0.0, 0.0, 0.0])
mse([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metrics.jl#L3-L15">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.mae" href="#CausalELM.mae"><code>CausalELM.mae</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mae(y, ŷ)</code></pre><p>Calculate the mean absolute error</p><p>See also <a href="#CausalELM.mse"><code>mse</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">mae([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])
mae([1.0, 1.0, 1.0], [2.0, 2.0, 2.0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metrics.jl#L24-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.accuracy" href="#CausalELM.accuracy"><code>CausalELM.accuracy</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">accuracy(y, ŷ)</code></pre><p>Calculate the accuracy for a classification task</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">accuracy([1, 1, 1, 1], [0, 1, 1, 0])
accuracy([1, 2, 3, 4], [1, 1, 1, 1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metrics.jl#L45-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Base.precision" href="#Base.precision"><code>Base.precision</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">precision(y, ŷ)</code></pre><p>Calculate the precision for a classification task</p><p>See also <a href="#CausalELM.recall"><code>recall</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">precision([0, 1, 0, 0], [0, 1, 1, 0])
precision([0, 1, 0, 0], [0, 1, 0, 0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metrics.jl#L70-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.recall" href="#CausalELM.recall"><code>CausalELM.recall</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">recall(y, ŷ)</code></pre><p>Calculate the recall for a classification task</p><p>See also <a href="#Base.precision"><code>precision</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">recall([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])
recall([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metrics.jl#L97-L109">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.F1" href="#CausalELM.F1"><code>CausalELM.F1</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">F1(y, ŷ)</code></pre><p>Calculate the F1 score for a classification task</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">F1([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])
F1([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metrics.jl#L124-L134">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.confusion_matrix" href="#CausalELM.confusion_matrix"><code>CausalELM.confusion_matrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">confusion_matrix(y, ŷ)</code></pre><p>Generate a confusion matrix</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">confusion_matrix([1, 1, 1, 1, 0], [1, 1, 1, 1, 0])
confusion_matrix([1, 1, 1, 1, 0, 2], [1, 1, 1, 1, 0, 2])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/metrics.jl#L140-L150">source</a></section></article><h2 id="Extreme-Learning-Machines"><a class="docs-heading-anchor" href="#Extreme-Learning-Machines">Extreme Learning Machines</a><a id="Extreme-Learning-Machines-1"></a><a class="docs-heading-anchor-permalink" href="#Extreme-Learning-Machines" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.fit!" href="#CausalELM.fit!"><code>CausalELM.fit!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">fit!(model)</code></pre><p>Make predictions with an ExtremeLearner.</p><p><strong>References</strong></p><p>For more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. &quot;Extreme learning machine: theory      and applications.&quot; Neurocomputing 70, no. 1-3 (2006): 489-501.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
m1 = ExtremeLearner(x, y, 10, σ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L78-L93">source</a></section><section><div><pre><code class="nohighlight hljs">fit!(model)</code></pre><p>Fit a Regularized Extreme Learner.</p><p><strong>References</strong></p><p>For more details see:      Li, Guoqiang, and Peifeng Niu. &quot;An enhanced extreme learning machine based on ridge      regression for regression.&quot; Neural Computing and Applications 22, no. 3 (2013):      803-810.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
m1 = RegularizedExtremeLearner(x, y, 10, σ)
f1 = fit!(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L102-L119">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.predict" href="#CausalELM.predict"><code>CausalELM.predict</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predict(model, X)</code></pre><p>Use an ExtremeLearningMachine to make predictions.</p><p><strong>References</strong></p><p>For more details see:      Huang G-B, Zhu Q-Y, Siew C. Extreme learning machine: theory and applications.      Neurocomputing. 2006;70:489–501. https://doi.org/10.1016/j.neucom.2005.12.126</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
m1 = ExtremeLearner(x, y, 10, σ)
f1 = fit(m1, sigmoid)
julia&gt; predict(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L132-L149">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.predict_counterfactual!" href="#CausalELM.predict_counterfactual!"><code>CausalELM.predict_counterfactual!</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">predictcounterfactual(model, X)</code></pre><p>Use an ExtremeLearningMachine to predict the counterfactual.</p><p><strong>Notes</strong></p><p>This should be run with the observed covariates. To use synthtic data for what-if scenarios  use predict.</p><p>See also <a href="#CausalELM.predict"><code>predict</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
m1 = ExtremeLearner(x, y, 10, σ)
f1 = fit(m1, sigmoid)
predict_counterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L158-L176">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.placebo_test" href="#CausalELM.placebo_test"><code>CausalELM.placebo_test</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">placebo_test(model)</code></pre><p>Conduct a placebo test.</p><p><strong>Notes</strong></p><p>This method makes predictions for the post-event or post-treatment period using data  in the pre-event or pre-treatment period and the post-event or post-treament. If there is a statistically significant difference between these predictions the study design may be flawed. Due to the multitude of significance tests for time series data, this function returns the predictions but does not test for statistical significance.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">x, y = [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0], [0.0, 1.0, 0.0, 1.0]
m1 = ExtremeLearner(x, y, 10, σ)
f1 = fit(m1, sigmoid)
predict_counterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])
placebo_test(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L183-L203">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.ridge_constant" href="#CausalELM.ridge_constant"><code>CausalELM.ridge_constant</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">ridge_constant(model, [,iterations])</code></pre><p>Calculate the L2 penalty for a regularized extreme learning machine using generalized cross  validation with successive halving.</p><p><strong>Arguments</strong></p><ul><li><code>model::RegularizedExtremeLearner</code>: a regularized extreme learning machine</li><li><code>iterations::Int</code>: the number of iterations to perform for successive halving.</li></ul><p><strong>References</strong></p><p>For more information see:      Golub, Gene H., Michael Heath, and Grace Wahba. &quot;Generalized cross-validation as a      method for choosing a good ridge parameter.&quot; Technometrics 21, no. 2 (1979): 215-223.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">m1 = RegularizedExtremeLearner(x, y, 10, σ)
ridge_constant(m1)
ridge_constant(m1, iterations=20)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L212-L233">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.set_weights_biases" href="#CausalELM.set_weights_biases"><code>CausalELM.set_weights_biases</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">set_weights_biases(model)</code></pre><p>Calculate the weights and biases for an extreme learning machine or regularized extreme  learning machine.</p><p><strong>References</strong></p><p>For details see;     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. &quot;Extreme learning machine: theory      and applications.&quot; Neurocomputing 70, no. 1-3 (2006): 489-501.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">m1 = RegularizedExtremeLearner(x, y, 10, σ)
set_weights_biases(m1)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/models.jl#L262-L278">source</a></section></article><h2 id="Utility-Functions"><a class="docs-heading-anchor" href="#Utility-Functions">Utility Functions</a><a id="Utility-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="CausalELM.mean" href="#CausalELM.mean"><code>CausalELM.mean</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mean(x)</code></pre><p>Calculate the mean of a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">mean([1, 2, 3, 4])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/utilities.jl#L1-L10">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.var" href="#CausalELM.var"><code>CausalELM.var</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">var(x)</code></pre><p>Calculate the (sample) mean of a vector.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">var([1, 2, 3, 4])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/utilities.jl#L13-L22">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.consecutive" href="#CausalELM.consecutive"><code>CausalELM.consecutive</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">consecutive(x)</code></pre><p>Subtract consecutive elements in a vector.</p><p><strong>Notes</strong></p><p>This function is only used to create a rolling average for interrupted time series analysis.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">consecutive([1, 2, 3, 4, 5])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/utilities.jl#L26-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="CausalELM.one_hot_encode" href="#CausalELM.one_hot_encode"><code>CausalELM.one_hot_encode</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">one_hot_encode(x)</code></pre><p>One hot encode a categorical vector for multiclass classification.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">one_hot_encode([1, 2, 3, 4, 5])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dscolby/CausalELM.jl/blob/c0a10d39cd71b4938a991237e907374547c36c8d/src/utilities.jl#L41-L50">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../guide/doublyrobust/">« Doubly Robust Estimation</a><a class="docs-footer-nextpage" href="../contributing/">Contributing »</a><div class="flexbox-break"></div><p class="footer-message">© 2024 Darren Colby</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Sunday 12 May 2024 19:06">Sunday 12 May 2024</span>. Using Julia version 1.8.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
