var documenterSearchIndex = {"docs":
[{"location":"release_notes/#Release-Notes","page":"Release Notes","title":"Release Notes","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"These release notes adhere to the keep a changelog format. Below is a list of changes since causalELM was first released.","category":"page"},{"location":"release_notes/#Version-[v0.5.1](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.5.1)-2024-01-15","page":"Release Notes","title":"Version v0.5.1 - 2024-01-15","text":"","category":"section"},{"location":"release_notes/#Added","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"More descriptive docstrings #21","category":"page"},{"location":"release_notes/#Fixed","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Permutation of continuous treatments draws from a continuous, instead of discrete uniform distribution during randomization inference","category":"page"},{"location":"release_notes/#Version-[v0.5.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.5.0)-2024-01-13","page":"Release Notes","title":"Version v0.5.0 - 2024-01-13","text":"","category":"section"},{"location":"release_notes/#Added-2","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Constructors for estimators taht accept dataframes from DataFrames.jl #25","category":"page"},{"location":"release_notes/#Changed","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Estimators can handle any array whose values are <:Real #23\nEstimator constructors are now called with model(X, T, Y) instead of model(X, Y, T)\nRemoved excess type constraints for many methods #23\nVectorized a few for loops\nIncreased test coverage","category":"page"},{"location":"release_notes/#Version-[v0.4.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.4.0)-2024-01-06","page":"Release Notes","title":"Version v0.4.0 - 2024-01-06","text":"","category":"section"},{"location":"release_notes/#Added-3","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"R-learning\nSoftmax function for arrays","category":"page"},{"location":"release_notes/#Changed-2","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Moved all types and methods under the main module\nDecreased size of function definitions #22\nSLearner has a G-computation field that does the heavy lifting for S-learning\nRemoved excess fields from estimator structs","category":"page"},{"location":"release_notes/#Fixed-2","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Changed the incorrect name of DoublyRobustEstimation struct to DoubleMachineLearning\nCaclulation of risk ratios and E-values\nCalculation of validation metrics for multiclass classification\nCalculation of output weights for L2 regularized extreme learning machines","category":"page"},{"location":"release_notes/#Version-[v0.3.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.3.0)-2023-11-25","page":"Release Notes","title":"Version v0.3.0 - 2023-11-25","text":"","category":"section"},{"location":"release_notes/#Added-4","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Splitting of temporal data for cross validation 18\nMethods to validate/test senstivity to violations of identifying assumptions #16","category":"page"},{"location":"release_notes/#Changed-3","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Converted all functions and methods to snake case #17\nRandomization inference for interrupted time series randomizes all the indices #15","category":"page"},{"location":"release_notes/#Fixed-3","page":"Release Notes","title":"Fixed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Issue related to recoding variables to calculate validation metrics for cross validation","category":"page"},{"location":"release_notes/#Version-[v0.2.1](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.2.1)-2023-06-07","page":"Release Notes","title":"Version v0.2.1 - 2023-06-07","text":"","category":"section"},{"location":"release_notes/#Added-5","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Cross fitting to the doubly robust estimator","category":"page"},{"location":"release_notes/#Version-[v0.2.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.2.0)-2023-04-16","page":"Release Notes","title":"Version v0.2.0 - 2023-04-16","text":"","category":"section"},{"location":"release_notes/#Added-6","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Calculation of p-values and standard errors via randomization inference","category":"page"},{"location":"release_notes/#Changed-4","page":"Release Notes","title":"Changed","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Divided package into modules","category":"page"},{"location":"release_notes/#Version-[v0.1.0](https://github.com/dscolby/CausalELM.jl/releases/tag/v0.1.0)-2023-02-14","page":"Release Notes","title":"Version v0.1.0 - 2023-02-14","text":"","category":"section"},{"location":"release_notes/#Added-7","page":"Release Notes","title":"Added","text":"","category":"section"},{"location":"release_notes/","page":"Release Notes","title":"Release Notes","text":"Event study, g-computation, and doubly robust estimators\nS-learning, T-learning, and X-learning\nModel summarization methods","category":"page"},{"location":"api/#causalELM","page":"API","title":"causalELM","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Most of the methods and structs here are private, not exported, should not be called by the  user, and are documented for the purpose of developing CausalELM or to facilitate  understanding of the implementation.","category":"page"},{"location":"api/#Types","page":"API","title":"Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"InterruptedTimeSeries\nGComputation\nDoubleMachineLearning\nSLearner\nTLearner\nRLearner\nCausalELM.CausalEstimator\nCausalELM.Metalearner\nCausalELM.ExtremeLearningMachine\nCausalELM.ExtremeLearner\nCausalELM.RegularizedExtremeLearner\nCausalELM.Nonbinary\nCausalELM.Binary\nCausalELM.Count\nCausalELM.Continuous","category":"page"},{"location":"api/#CausalELM.InterruptedTimeSeries","page":"API","title":"CausalELM.InterruptedTimeSeries","text":"Container for the results of an interrupted time series analysis\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.GComputation","page":"API","title":"CausalELM.GComputation","text":"Container for the results of G-Computation\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.DoubleMachineLearning","page":"API","title":"CausalELM.DoubleMachineLearning","text":"Container for the results of a double machine learning estimator\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.SLearner","page":"API","title":"CausalELM.SLearner","text":"S-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.TLearner","page":"API","title":"CausalELM.TLearner","text":"T-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.RLearner","page":"API","title":"CausalELM.RLearner","text":"Container for the results of an R-learner\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.CausalEstimator","page":"API","title":"CausalELM.CausalEstimator","text":"Abstract type for GComputation and DoubleMachineLearning\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Metalearner","page":"API","title":"CausalELM.Metalearner","text":"Abstract type for metalearners\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.ExtremeLearningMachine","page":"API","title":"CausalELM.ExtremeLearningMachine","text":"Abstract type that includes vanilla and L2 regularized Extreme Learning Machines\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.ExtremeLearner","page":"API","title":"CausalELM.ExtremeLearner","text":"Struct to hold data for an Extreme Learning machine\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.RegularizedExtremeLearner","page":"API","title":"CausalELM.RegularizedExtremeLearner","text":"Struct to hold data for a regularized Extreme Learning Machine\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Nonbinary","page":"API","title":"CausalELM.Nonbinary","text":"Abstract type used to dispatch risk_ratio on nonbinary treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Binary","page":"API","title":"CausalELM.Binary","text":"Type used to dispatch risk_ratio on binary treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Count","page":"API","title":"CausalELM.Count","text":"Type used to dispatch risk_ratio on count and categorical treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.Continuous","page":"API","title":"CausalELM.Continuous","text":"Type used to dispatch risk_ratio on continuous treatments\n\n\n\n\n\n","category":"type"},{"location":"api/#Activation-Functions","page":"API","title":"Activation Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"binary_step\nσ\ntanh\nrelu\nleaky_relu\nswish\nsoftmax\nsoftplus\ngelu\ngaussian\nhard_tanh\nelish\nfourier","category":"page"},{"location":"api/#CausalELM.binary_step","page":"API","title":"CausalELM.binary_step","text":"binary_step(x)\n\nApply the binary step activation function to a real number.\n\nExamples\n\njulia> binary_step(1)\n 1.0\n\n\n\n\n\nbinary_step(x)\n\nApply the binary step activation function to an array.\n\nExamples\n\njulia> binary_step([-1000, 100, 1, 0, -0.001, -3])\n6-element Vector{Float64}\n 0.0 \n 1.0 \n 1.0 \n 1.0 \n 0.0 \n 0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.σ","page":"API","title":"CausalELM.σ","text":"σ(x)\n\nApply the sigmoid activation function to a real number.\n\nExamples\n\njulia> σ(1)\n 0.7310585786300049\n\n\n\n\n\nσ(x)\n\nApply the sigmoid activation function to an array.\n\nExamples\n\njulia> σ([1, 0])\n2-element Vector{Float64}\n 0.7310585786300049\n 0.5\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.tanh","page":"API","title":"Base.tanh","text":"tanh(x)\n\nApply the tanh activation function to an array.\n\nThis is just a vectorized version of Base.tanh\n\nExamples\n\njulia> tanh([1, 0])\n2-element Vector{Float64}\n 0.7615941559557649 \n 0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.relu","page":"API","title":"CausalELM.relu","text":"relu(x)\n\nApply the ReLU activation function to a real number.\n\nExamples\n\njulia> relu(1)\n 1.0\n\n\n\n\n\nrelu(x)\n\nApply the ReLU activation function to an array.\n\nExamples\n\njulia> relu([1, 0, -1])\n3-element Vector{Float64}\n 1.0 \n 0.0 \n 0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.leaky_relu","page":"API","title":"CausalELM.leaky_relu","text":"leaky_relu(x)\n\nApply the leaky ReLU activation function to a real number.\n\nExamples\n\njulia> leaky_relu(1)\n 1.0\n\n\n\n\n\nleaky_relu(x)\n\nApply the leaky ReLU activation function to an array.\n\nExamples\n\njulia> leaky_relu([-0.01, 0, 1])\n3-element Vector{Float64}\n 1.0 \n 0.0 \n 0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.swish","page":"API","title":"CausalELM.swish","text":"swish(x)\n\nApply the swish activation function to a real number.\n\nExamples\n\njulia> swish(1)\n 0.7310585786300049\n\n\n\n\n\nswish(x)\n\nApply the swish activation function to an array.\n\nExamples\n\njulia> swish([1, 0, -1])\n3-element Vector{Float64}\n 0.7310585786300049 \n 0.0 \n -0.2689414213699951\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.softmax","page":"API","title":"CausalELM.softmax","text":"softmax(x)\n\nApply the softmax activation function to a real number.\n\nExamples\n\njulia> softmax(1)\n 2.718281828459045\n\n\n\n\n\nsoftmax(x)\n\nApply the softmax activation function to a vector.\n\nExamples\n\njulia> softmax([1, 2, 3])\n3-element Vector{Float64}:\n 0.09003057317038046\n 0.24472847105479767\n 0.6652409557748219\n\n\n\n\n\nsoftmax(x)\n\nApply the softmax activation function to the rows of an array.\n\nExamples\n\njulia> x = rand(5, 3)\n5×3 Matrix{Float64}:\n 0.482117  0.225359  0.615589\n 0.255572  0.165051  0.427035\n 0.387384  0.424856  0.369219\n 0.175362  0.172561  0.111878\n 0.508207  0.258347  0.591111\njulia> softmax(x)\n5×3 Matrix{Float64}:\n 0.342895  0.265248  0.391857\n 0.322529  0.294616  0.382855\n 0.331106  0.343749  0.325146\n 0.340635  0.339682  0.319682\n 0.348998  0.271838  0.379164\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.softplus","page":"API","title":"CausalELM.softplus","text":"softplus(x)\n\nApply the softplus activation function to a real number.\n\nExamples\n\njulia> softplus(1)\n 1.3132616875182228\n\n\n\n\n\nsoftplus(x)\n\nApply the softplus activation function to an array.\n\nExamples\n\njulia> softplus([1, -1])\n2-element Vector{Float64}\n 1.3132616875182228 \n 0.31326168751822286\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.gelu","page":"API","title":"CausalELM.gelu","text":"gelu(x)\n\nApply the GeLU activation function to a real number.\n\nExamples\n\njulia> gelu(1)\n 0.8411919906082768\n\n\n\n\n\ngelu(x)\n\nApply the GeLU activation function to an array.\n\nExamples\n\njulia> gelu([-1, 0, 1])\n3-element Vector{Float64}\n -0.15880800939172324 \n 0.0 \n 0.8411919906082768\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.gaussian","page":"API","title":"CausalELM.gaussian","text":"gaussian(x)\n\nApply the gaussian activation function to a real number.\n\nExamples\n\njulia> gaussian(1)\n 0.11443511435028261\n\n\n\n\n\ngaussian(x)\n\nApply the gaussian activation function to an array.\n\nExamples\n\njulia> gaussian([1, -1])\n2-element Vector{Float64}\n 0.36787944117144233 \n 0.36787944117144233\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.hard_tanh","page":"API","title":"CausalELM.hard_tanh","text":"hard_tanh(x)\n\nApply the hard_tanh activation function to a real number.\n\nExamples\n\njulia> hard_tanh(-2)\n -1.0\n\n\n\n\n\nhard_tanh(x)\n\nApply the hard_tanh activation function to an array.\n\nExamples\n\njulia> hard_tanh([-2, 0, 2])\n3-element Vector{Float64}\n -1.0 \n 0.0 \n 1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.elish","page":"API","title":"CausalELM.elish","text":"elish(x)\n\nApply the ELiSH activation function to a real number.\n\nExamples\n\njulia> elish(1)\n 0.7310585786300049\n\n\n\n\n\nelish(x)\n\nApply the ELiSH activation function to an array.\n\nExamples\n\njulia> elish([-1, 1])\n2-element Vector{Float64}\n -0.17000340156854793 \n 0.7310585786300049\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.fourier","page":"API","title":"CausalELM.fourier","text":"fourrier(x)\n\nApply the Fourier activation function to a real number.\n\nExamples\n\njulia> fourier(1)\n 0.8414709848078965\n\n\n\n\n\nfourrier(x)\n\nApply the Fourier activation function to an array.\n\nExamples\n\njulia> fourier([-1, 1])\n2-element Vector{Float64}\n -0.8414709848078965 \n 0.8414709848078965\n\n\n\n\n\n","category":"function"},{"location":"api/#Cross-Validation","page":"API","title":"Cross Validation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.generate_folds\nCausalELM.generate_temporal_folds\nCausalELM.validation_loss\nCausalELM.cross_validate\nCausalELM.best_size\nCausalELM.shuffle_data","category":"page"},{"location":"api/#CausalELM.generate_folds","page":"API","title":"CausalELM.generate_folds","text":"generate_folds(X, Y, folds)\n\nCreates folds for cross validation.\n\nExamples\n\njulia> xfolds, y_folds = generate_folds(zeros(20, 2), zeros(20), 5)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.generate_temporal_folds","page":"API","title":"CausalELM.generate_temporal_folds","text":"generate_folds(X, Y, folds)\n\nCreates rolling folds for cross validation of time series data.\n\nExamples\n\njulia> xfolds, y_folds = generate_temporal_folds(zeros(20, 2), zeros(20), 5, temporal=true)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.validation_loss","page":"API","title":"CausalELM.validation_loss","text":"validation_loss(xtrain, ytrain, xtest, ytest, nodes, metric; <keyword arguments>)\n\nCalculate a validation metric for a single fold in k-fold cross validation.\n\n...\n\nArguments\n\nxtrain::Any: an array of features to train on.\nytrain::Any: an array of training labels.\nxtest::Any: an array of features to test on.\nytrain::Any: an array of testing labels.\nnodes::Int: the number of neurons in the extreme learning machine.\nmetric::Function: the validation metric to calculate.\nactivation::Function=relu: the activation function to use.\nregularized::Function=true: whether to use L2 regularization.\n\n...\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> validation_loss(x, y, 5, accuracy, 3)\n0.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.cross_validate","page":"API","title":"CausalELM.cross_validate","text":"cross_validate(X, Y, neurons, metric, activation, regularized, folds, temporal)\n\n...\n\nArguments\n\nX::Array: an array of features to train on.\nY::Vector: a vector of labels to train on.\nneurons::Int: the number of neurons to use in the extreme learning machine.\nmetric::Function: the validation metric to calculate.\nactivation::Function=relu: the activation function to use.\nregularized::Function=true: whether to use L2 regularization\nfolds::Int: the number of folds to use for cross validation.\ntemporal::Function=true: whether the data is of a time series or panel nature.\n\n...\n\nCalculate a validation metric for k folds using a single set of hyperparameters.\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> cross_validate(x, y, 5, accuracy)\n0.0257841765251021\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.best_size","page":"API","title":"CausalELM.best_size","text":"best_size(X, Y, metric, task, activation, min_neurons, max_neurons, regularized, folds, \n    temporal, iterations, elm_size)\n\nCompute the best number of neurons for an Extreme Learning Machine.\n\nThe procedure tests networks with numbers of neurons in a sequence whose length is given  by iterations on the interval [minneurons, maxneurons]. Then, it uses the networks  sizes and validation errors from the sequence to predict the validation error or metric  for every network size between minneurons and maxneurons using the function  approximation ability of an Extreme Learning Machine. Finally, it returns the network  size with the best predicted validation error or metric.\n\n...\n\nArguments\n\nX::Array: an array of features to train on.\nY::Vector: a vector of labels to train on.\nmetric::Function: the validation metric to calculate.\ntask::String: either regression or classification.\nactivation::Function=relu: the activation function to use.\nmin_neurons::Int: the minimum number of neurons to consider for the extreme learner.\nmax_neurons::Int: the maximum number of neurons to consider for the extreme learner.\nregularized::Function=true: whether to use L2 regularization\nfolds::Int: the number of folds to use for cross validation.\ntemporal::Function=true: whether the data is of a time series or panel nature.\niterations::Int: the number of iterations to perform cross validation between    minneurons and maxneurons.\nelm_size::Int: the number of nuerons in the validation loss approximator network.\n\n...\n\nExamples\n\njulia> best_size(rand(100, 5), rand(100), mse, \"regression\")\n11\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.shuffle_data","page":"API","title":"CausalELM.shuffle_data","text":"shuffle_data(X, Y)\n\nShuffles covariates and outcome vector for cross validation.\n\nExamples\n\njulia> x, y, t = rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> shuffle_data(x, y, t)\n([0.6124923085225416 0.2713900065807924 … 0.6094796972512194 0.6067966603192685; \n0.7186612932571539 0.8047878363606299 … 0.9787878554455594 0.885819212905816; … ; \n0.773543733306263 0.10880091279797399 … 0.10525512055751185 0.6303472234021711; \n0.10845217539341823 0.9911071602976902 … 0.014754069216096566 0.5256103389041187], \n[0.4302689295553531, 0.2396683446618325, 0.7954433314513768, 0.7191098533903124, \n0.8168563428651753, 0.7064320936729905, 0.048113106979693065, 0.3102938851371281, \n0.6246380539228858, 0.3510284321966193  …  0.5324022501182528, 0.8354720951777901, \n0.7526652774981095, 0.3639742621882005, 0.21030903031988923, 0.6936212944871928, \n0.3910592143534404, 0.15152013651215301, 0.38891692138831735, 0.08827711410802941], \nFloat64[0, 0, 1, 1, 0, 1, 0, 0, 1, 0  …  0, 0, 1, 1, 1, 1, 0, 1, 0, 0])\n\n\n\n\n\n","category":"function"},{"location":"api/#Average-Causal-Effect-Estimators","page":"API","title":"Average Causal Effect Estimators","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"XLearner\nCausalELM.estimate_effect!\nCausalELM.predict_residuals\nCausalELM.moving_average","category":"page"},{"location":"api/#CausalELM.XLearner","page":"API","title":"CausalELM.XLearner","text":"X-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"api/#CausalELM.estimate_effect!","page":"API","title":"CausalELM.estimate_effect!","text":"estimate_effect!(DML, cate=false)\n\nEstimate a treatment effect using double machine learning.\n\nThis method should not be called directly.\n\n...\n\nArguments\n\nDML::DoubleMachineLearning: the DoubleMachineLearning struct to estimate the effect for.\ncate::Bool=fales: whether to estimate the cate.\n\n...\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> estimate_effect!(m1)\n 0.31067439\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.predict_residuals","page":"API","title":"CausalELM.predict_residuals","text":"predict_residuals(DML, x_train, x_test, y_train, y_test, t_train, t_test)\n\nPredict treatment and outcome residuals for doubl machine learning.\n\nThis method should not be called directly.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> x_train, x_test = X[1:80, :], X[81:end, :]\njulia> y_train, y_test = Y[1:80], Y[81:end]\njulia> t_train, t_test = T[1:80], T[81:100]\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> predict_residuals(m1, x_train, x_test, y_train, y_test, t_train, t_test)\n100-element Vector{Float64}\n 0.6944714802199426\n 0.6102318624294397\n 0.9563033347529682\n ⋮\n 0.14016601301278353, \n 0.2217194742841072\n 0.199372555924635\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.moving_average","page":"API","title":"CausalELM.moving_average","text":"moving_average(x)\n\nCalculates a cumulative moving average.\n\nExamples\n\njulia> moving_average([1, 2, 3])\n3-element Vector{Float64}\n 1.0\n 1.5\n 2.0\n\n\n\n\n\n","category":"function"},{"location":"api/#Metalearners","page":"API","title":"Metalearners","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.stage1!\nCausalELM.stage2!","category":"page"},{"location":"api/#CausalELM.stage1!","page":"API","title":"CausalELM.stage1!","text":"stage1!(x)\n\nEstimate the first stage models for an X-learner.\n\nThis method should not be called by the user.\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> stage1!(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.stage2!","page":"API","title":"CausalELM.stage2!","text":"stage2!(x)\n\nEstimate the second stage models for an X-learner.\n\nThis method should not be called by the user.\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> stage1!(m1)\njulia> stage2!(m1)\n100-element Vector{Float64}\n 0.6579129842054047\n 0.7644471766429705\n 0.5462780002052421\n ⋮\n 0.8755515354984005\n 0.947588000142362\n 0.29294343704001025\n\n\n\n\n\n","category":"function"},{"location":"api/#Common-Methods","page":"API","title":"Common Methods","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"estimate_causal_effect!","category":"page"},{"location":"api/#CausalELM.estimate_causal_effect!","page":"API","title":"CausalELM.estimate_causal_effect!","text":"estimate_causal_effect!(its)\n\nEstimate the effect of an event relative to a predicted counterfactual.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> estimate_causal_effect!(m1)\n 0.25714308\n\n\n\n\n\nestimate_causal_effect!(g)\n\nEstimate a causal effect of interest using G-Computation.\n\nIf treatents are administered at multiple time periods, the effect will be estimated as the  average difference between the outcome of being treated in all periods and being treated in  no periods.For example, given that individuals 1, 2, ..., i ∈ I recieved either a treatment  or a placebo in p different periods, the model would estimate the average treatment effect  as E[Yᵢ|T₁=1, T₂=1, ... Tₚ=1, Xₚ] - E[Yᵢ|T₁=0, T₂=0, ... Tₚ=0, Xₚ].\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = GComputation(X, T, Y)\njulia> estimate_causal_effect!(m1)\n 0.31067439\n\n\n\n\n\nestimate_causal_effect!(DML)\n\nEstimate a causal effect of interest using double machine learning.\n\nUnlike other estimators, this method does not support time series or panel data. This method  also does not work as well with smaller datasets because it estimates separate outcome  models for the treatment and control groups.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = DoubleMachineLearning(X, T, Y)\njulia> estimate_causal_effect!(m1)\n 0.31067439\n\n\n\n\n\nestimate_causal_effect!(s)\n\nEstimate the CATE using an S-learner.\n\nFor an overview of S-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m4 = SLearner(X, T, Y)\njulia> estimate_causal_effect!(m4)\n100-element Vector{Float64}\n 0.20729633391630697\n 0.20729633391630697\n 0.20729633391630692\n ⋮\n 0.20729633391630697\n 0.20729633391630697\n 0.20729633391630697\n\n\n\n\n\nestimate_causal_effect!(t)\n\nEstimate the CATE using an T-learner.\n\nFor an overview of T-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m5 = TLearner(X, T, Y)\njulia> estimate_causal_effect!(m5)\n100-element Vector{Float64}\n 0.0493951571746305\n 0.049395157174630444\n 0.0493951571746305\n ⋮ \n 0.049395157174630444\n 0.04939515717463039\n 0.049395157174630444\n\n\n\n\n\nestimate_causal_effect!(x)\n\nEstimate the CATE using an X-learner.\n\nFor an overview of X-learning see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = XLearner(X, T, Y)\njulia> estimate_causal_effect!(m1)\n-0.025012644892878473\n-0.024634294305967294\n-0.022144246680543364\n⋮ \n-0.021163590874553318\n-0.014607310062509895\n-0.022449034332142046\n\n\n\n\n\nestimate_causal_effect!(R)\n\nEstimate the CATE using an R-learner.\n\nFor an overview of R-learning see:     Nie, Xinkun, and Stefan Wager. \"Quasi-oracle estimation of heterogeneous treatment      effects.\" Biometrika 108, no. 2 (2021): 299-319.\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = RLearner(X, T, Y)\njulia> estimate_causal_effect!(m1)\n -0.025012644892878473\n -0.024634294305967294\n -0.022144246680543364\n ⋮\n -0.021163590874553318\n -0.014607310062509895 \n-0.022449034332142046\n\n\n\n\n\n","category":"function"},{"location":"api/#Inference","page":"API","title":"Inference","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"summarize\nCausalELM.generate_null_distribution\nCausalELM.quantities_of_interest","category":"page"},{"location":"api/#CausalELM.summarize","page":"API","title":"CausalELM.summarize","text":"summarize(its, mean_effect)\n\nReturn a summary from an interrupted time series estimator.\n\np-values and standard errors are estimated using approximate randomization inference that permutes the time of the intervention.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> estimate_causal_effect!(m1)\n1-element Vector{Float64}\n 0.25714308\njulia> summarize(m1)\n {\"Task\" => \"Regression\", \"Regularized\" => true, \"Activation Function\" => relu, \n \"Validation Metric\" => \"mse\",\"Number of Neurons\" => 2, \n \"Number of Neurons in Approximator\" => 10, \"β\" => [0.25714308], \n \"Causal Effect\" => -3.9101138, \"Standard Error\" => 1.903434356, \"p-value\" = 0.00123356}\n\n\n\n\n\nsummarize(mod, n)\n\nReturn a summary from a CausalEstimator or Metalearner.\n\np-values and standard errors are estimated using approximate randomization inference.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = GComputation(X, T, Y)\njulia> estimate_causal_effect!(m1)\n 0.3100468253\njulia> summarize(m1)\n {\"Task\" => \"Regression\", \"Quantity of Interest\" => \"ATE\", Regularized\" => \"true\", \n \"Activation Function\" => \"relu\", \"Time Series/Panel Data\" => \"false\", \n \"Validation Metric\" => \"mse\",\"Number of Neurons\" => \"5\", \n \"Number of Neurons in Approximator\" => \"10\", \"Causal Effect: 0.00589761, \n \"Standard Error\" => 5.12900734, \"p-value\" => 0.479011245} \n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = RLearner(X, T, Y)\njulia> estimate_causal_effect(m1)\n1-element Vector{Float64}\n [0.5804032956]\njulia> summarize(m1)\n {\"Task\" => \"Regression\", \"Quantity of Interest\" => \"ATE\", Regularized\" => \"true\", \n \"Activation Function\" => \"relu\", \"Validation Metric\" => \"mse\", \"Number of Neurons\" => \"5\", \n \"Number of Neurons in Approximator\" => \"10\", \"Causal Effect\" = 0.5804032956, \n \"Standard Error\" => 2.129400324, \"p-value\" => 0.0008342356}\n\njulia> X, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\njulia> m1 = SLearner(X, T, Y)\njulia> estimate_causal_effect!(m1)\n100-element Vector{Float64}\n 0.20729633391630697\n 0.20729633391630697\n 0.20729633391630692\n ⋮\n 0.20729633391630697\n 0.20729633391630697\n 0.20729633391630697\njulia> summarise(m1)\n {\"Task\" => \"Regression\", Regularized\" => \"true\", \"Activation Function\" => \"relu\", \n \"Validation Metric\" => \"mse\", \"Number of Neurons\" => \"5\", \n \"Number of Neurons in Approximator\" => \"10\", \n \"Causal Effect: [0.20729633391630697, 0.20729633391630697, 0.20729633391630692, \n 0.20729633391630697, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697, \n 0.20729633391630703, 0.20729633391630697, 0.20729633391630697  …  0.20729633391630703, \n 0.20729633391630697, 0.20729633391630692, 0.20729633391630703, 0.20729633391630697, \n 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, 0.20729633391630697, \n 0.20729633391630697], \"Standard Error\" => 5.3121435085, \"p-value\" => 0.0632454855}\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.generate_null_distribution","page":"API","title":"CausalELM.generate_null_distribution","text":"generate_null_distribution(mod, n)\n\nGenerate a null distribution for the treatment effect of G-computation, double machine  learning, or metalearning.\n\nThis method estimates the same model that is provided using random permutations of the  treatment assignment to generate a vector of estimated effects under different treatment regimes. When mod is a metalearner the null statistic is the difference is the ATE.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nExamples\n\njulia> x, t, y = rand(100, 5), [rand()<0.4 for i in 1:100], rand(1:100, 100, 1)\njulia> g_computer = GComputation(x, t, y)\njulia> estimate_causal_effect!(g_computer)\njulia> generate_null_distribution(g_computer, 500)\n500-element Vector{Float64}\n500-element Vector{Float64}\n 0.016297180690693656\n 0.0635928694685571\n 0.20004144093635673\n ⋮\n 24.739658523175912\n 25.30523686137909\n 28.07474553316176\n\n\n\n\n\ngenerate_null_distribution(its, n, mean_effect)\n\nGenerate a null distribution for the treatment effect in an interrupted time series  analysis. By default, this method generates a null distribution of mean differences. To  generate a null distribution of cummulative differences, set the mean_effect argument to  false.\n\nRandomization is conducted by randomly assigning observations to the pre and  post-intervention periods, resestimating the causal effect, and repeating n times. The null  distribution is the set of n casual effect estimates.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causale_ffect!(its)\njulia> generate_null_distribution(its, 10)\n10-element Vector{Float64}\n -0.5012456678829079\n -0.33790650529972194\n -0.2534340182760628\n ⋮\n -0.06217013151235991 \n -0.05905529159312335\n -0.04927743270606937\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.quantities_of_interest","page":"API","title":"CausalELM.quantities_of_interest","text":"quantities_of_interest(mod, n)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from the generated distribution.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x, t, y = rand(100, 5), [rand()<0.4 for i in 1:100], rand(1:100, 100, 1)\njulia> g_computer = GComputation(x, t, y)\njulia> estimate_causal_effect!(g_computer)\njulia> quantities_of_interest(g_computer, 1000)\n (0.114, 6.953133617011371)\n\n\n\n\n\nquantities_of_interest(mod, n)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from  the generated distribution. Randomization for event studies is done by creating time splits  at even intervals and reestimating the causal effect.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> quantities_of_interest(its, 10)\n (0.0, 0.07703275541001667)\n\n\n\n\n\n","category":"function"},{"location":"api/#Model-Validation","page":"API","title":"Model Validation","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"validate\nCausalELM.covariate_independence\nCausalELM.omitted_predictor\nCausalELM.sup_wald\nCausalELM.p_val\nCausalELM.counterfactual_consistency\nCausalELM.exchangeability\nCausalELM.e_value\nCausalELM.binarize\nCausalELM.risk_ratio\nCausalELM.positivity\nCausalELM.sums_of_squares\nCausalELM.class_pointers\nCausalELM.backtrack_to_find_breaks\nCausalELM.variance\nCausalELM.best_splits\nCausalELM.group_by_class\nCausalELM.jenks_breaks\nCausalELM.fake_treatments\nCausalELM.sdam\nCausalELM.scdm\nCausalELM.gvf\nCausalELM.var_type","category":"page"},{"location":"api/#CausalELM.validate","page":"API","title":"CausalELM.validate","text":"validate(its; <keyword arguments>)\n\nTest the validity of an estimated interrupted time series analysis.\n\nThis method coducts a Chow Test, a Wald supremeum test, and tests the model's sensitivity to  confounders. The Chow Test tests for structural breaks in the covariates between the time  before and after the event. p-values represent the proportion of times the magnitude of the  break in a covariate would have been greater due to chance. Lower p-values suggest a higher  probability the event effected the covariates and they cannot provide unbiased  counterfactual predictions. The Wald supremum test finds the structural break with the  highest Wald statistic. If this is not the same as the hypothesized break, it could indicate  an anticipation effect, a confounding event, or that the intervention or policy took place  in multiple phases. p-values represent the proportion of times we would see a larger Wald  statistic if the data points were randomly allocated to pre and post-event periods for the  predicted structural break. Ideally, the hypothesized break will be the same as the  predicted break and it will also have a low p-value. The omitted predictors test adds  normal random variables with uniform noise as predictors. If the included covariates are  good predictors of the counterfactual outcome, adding irrelevant predictors should not have  a large effect on the predicted counterfactual outcomes or the estimated effect.\n\nFor more details on the assumptions and validity of interrupted time series designs, see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nNote that this method does not implement the second test in Baicker and Svoronos because \n\nthe estimator in this package models the relationship between covariates and the outcome and  uses an extreme learning machine instead of linear regression, so variance in the outcome  across different bins is not much of an issue.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\n...\n\nArguments\n\nits::InterruptedTimeSeries: an interrupted time seiries estimator.\nn::Int: the number of times to simulate a confounder.\nlow::Float64=0.15: the minimum proportion of data points to include before or after the    tested break in the Wald supremum test.\nhigh::Float64=0.85: the maximum proportion of data points to include before or after the    tested break in the Wald supremum test.\n\n...\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)\njulia> estimate_causal_effect!(m1)\n[0.25714308]\njulia> validate(m1)\n{\"Task\" => \"Regression\", \"Regularized\" => true, \"Activation Function\" => relu, \n\"Validation Metric\" => \"mse\",\"Number of Neurons\" => 2, \n\"Number of Neurons in Approximator\" => 10, \"β\" => [0.25714308], \n\"Causal Effect\" => -3.9101138, \"Standard Error\" => 1.903434356, \"p-value\" = 0.00123356}\n\n\n\n\n\nvalidate(m; <keyword arguments>)\n\nThis method tests the counterfactual consistency, exchangeability, and positivity  assumptions required for causal inference. It should be noted that consistency and  exchangeability are not directly testable, so instead, these tests do not provide definitive  evidence of a violation of these assumptions. To probe the counterfactual consistency  assumption, we assume there were multiple levels of treatments and find them by binning the dependent vairable for treated observations using Jenks breaks. The optimal number of breaks  between 2 and num_treatments is found using the elbow method. Using these hypothesized  treatment assignemnts, this method compares the MSE of linear regressions using the observed  and hypothesized treatments. If the counterfactual consistency assumption holds then the  difference between the MSE with hypothesized treatments and the observed treatments should  be positive because the hypothesized treatments should not provide useful information. If  it is negative, that indicates there was more useful information provided by the  hypothesized treatments than the observed treatments or that there is an unobserved  confounder. Next, this methods tests the model's sensitivity to a violation of the  exchangeability assumption by calculating the E-value, which is the minimum strength of  association, on the risk ratio scale, that an unobserved confounder would need to have with  the treatment and outcome variable to fully explain away the estimated effect. Thus, higher  E-values imply the model is more robust to a violation of the exchangeability assumption.  Finally, this method tests the positivity assumption by estimating propensity scores. Rows in the matrix are levels of covariates that have a zero probability of treatment. If the  matrix is empty, none of the observations have an estimated zero probability of treatment,  which implies the positivity assumption is satisfied.\n\nFor a thorough review of casual inference assumptions see:     Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and      Francis, 2024. \n\nFor more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\n...\n\nArguments\n\nm::Union{CausalEstimator, Metalearner}: a model to validate/test the assumptions of.\nnum_treatments=5::Int: the maximum number of treatments to use when testing the    plausability of the counterfactual consistency assumption.\nmin::Float64=1.0e-6: minimum probability of treatment for the positivity assumption.\nhigh::Float64=1-min: the maximum probability of treatment for the positivity assumption.\n\n...\n\nExamples julia julia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100]),              vec(rand(1:100, 100, 1)),  julia> g_computer = GComputation(x, t, y, temporal=false) julia> estimate_causal_effect!(g_computer) julia> validate(g_computer)  2.7653668647301795\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.covariate_independence","page":"API","title":"CausalELM.covariate_independence","text":"covariate_independence(its; <keyword arguments>)\n\nTest for independence between covariates and the event or intervention.\n\nThis is a Chow Test for covariates with p-values estimated via randomization inference. The  p-values are the proportion of times randomly assigning observations to the pre or  post-intervention period would have a larger estimated effect on the the slope of the  covariates. The lower the p-values, the more likely it is that the event/intervention  effected the covariates and they cannot provide an unbiased prediction of the counterfactual  outcomes.\n\nFor more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\n...\n\nArguments\n\nits::InterruptedTImeSeries: an interrupted time seiries estimator.\nn::Int: the number of permutations for assigning observations to the pre and    post-treatment periods.\n\n...\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), \n           randn(10))\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> covariate_independence(its)\n Dict(\"Column 1 p-value\" => 0.421, \"Column 5 p-value\" => 0.07, \"Column 3 p-value\" => 0.01, \n \"Column 2 p-value\" => 0.713, \"Column 4 p-value\" => 0.043)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.omitted_predictor","page":"API","title":"CausalELM.omitted_predictor","text":"omitted_predictor(its; <keyword arguments>)\n\nSee how an omitted predictor/variable could change the results of an interrupted time series  analysis.\n\nThis method reestimates interrupted time series models with uniform random variables. If the  included covariates are good predictors of the counterfactual outcome, adding a random  variable as a covariate should not have a large effect on the predicted counterfactual  outcomes and therefore the estimated average effect.\n\nFor more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\n...\n\nArguments\n\nits::InterruptedTImeSeries: an interrupted time seiries estimator.\nn::Int: the number of times to simulate a confounder.\n\n...\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), \n           randn(10))\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> omitted_predictor(its)\n Dict(\"Mean Biased Effect/Original Effect\" => -0.1943184744720332, \"Median Biased \n Effect/Original Effect\" => -0.1881814122689084, \"Minimum Biased Effect/Original Effect\" => \n -0.2725194360603799, \"Maximum Biased Effect/Original Effect\" => -0.1419197976977072)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.sup_wald","page":"API","title":"CausalELM.sup_wald","text":"sup_wald(its; <keyword arguments>)\n\nCheck if the predicted structural break is the hypothesized structural break.\n\nThis method conducts Wald tests and identifies the structural break with the highest Wald  statistic. If this break is not the same as the hypothesized break, it could indicate an  anticipation effect, confounding by some other event or intervention, or that the  intervention or policy took place in multiple phases. p-values are estimated using  approximate randomization inference and represent the proportion of times we would see a  larger Wald statistic if the data points were randomly allocated to pre and post-event  periods for the predicted structural break.\n\nFor more information on using a Chow Test to test for structural breaks see:     Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single      interrupted time series design. No. w26080. National Bureau of Economic Research, 2019.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\n...\n\nArguments\n\nits::InterruptedTimeSeries: an interrupted time seiries estimator.\nn::Int: the number of times to simulate a confounder.\nlow::Float64=0.15: the minimum proportion of data points to include before or after the    tested break in the Wald supremum test.\nhigh::Float64=0.85: the maximum proportion of data points to include before or after the    tested break in the Wald supremum test.\n\n...\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = (Float64.(rand(1:5, 100, 5)), randn(100), rand(1:5, (10, 5)), \n           randn(10))\njulia> its = InterruptedTimeSeries(x₀, y₀, x₁, y₁)\njulia> estimate_causal_effect!(its)\njulia> sup_wald(its)\n Dict{String, Real}(\"Wald Statistic\" => 58.16649796321913, \"p-value\" => 0.005, \"Predicted \n Break Point\" => 39, \"Hypothesized Break Point\" => 100)\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.p_val","page":"API","title":"CausalELM.p_val","text":"p_val(x, y, β; <keyword arguments>)\n\nEstimate the p-value for the hypothesis that an event had a statistically significant effect  on the slope of a covariate using randomization inference.\n\n...\n\nArguments\n\nx::Array{<:Real}: covariates.\ny::Array{<:Real}: the outcome.\nβ::Array{<:Real}=0.15: the fitted weights.\ntwo_sided::Bool=false: whether to conduct a one-sided hypothesis test.\n\n...\n\nExamples\n\njulia> x, y, β = reduce(hcat, (float(rand(0:1, 10)), ones(10))), rand(10), 0.5\njulia> p_val(x, y, β)\n 0.98\njulia> p_val(x, y, β; n=100, two_sided=true)\n 0.08534054\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.counterfactual_consistency","page":"API","title":"CausalELM.counterfactual_consistency","text":"counterfactual_consistency(m; <keyword arguments>)\n\nExamine the counterfactual consistency assumption. First, this function generates Jenks  breaks based on outcome values for the treatment group. Then, it replaces treatment statuses  with the numbers corresponding to each group. Next, it runs two linear regressions on for  the treatment group, one with and one without the fake treatment assignemnts generated by  the Jenks breaks. Finally, it subtracts the mean squared error from the regression with real  data from the mean squared error from the regression with the fake treatment statuses. If  this number is negative, it might indicate a violation of the counterfactual consistency  assumption or omitted variable bias.\n\nFor a primer on G-computation and its assumptions see:     Naimi, Ashley I., Stephen R. Cole, and Edward H. Kennedy. \"An introduction to g      methods.\" International journal of epidemiology 46, no. 2 (2017): 756-762.\n\n...\n\nArguments\n\nm::Union{CausalEstimator, Metalearner}: a model to validate/test the assumptions of.\nnum_treatments=5::Int: the maximum number of treatments to use when testing the    plausability of the counterfactual consistency assumption.\n\n...\n\nExamples\n\njulia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100], \n            vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> counterfactual_consistency(g_computer)\n 2.7653668647301795\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.exchangeability","page":"API","title":"CausalELM.exchangeability","text":"exchangeability(model)\n\nTest the sensitivity of a G-computation or doubly robust estimator or metalearner to a  violation of the exchangeability assumption.\n\nFor more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100], \n            vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> e_value(g_computer)\n 1.13729886008143832\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.e_value","page":"API","title":"CausalELM.e_value","text":"e_value(model)\n\nTest the sensitivity of an estimator to a violation of the exchangeability assumption.\n\nFor more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100], \n            vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> e_value(g_computer)\n 2.2555405766985125\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.binarize","page":"API","title":"CausalELM.binarize","text":"binarize(x, cutoff)\n\nConvert a vector of counts or a continuous vector to a binary vector.\n\nExamples\n\njulia> binarize([1, 2, 3], 2)\n3-element Vector{Int64}:\n 0\n 0\n 1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.risk_ratio","page":"API","title":"CausalELM.risk_ratio","text":"risk_ratio(model)\n\nCalculate the risk ratio for an estimated model.\n\nIf the treatment variable is not binary and the outcome variable is not continuous then the  treatment variable will be binarized.\n\nFor more information on how other quantities of interest are converted to risk ratios see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.\n\nExamples\n\njulia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100], \n            vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> risk_ratio(g_computer)\n 2.5320694766985125\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.positivity","page":"API","title":"CausalELM.positivity","text":"positivity(model[,min][,max])\n\nFind likely violations of the positivity assumption.\n\nThis method uses an extreme learning machine or regularized extreme learning machine to  estimate probabilities of treatment. The returned matrix, which may be empty, are the  covariates that have a (near) zero probability of treatment or near zero probability of  being assigned to the control group, whith their entry in the last column being their  estimated treatment probability. In other words, they likely violate the positivity  assumption.\n\n...\n\nArguments\n\nmodel::Union{CausalEstimator, Metalearner}: a model to validate/test the assumptions of.\nmin::Float64=1.0e-6: minimum probability of treatment for the positivity assumption.\nhigh::Float64=1-min: the maximum probability of treatment for the positivity assumption.\n\n...\n\nExamples\n\njulia> x, t, y = rand(100, 5), Float64.([rand()<0.4 for i in 1:100], \n            vec(rand(1:100, 100, 1)))\njulia> g_computer = GComputation(x, t, y, temporal=false)\njulia> estimate_causal_effect!(g_computer)\njulia> positivity(g_computer)\n0×5 Matrix{Float64}\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.sums_of_squares","page":"API","title":"CausalELM.sums_of_squares","text":"sums_of_squares(data, num_classes)\n\nCalculate the minimum sum of squares for each data point and class for the Jenks breaks      algorithm.\n\nThis should not be called by the user.\n\nExamples\n\njulia> sums_of_squares([1, 2, 3, 4, 5], 2)\n5×2 Matrix{Real}:\n 0.0       0.0\n 0.25      0.25\n 0.666667  0.666667\n 1.25      1.16667\n 2.0       1.75\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.class_pointers","page":"API","title":"CausalELM.class_pointers","text":"class_pointers(data, num_classes, sums_of_sqs)\n\nCompute class pointers that minimize the sum of squares for Jenks breaks.\n\nThis should not be callled by the user.\n\nExamples\n\njulia> sums_squares = sums_of_sqs::Matrix{Float64}\n5×2 Matrix{Float64}:\n 0.0       0.0\n 0.25      0.25\n 0.666667  0.666667\n 1.25      1.16667\n 2.0       1.75\njulia> class_pointers([1, 2, 3, 4, 5], 2, sums_squares)\n5×2 Matrix{Int64}:\n 1  0\n 1  1\n 1  1\n 1  1\n 1  1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.backtrack_to_find_breaks","page":"API","title":"CausalELM.backtrack_to_find_breaks","text":"backtrack_to_find_breaks(data, num_classes, sums_of_sqs)\n\nDetermine break points from class assignments.\n\nThis should not be called by the user.\n\nExamples\n\njulia> data = [1, 2, 3, 4, 5]\n[1, 2, 3, 4, 5]\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 5\njulia> ptr = class_pointers([1, 2, 3, 4, 5], 2, sums_of_squares([1, 2, 3, 4, 5], 2))\n5×2 Matrix{Int64}:\n 1  28\n 1   1\n 1   1\n 1   1\n 1   1\njulia> backtrack_to_find_breaks([1, 2, 3, 4, 5], ptr)\n2-element Vector{Int64}:\n 1\n 4\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.variance","page":"API","title":"CausalELM.variance","text":"variance(data)\n\nCalculate the variance of some numbers.\n\nNote this function does not use Besel's correction.\n\nExamples\n\njulia> variance([1, 2, 3, 4, 5])\n2.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.best_splits","page":"API","title":"CausalELM.best_splits","text":"best_splits(data, num_classes)\n\nFind the best number of splits for Jenks breaks.\n\nThis function finds the best number of splits by finding the number of splits that results      in the greatest decrease in the slope of the line between itself and its GVF and the      next higher number of splits and its GVF. This is the same thing as the elbow method.\n\nThis should nto be called by the user.\n\nExamples\n\njulia> best_splits(collect(1:10), 5)\n10-element Vector{Int64}:\n 1\n 3\n 3\n ⋮\n 3\n 4\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.group_by_class","page":"API","title":"CausalELM.group_by_class","text":"group_by_class(data, classes)\n\nGroup data points into vectors such that data points assigned to the same class are in the  same vector.\n\nThis should nto be called by the user.\n\nExamples\n\njulia> group_by_class([1, 2, 3, 4, 5], [1, 1, 1, 2, 3])\n3-element Vector{Vector{Real}}:\n [1, 2, 3]\n [4]\n [5]\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.jenks_breaks","page":"API","title":"CausalELM.jenks_breaks","text":"jenks_breaks(data, num_classes)\n\nGenerate Jenks breaks for a vector of real numbers.\n\nExamples\n\njulia> jenks_breaks([1, 2, 3, 4, 5], 3)\n3-element Vector{Int64}:\n 1\n 3\n 4\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.fake_treatments","page":"API","title":"CausalELM.fake_treatments","text":"fake_treatments(data, num_classes)\n\nGenerate fake treatment statuses corresponding to the classes assigned by the Jenks breaks  algorithm.\n\nExamples\n\njulia> fake_treatments([1, 2, 3, 4, 5], 4)\n5-element Vector{Int64}:\n 1\n 2\n 3\n 4\n 4\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.sdam","page":"API","title":"CausalELM.sdam","text":"sdam(x)\n\nCalculate the sum of squared deviations for array mean for a set of sub arrays.\n\nExamples\n\njulia> sdam([5, 4, 9, 10]) \n26.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.scdm","page":"API","title":"CausalELM.scdm","text":"sdcm(x)\n\nCalculate the sum of squared deviations for class means for a set of sub arrays.\n\nExamples\n\njulia> scdm([[4], [5, 9, 10]]) \n14.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.gvf","page":"API","title":"CausalELM.gvf","text":"gvf(x)\n\nCalculate the goodness of variance fit for a set of sub vectors.\n\nExamples\n\njulia> gvf([[4, 5], [9, 10]])\n0.96153846153\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.var_type","page":"API","title":"CausalELM.var_type","text":"var_type(x)\n\nDetermine the type of variable held by a vector.\n\nExamples\n\njulia> var_type([1, 2, 3, 2, 3, 1, 1, 3, 2])\nBinary\n\n\n\n\n\n","category":"function"},{"location":"api/#Validation-Metrics","page":"API","title":"Validation Metrics","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"mse\nmae\naccuracy\nprecision\nrecall\nF1\nCausalELM.confusion_matrix","category":"page"},{"location":"api/#CausalELM.mse","page":"API","title":"CausalELM.mse","text":"mse(y, ŷ)\n\nCalculate the mean squared error\n\nSee also mae.\n\nExamples\n\njulia> mse([0.0, 0.0, 0.0], [0.0, 0.0, 0.0])\n 0.0\njulia> mse([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n 4.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.mae","page":"API","title":"CausalELM.mae","text":"mae(y, ŷ)\n\nCalculate the mean absolute error\n\nSee also mse.\n\nExamples\n\njulia> mae([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n 2.0\njulia> mae([1.0, 1.0, 1.0], [2.0, 2.0, 2.0])\n 1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.accuracy","page":"API","title":"CausalELM.accuracy","text":"accuracy(y, ŷ)\n\nCalculate the accuracy for a classification task\n\nExamples\n\njulia> accuracy([1, 1, 1, 1], [0, 1, 1, 0])\n 0.5\njulia> accuracy([1, 2, 3, 4], [1, 1, 1, 1])\n 0.25\n\n\n\n\n\n","category":"function"},{"location":"api/#Base.precision","page":"API","title":"Base.precision","text":"precision(y, ŷ)\n\nCalculate the precision for a classification task\n\nSee also recall.\n\nExamples\n\njulia> precision([0, 1, 0, 0], [0, 1, 1, 0])\n 0.5\njulia> precision([0, 1, 0, 0], [0, 1, 0, 0])\n 1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.recall","page":"API","title":"CausalELM.recall","text":"recall(y, ŷ)\n\nCalculate the recall for a classification task\n\nSee also precision.\n\nExamples\n\njulia> recall([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n 0.5\njulia> recall([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])\n 1.0\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.F1","page":"API","title":"CausalELM.F1","text":"F1(y, ŷ)\n\nCalculate the F1 score for a classification task\n\nExamples\n\njulia> F1([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n 0.4\njulia> F1([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])\n 0.47058823529411764\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.confusion_matrix","page":"API","title":"CausalELM.confusion_matrix","text":"confusion_matrix(y, ŷ)\n\nGenerate a confusion matrix\n\nExamples\n\njulia> confusion_matrix([1, 1, 1, 1, 0], [1, 1, 1, 1, 0])\n2×2 Matrix{Int64}:\n 1  0\n 0 4\njulia> confusion_matrix([1, 1, 1, 1, 0, 2], [1, 1, 1, 1, 0, 2])\n3×3 Matrix{Int64}:\n 1  0 0\n 0 4 0\n 0 0 1\n\n\n\n\n\n","category":"function"},{"location":"api/#Extreme-Learning-Machines","page":"API","title":"Extreme Learning Machines","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.fit!\nCausalELM.predict\nCausalELM.predict_counterfactual!\nCausalELM.placebo_test\nCausalELM.ridge_constant\nCausalELM.set_weights_biases","category":"page"},{"location":"api/#CausalELM.fit!","page":"API","title":"CausalELM.fit!","text":"fit!(model)\n\nMake predictions with an ExtremeLearner.\n\nFor more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples\n\njulia> m1 = ExtremeLearner(x, y, 10, σ)\n Extreme Learning Machine with 10 hidden neurons\n julia> f1 = fit!(m1)\n 10-element Vector{Float64}\n -4.403356409043448\n -5.577616954029608\n -2.1732800642523595\n ⋮\n -2.4741301876094655\n 40.642730531608635\n -11.058942121275233\n\n\n\n\n\nfit!(model)\n\nFit a Regularized Extreme Learner.\n\nFor more details see:      Li, Guoqiang, and Peifeng Niu. \"An enhanced extreme learning machine based on ridge      regression for regression.\" Neural Computing and Applications 22, no. 3 (2013):      803-810.\n\nExamples\n\njulia> m1 = RegularizedExtremeLearner(x, y, 10, σ)\nRegularized Extreme Learning Machine with 10 hidden neurons\njulia> f1 = fit!(m1)\n10-element Vector{Float64}\n -4.403356409043448\n -5.577616954029608\n -2.1732800642523595\n ⋮\n -2.4741301876094655\n 40.642730531608635\n -11.058942121275233\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.predict","page":"API","title":"CausalELM.predict","text":"predict(model, X)\n\nUse an ExtremeLearningMachine to make predictions.\n\nFor more details see:      Huang G-B, Zhu Q-Y, Siew C. Extreme learning machine: theory and applications.      Neurocomputing. 2006;70:489–501. https://doi.org/10.1016/j.neucom.2005.12.126\n\nExamples\n\njulia> m1 = ExtremeLearner(x, y, 10, σ)\nExtreme Learning Machine with 10 hidden neurons\njulia> f1 = fit(m1, sigmoid)\n10-element Vector{Float64}\n -4.403356409043448\n -5.577616954029608\n -2.1732800642523595\n ⋮\n -2.4741301876094655\n 40.642730531608635\n -11.058942121275233\njulia> predict(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])\n4-element Vector{Float64}\n 9.811656638113011e-16\n 0.9999999999999962\n -9.020553785284482e-17\n 0.9999999999999978\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.predict_counterfactual!","page":"API","title":"CausalELM.predict_counterfactual!","text":"predictcounterfactual(model, X)\n\nUse an ExtremeLearningMachine to predict the counterfactual.\n\nThis should be run with the observed covariates. To use synthtic data for what-if      scenarios use predict.\n\nSee also predict.\n\nExamples\n\njulia> m1 = ExtremeLearner(x, y, 10, σ)\n Extreme Learning Machine with 10 hidden neurons\n julia> f1 = fit(m1, sigmoid)\n 10-element Vector{Float64}\n -4.403356409043448\n -5.577616954029608\n -2.1732800642523595\n ⋮\n -2.4741301876094655\n 40.642730531608635\n -11.058942121275233\njulia> predict_counterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])\n4-element Vector{Float64}\n 9.811656638113011e-16\n 0.9999999999999962\n -9.020553785284482e-17\n 0.9999999999999978\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.placebo_test","page":"API","title":"CausalELM.placebo_test","text":"placebo_test(model)\n\nConduct a placebo test.\n\nThis method makes predictions for the post-event or post-treatment period using data  in the pre-event or pre-treatment period and the post-event or post-treament. If there is a statistically significant difference between these predictions the study design may be flawed. Due to the multitude of significance tests for time series data, this function returns the predictions but does not test for statistical significance.\n\nExamples\n\njulia> m1 = ExtremeLearner(x, y, 10, σ)\nExtreme Learning Machine with 10 hidden neurons\njulia> f1 = fit(m1, sigmoid)\n10-element Vector{Float64}\n -4.403356409043448\n -5.577616954029608\n -2.1732800642523595\n ⋮\n -2.4741301876094655\n 40.642730531608635\n -11.058942121275233\njulia> predict_counterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])\n4-element Vector{Float64}\n 9.811656638113011e-16\n 0.9999999999999962\n -9.020553785284482e-17\n 0.9999999999999978\njulia> placebo_test(m1)\n ([9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978],\n [0.5, 0.4, 0.3, 0.2])\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.ridge_constant","page":"API","title":"CausalELM.ridge_constant","text":"ridge_constant(model)\n\nCalculate the L2 penalty for a regularized extreme learning machine.\n\nFor more information see:      Li, Guoqiang, and Peifeng Niu. \"An enhanced extreme learning machine based on ridge      regression for regression.\" Neural Computing and Applications 22, no. 3 (2013):      803-810.\n\nExamples\n\njulia> m1 = RegularizedExtremeLearner(x, y, 10, σ)\nExtreme Learning Machine with 10 hidden neurons\njulia> ridge_constant(m1)\n 0.26789338524662887\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.set_weights_biases","page":"API","title":"CausalELM.set_weights_biases","text":"set_weights_biases(model)\n\nCalculate the weights and biases for an extreme learning machine or regularized extreme  learning machine.\n\nFor details see;     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples\n\njulia> m1 = RegularizedExtremeLearner(x, y, 10, σ)\nExtreme Learning Machine with 10 hidden neurons\njulia> set_weights_biases(m1)\n\n\n\n\n\n","category":"function"},{"location":"api/#Utility-Functions","page":"API","title":"Utility Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"CausalELM.mean\nCausalELM.var\nCausalELM.consecutive\nCausalELM.one_hot_encode","category":"page"},{"location":"api/#CausalELM.mean","page":"API","title":"CausalELM.mean","text":"mean(x)\n\nCalculate the mean of a vector.\n\nExamples\n\njulia> mean([1, 2, 3, 4])\n2.5\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.var","page":"API","title":"CausalELM.var","text":"var(x)\n\nCalculate the (sample) mean of a vector.\n\nExamples\n\njulia> var([1, 2, 3, 4])\n1.6666666666666667\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.consecutive","page":"API","title":"CausalELM.consecutive","text":"consecutive(x)\n\nSubtract consecutive elements in a vector.\n\nThis function is only used to create a rolling average for interrupted time series analysis.\n\nExamples\n\njulia> consecutive([1, 2, 3, 4, 5])\n4-element Vector{Int64}:\n 1\n 1\n 1\n 1\n\n\n\n\n\n","category":"function"},{"location":"api/#CausalELM.one_hot_encode","page":"API","title":"CausalELM.one_hot_encode","text":"one_hot_encode(x)\n\nOne hot encode a categorical vector for multiclass classification.\n\nExamples\n\njulia> one_hot_encode([1, 2, 3, 4, 5])\n5×5 Matrix{Float64}:\n 1.0  0.0  0.0  0.0  0.0\n 0.0  1.0  0.0  0.0  0.0\n 0.0  0.0  1.0  0.0  0.0\n 0.0  0.0  0.0  1.0  0.0\n 0.0  0.0  0.0  0.0  1.0\n\n\n\n\n\n","category":"function"},{"location":"guide/estimatorselection/#Deciding-Which-Estimator-to-Use","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"","category":"section"},{"location":"guide/estimatorselection/","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"Which model you should use depends on what you are trying to model and the type of data you  have. The table below can serve as a useful reference when deciding which model to use for a  given dataset and causal question.","category":"page"},{"location":"guide/estimatorselection/","page":"Deciding Which Estimator to Use","title":"Deciding Which Estimator to Use","text":"Model Struct Causal Estimands Supported Treatment Types Supported Outcome Types\nInterrupted Time Series Analysis InterruptedTimeSeries ATE, Cumulative Treatment Effect Binary Binary, Continuous\nG-computation GComputation ATE, ATT, ITT Binary Binary, Continuous, Time to Event\nDouble Machine Learning DoubleMachineLearning ATE Binary, Count, Categorical, Continuous Continuous\nS-learning SLearner CATE Binary Binary, Continuous, Count\nT-learning TLearner CATE Binary Binary, Continuous\nX-learning XLearner CATE Binary Binary, Continuous, Count\nR-learning RLearner CATE Binary, Count, Categorical, Continuous Continuous","category":"page"},{"location":"guide/gcomputation/#G-Computation","page":"G-computation","title":"G-Computation","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"In some cases, we may want to know the causal effect of a treatment that varies and is  confounded over time. For example, a doctor might want to know the effect of a treatment  given at multiple times whose status depends on the health of the patient at a given time.  One way to get an unbiased estimate of the causal effect is to use G-computation. The basic  steps for using G-computation in CausalELM are below.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"note: Note\nFor a good overview of G-Computation see:Chatton, Arthur, Florent Le Borgne, Clémence Leyrat, Florence Gillaizeau, Chloé \nRousseau, Laetitia Barbin, David Laplaud, Maxime Léger, Bruno Giraudeau, and Yohann \nFoucher. \"G-computation, propensity score-based methods, and targeted maximum likelihood \nestimator for causal inference with different covariates sets: a comparative simulation \nstudy.\" Scientific reports 10, no. 1 (2020): 9219.","category":"page"},{"location":"guide/gcomputation/#Step-1:-Initialize-a-Model","page":"G-computation","title":"Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"The GComputation method takes at least three arguments: an array of covariates, a vector of  treatment statuses, and an outcome vector. ","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"tip: Tip\nYou can also specify the causal estimand, whether to employ L2 regularization, which  activation function to use, whether the data is of a temporal nature, the metric to use when  using cross validation to find the best number of neurons, the minimum number of neurons to  consider, the maximum number of neurons to consider, the number of folds to use during cross  caidation, and the number of neurons to use in the ELM that learns a mapping from number of  neurons to validation loss. These options are specified with the following keyword  arguments: quantity_of_interest, regularized, activation, temporal, validation_metric,  min_neurons, max_neurons, folds, iterations, and approximator_neurons.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Create some data with a binary treatment\nX, T, Y =  rand(1000, 5), [rand()<0.4 for i in 1:1000], rand(1000)\n\n# We could also use DataFrames\n# using DataFrames\n# X = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# T, Y = DataFrame(t=[rand()<0.4 for i in 1:1000]), DataFrame(y=rand(1000))\n\ng_computer = GComputation(X, T, Y)","category":"page"},{"location":"guide/gcomputation/#Step-2:-Estimate-the-Causal-Effect","page":"G-computation","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"To estimate the causal effect, we pass the model above to estimatecausaleffect!.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Note that we could also estimate the ATT by setting quantity_of_interest=\"ATT\"\nestimate_causal_effect!(g_computer)","category":"page"},{"location":"guide/gcomputation/#Step-3:-Get-a-Summary","page":"G-computation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"We get a summary of the model that includes a p-value and standard error estimated via  asymptotic randomization inference by passing our model to the summarize method.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"Calling the summarize method returns a dictionary with the estimator's task (regression or  classification), the quantity of interest being estimated (ATE or ATT), whether the model  uses an L2 penalty, the activation function used in the model's outcome predictors, whether  the data is temporal, the validation metric used for cross validation to find the best  number of neurons, the number of neurons used in the ELMs used by the estimator, the number  of neurons used in the ELM used to learn a mapping from number of neurons to validation  loss during cross validation, the causal effect, standard error, and p-value.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"summarize(g_computer)","category":"page"},{"location":"guide/gcomputation/#Step-4:-Validate-the-Model","page":"G-computation","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we assume there were multiple levels of treatments  and find them by binning the dependent vairable for treated observations using Jenks breaks.  The optimal number of breaks between 2 and num_treatments is found using the elbow method.  Using these hypothesized treatment assignemnts, this method compares the MSE of linear  regressions using the observed and hypothesized treatments. If the counterfactual  consistency assumption holds then the difference between the MSE with hypothesized  treatments and the observed treatments should be positive because the hypothesized  treatments should not provide useful information. If it is negative, that indicates there  was more useful information provided by the hypothesized treatments than the observed  treatments or that there is an unobserved confounder. Next, this methods tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  probability of treatment. If the matrix is empty, none of the observations have an estimated  zero probability of treatment, which implies the positivity assumption is satisfied.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"tip: Tip\nOne can also specify the maxium number of possible treatments to consider for the causal  consistency assumption and the minimum and maximum probabilities of treatment for the  positivity assumption with the num_treatments, min, and max keyword arguments.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for G-computation.  If the assumptions are not met then any estimates may be biased and lead to incorrect  conclusions.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"note: Note\nFor a thorough review of casual inference assumptions see:Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and \nFrancis, 2024.For more information on the E-value test see:VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research: \nintroducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"validate(g_computer)","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All contributions are welcome. To ensure contributions align with the existing code base and  are not duplicated, please follow the guidelines below.","category":"page"},{"location":"contributing/#Reporting-a-Bug","page":"Contributing","title":"Reporting a Bug","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To report a bug, open an issue on the CausalELM.jl GitHub page. Please include all relevant information, such as what methods were called, the operating system used, the  verion/s of causalELM used, the verion/s of Julia used, any tracebacks or error codes, and  any other information that would be helpful for debugging. Also be sure to use the bug label.","category":"page"},{"location":"contributing/#Requesting-New-Features","page":"Contributing","title":"Requesting New Features","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before requesting a new feature, please check the issues page on GitHub to make sure someone else did not already request the same feature. If this is not the case, then please  open an issue that explains what function or method you would like to be added and how you  believe it should behave. Also be sure to use the enhancement tag.","category":"page"},{"location":"contributing/#Contributing-Code","page":"Contributing","title":"Contributing Code","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before submitting a pull request, please open an issue explaining what the proposed code is and why you want to add it, if there is not already an issue that addresses your  changes and you are not fixing something very minor. When submitting a pull request, please  reference the relevant issue/s and ensure your code follows the guidelines below.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All abstract types, structs, functions, methods, macros, and constants have docstrings    that follow the same format as the other docstrings. These functions should also be    included in the relevant section of the API Manual.\nThere are no repeated code blocks. If there are repeated codeblocks, then they should be    consolidated into a separate function.\nMethods should generally include types and be type stable. If there is a strong reason    to deviate from this point, there should be a comment in the code explaining why.\nMinimize use of new constants and macros. If they must be included, the reason for their    inclusion should be obvious or included in the docstring.\nAvoid using global variables and constants.\nCode should take advantage of Julia's built in macros for performance. Use @inbounds,    @view, @fastmath, and @simd when possible.\nWhen appending to an array in a loop, preallocate the array and update its values by    index.\nAvoid long functions and decompose them into smaller functions or methods. A general    rule is that function definitions should fit within the screen of a laptop.\nUse self-explanatory names for variables, methods, structs, constants, and macros.","category":"page"},{"location":"contributing/#Updating-or-Fixing-Documentation","page":"Contributing","title":"Updating or Fixing Documentation","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To propose a change to the documentation please submit a pull request.","category":"page"},{"location":"guide/doublemachinelearning/#Double-Machine-Learning","page":"Double Machine Learning","title":"Double Machine Learning","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"Double machine learning, also called debiased or orthogonalized machine learning, enables estimating causal effects when the dimensionality of the covariates is too high for linear  regression or the model does not assume a parametric form. In other words, when the  relathionship between the treatment or covariates and outcome is nonlinear and we do not  know the functional form. ","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"note: Note\nFor more information see:Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,  Whitney Newey, and James Robins. \"Double/debiased machine learning for treatment and  structural parameters.\" (2018): C1-C68.","category":"page"},{"location":"guide/doublemachinelearning/#Step-1:-Initialize-a-Model","page":"Double Machine Learning","title":"Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"The DoubleMachineLearning constructor takes at least three arguments, an array of  covariates, a treatment vector, and an outcome vector. ","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"tip: Tip\nYou can also specify the following options: whether the treatment vector is categorical ie  not continuous and containing more than two classes, whether to use L2 regularization, the  activation function, the validation metric to use when searching for the best number of  neurons, the minimum and maximum number of neurons to consider, the number of folds to use  for cross validation, the number of iterations to perform cross validation, and the number  of neurons to use in the ELM used to learn the function from number of neurons to validation  loss. These arguments are specified with the following keyword arguments: t_cat,  regularized, activation, validation_metric, min_neurons, max_neurons, folds, iterations,  and approximator_neurons.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# Create some data with a binary treatment\nX, T, Y =  rand(100, 5), [rand()<0.4 for i in 1:100], rand(100)\n\n# We could also use DataFrames\n# using DataFrames\n# X = DataFrame(x1=rand(100), x2=rand(100), x3=rand(100), x4=rand(100), x5=rand(100))\n# T, Y = DataFrame(t=[rand()<0.4 for i in 1:100]), DataFrame(y=rand(100))\n\ndml = DoubleMachineLearning(X, T, Y)","category":"page"},{"location":"guide/doublemachinelearning/#Step-2:-Estimate-the-Causal-Effect","page":"Double Machine Learning","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"To estimate the causal effect, we call estimatecausaleffect! on the model above.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# we could also estimate the ATT by passing quantity_of_interest=\"ATT\"\nestimate_causal_effect!(dml)","category":"page"},{"location":"guide/doublemachinelearning/#Get-a-Summary","page":"Double Machine Learning","title":"Get a Summary","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"We can get a summary that includes a p-value and standard error estimated via asymptotic  randomization inference by passing our model to the summarize method.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"Calling the summarize method returns a dictionary with the estimator's task (regression or  classification), the quantity of interest being estimated (ATE), whether the model uses an  L2 penalty (always true for DML), the activation function used in the model's outcome  predictors, whether the data is temporal (always false for DML), the validation metric used  for cross validation to find the best number of neurons, the number of neurons used in the  ELMs used by the estimator, the number of neurons used in the ELM used to learn a mapping  from number of neurons to validation loss during cross validation, the causal effect,  standard error, and p-value.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# Can also use the British spelling\n# summarise(dml)\n\nsummarize(dml)","category":"page"},{"location":"guide/doublemachinelearning/#Step-4:-Validate-the-Model","page":"Double Machine Learning","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we assume there were multiple levels of treatments  and find them by binning the dependent vairable for treated observations using Jenks breaks.  The optimal number of breaks between 2 and num_treatments is found using the elbow method.  Using these hypothesized treatment assignemnts, this method compares the MSE of linear  regressions using the observed and hypothesized treatments. If the counterfactual  consistency assumption holds then the difference between the MSE with hypothesized  treatments and the observed treatments should be positive because the hypothesized  treatments should not provide useful information. If it is negative, that indicates there  was more useful information provided by the hypothesized treatments than the observed  treatments or that there is an unobserved confounder. Next, this methods tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  or near zero probability of treatment. If the matrix is empty, none of the observations have  an estimated zero probability of treatment, which implies the positivity assumption is  satisfied.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"tip: Tip\nOne can also specify the maxium number of possible treatments to consider for the causal  consistency assumption and the minimum and maximum probabilities of treatment for the  positivity assumption with the num_treatments, min, and max keyword arguments.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for double machine  learning. If the assumptions are not met then any estimates may be biased and lead to  incorrect conclusions.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"note: Note\nFor a thorough review of casual inference assumptions see:Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and \nFrancis, 2024.For more information on the E-value test see:VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research: \nintroducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"validate(g_computer)","category":"page"},{"location":"guide/its/#Interrupted-Time-Series-Analysis","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Analysis","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Sometimes we want to know how an outcome variable for a single unit changed after an event  or intervention. For example, if regulators announce sanctions against company A, we might  want to know how the price of stock A changed after the announcement. Since we do not know what the price of Company A's stock would have been if the santions were not announced, we need some way to predict those values. An interrupted time series analysis does this by  using some covariates that are related to the oucome variable but not related to whether the  event happened to predict what would have happened. The estimated effects are the  differences between the predicted post-event counterfactual outcomes and the observed  post-event outcomes, which can also be aggregated to mean or cumulative effects.  Estimating an interrupted time series design in CausalELM consists of three steps.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nFor a deeper dive see:Bernal, James Lopez, Steven Cummins, and Antonio Gasparrini. \"Interrupted time series \nregression for the evaluation of public health interventions: a tutorial.\" International \njournal of epidemiology 46, no. 1 (2017): 348-355.","category":"page"},{"location":"guide/its/#Step-1:-Initialize-an-interrupted-time-series-estimator","page":"Interrupted Time Series Estimation","title":"Step 1: Initialize an interrupted time series estimator","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"The InterruptedTimeSeries method takes at least four agruments: an array of pre-event  covariates, a vector of pre-event outcomes, an array of post-event covariates, and a vector  of post-event outcomes. ","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"tip: Tip\nYou can also specify whether or not to use L2 regularization, which activation function to  use, the metric to use when using cross validation to find the best number of neurons, the  minimum number of neurons to consider, the maximum number of neurons to consider, the number  of folds to use during cross caidation, the number of neurons to use in the ELM that learns  a mapping from number of neurons to validation loss, and whether to include a rolling  average autoregressive term. These options can be specified using the keyword arguments  regularized, activation, validationmetric, minneurons, max_neurons, folds, iterations,  approximator_neurons, and autoregression.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"# Generate some data to use\nX₀, Y₀, X₁, Y₁ =  rand(1000, 5), rand(1000), rand(100, 5), rand(100)\n\n# We could also use DataFrames\n# using DataFrames\n# X₀ = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# X₁ = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# Y₀, Y₁ = DataFrame(y=rand(1000)), DataFrame(y=rand(1000))\n\nits = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)","category":"page"},{"location":"guide/its/#Step-2:-Estimate-the-Treatment-Effect","page":"Interrupted Time Series Estimation","title":"Step 2: Estimate the Treatment Effect","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Estimating the treatment effect only requires one argument: an InterruptedTimeSeries struct.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"estimate_causal_effect!(its)","category":"page"},{"location":"guide/its/#Step-3:-Get-a-Summary","page":"Interrupted Time Series Estimation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"We can get a summary of the model, including a p-value and statndard via asymptotic  randomization inference, by pasing the model to the summarize method.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Calling the summarize method returns a dictionary with the estimator's task (always  regression for interrupted time series analysis), whether the model uses an L2 penalty,  the activation function used in the model's outcome predictors, the validation metric used  for cross validation to find the best number of neurons, the number of neurons used in the  ELMs used by the estimator, the number of neurons used in the ELM used to learn a mapping  from number of neurons to validation loss during cross validation, the causal effect,  standard error, and p-value.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"summarize(its)","category":"page"},{"location":"guide/its/#Step-4:-Validate-the-Model","page":"Interrupted Time Series Estimation","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"For an interrupted time series design to work well we need to be able to get an unbiased  prediction of the counterfactual outcomes. If the event or intervention effected the  covariates we are using to predict the counterfactual outcomes, then we will not be able to  get unbiased predictions. We can verify this by conducting a Chow Test on the covariates. An ITS design also assumes that any observed effect is due to the hypothesized intervention,  rather than any simultaneous interventions, anticipation of the intervention, or any  intervention that ocurred after the hypothesized intervention. We can use a Wald supremum  test to see if the hypothesized intervention ocurred where there is the largest structural  break in the outcome or if there was a larger, statistically significant break in the  outcome that could confound an ITS analysis. The covariates in an ITS analysis should be  good predictors of the outcome. If this is the case, then adding irrelevant predictors  should not have much of a change on the results of the analysis. We can conduct all these  tests in one line of code.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"tip: Tip\nOne can also specify the number of simulated confounders to generate to test the sensitivity  of the model to confounding and the minimum and maximum proportion of data to use in the  Wald supremum test by including the n, low, and high keyword arguments.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for interrupted time  series estimation. If the assumptions are not met then any estimates may be biased and  lead to incorrect conclusions.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"note: Note\nFor a review of interrupted time series identifying assumptions and robustness checks, see:Baicker, Katherine, and Theodore Svoronos. Testing the validity of the single \ninterrupted time series design. No. w26080. National Bureau of Economic Research, 2019.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"validate(its)","category":"page"},{"location":"","page":"causalELM","title":"causalELM","text":"<div style=\"width:100%; height:15px;;\n        border-radius:6px;text-align:center;\n        color:#1e1e20\">\n    <a class=\"github-button\" href=\"https://github.com/dscolby/CausalELM.jl\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star dscolby/CausalELM.jl on GitHub\" style=\"margin:auto\">Star</a>\n    <script async defer src=\"https://buttons.github.io/buttons.js\"></script>\n</div>","category":"page"},{"location":"","page":"causalELM","title":"causalELM","text":"CurrentModule = CausalELM","category":"page"},{"location":"#Overview","page":"causalELM","title":"Overview","text":"","category":"section"},{"location":"","page":"causalELM","title":"causalELM","text":"causalELM enables Estimation of causal quantities of interest in research designs where a  counterfactual must be predicted and compared to the observed outcomes. More specifically,  CausalELM provides a simple API to execute interupted time series analysis, G-Computation,  and double machine learning as well as estimation of the CATE via S-Learning, T-Learning,  X-Learning, and R-learning. Once a causal model has beeen estimated, causalELM's summarize  method provides basic information about the model as well as a p-value and standard error  estimated with approximate randomization inference. One can then validate causal modeling  assumptions for any model with a single call to the validate method. In all of these  implementations, causalELM predicts the counterfactuals using an Extreme Learning Machine  that includes an L2 penalty by default. In this context, ELMs strike a good balance between  prediction accuracy, generalization, ease of implementation, speed, and interpretability. ","category":"page"},{"location":"#Features","page":"causalELM","title":"Features","text":"","category":"section"},{"location":"","page":"causalELM","title":"causalELM","text":"Simple interface enables estimating causal effects in only a few lines of code\nAnalytically derived L2 penalty reduces cross validation time and multicollinearity\nFast automatic cross validation works with longitudinal, panel, and time series data\nIncludes 13 activation functions and allows user-defined activation functions\nSingle interface for continous, binary, and categorical outcome variables\nEstimation of p-values and standard errors via asymptotic randomization inference\nNo dependencies outside of the Julia standard library\nValidate causal modeling assumptions with one line of code\nNon-parametric randomization (permutation) inference-based p-values for all models","category":"page"},{"location":"#What's-New?","page":"causalELM","title":"What's New?","text":"","category":"section"},{"location":"","page":"causalELM","title":"causalELM","text":"Added support for dataframes\nEstimators can handle any array whose values are <:Real\nEstimator constructors are now called with model(X, T, Y) instead of model(X, Y, T)\nImproved documentation\ncausalELM has a new logo","category":"page"},{"location":"#Comparison-with-Other-Packages","page":"causalELM","title":"Comparison with Other Packages","text":"","category":"section"},{"location":"","page":"causalELM","title":"causalELM","text":"Other packages, mainly EconML, DoWhy, and CausalML, have similar funcitonality. Beides being  written in Julia rather than Python, the main differences between CausalELM and these  libraries are:","category":"page"},{"location":"","page":"causalELM","title":"causalELM","text":"causalELM uses extreme learning machines instead of tree-based, linear, or deep learners\ncausalELM performs cross validation during training\ncausalELM performs inference via asymptotic randomization inference rather than    bootstrapping\ncausalELM does not require you to instantiate a model and pass it into a separate class    or struct for training\ncausalELM creates train/test splits automatically\ncausalELM does not have external dependencies: all the functions it uses are in the    Julia standard library\ncausalELM is simpler to use but has less flexibility than the other libraries","category":"page"},{"location":"#Installation","page":"causalELM","title":"Installation","text":"","category":"section"},{"location":"","page":"causalELM","title":"causalELM","text":"causalELM requires Julia version 1.7 or greater and can be installed from the REPL as shown  below. ","category":"page"},{"location":"","page":"causalELM","title":"causalELM","text":"using Pkg \nPkg.add(\"CausalELM\")","category":"page"},{"location":"guide/metalearners/#Metalearners","page":"Metalearners","title":"Metalearners","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"Instead of knowing the average cuasal effect, we might want to know which units benefit and  which units lose by being exposed to a treatment. For example, a cash transfer program might  motivate some people to work harder and incentivize others to work less. Thus, we might want  to know how the cash transfer program affects individuals instead of it average affect on  the population. To do so, we can use metalearners. Depending on the scenario, we may want to  use an S-learner, a T-learner, an X-learner, or an R-learner. The basic steps to use all  three metalearners are below. The difference between the metalearners is how they estimate  the CATE and what types of variables they can handle. In the case of S, T, and X learners,  they can only handle binary treatments. On the other hand, R-learners can handle binary,  categorical, count, or continuous treatments but only supports continuous outcomes.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"note: Note\nFor a deeper dive on S-learning, T-learning, and X-learning see:Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.To learn more about R-learning see:Nie, Xinkun, and Stefan Wager. \"Quasi-oracle estimation of heterogeneous treatment \neffects.\" Biometrika 108, no. 2 (2021): 299-319.","category":"page"},{"location":"guide/metalearners/#Initialize-a-Metalearner","page":"Metalearners","title":"Initialize a Metalearner","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"S-learners, T-learners, and X-learners all take at least three arguments: an array of  covariates, a vector of outcomes, and a vector of treatment statuses. ","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"tip: Tip\nAdditional options can be specified for each type of metalearner using its keyword arguments.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"# Generate data to use\nX, Y, T =  rand(1000, 5), rand(1000), [rand()<0.4 for i in 1:1000]\n\n# We could also use DataFrames\n# using DataFrames\n# X = DataFrame(x1=rand(1000), x2=rand(1000), x3=rand(1000), x4=rand(1000), x5=rand(1000))\n# T, Y = DataFrame(t=[rand()<0.4 for i in 1:1000]), DataFrame(y=rand(1000))\n\ns_learner = SLearner(X, Y, T)\nt_learner = TLearner(X, Y, T)\nx_learner = XLearner(X, Y, T)\nr_learner = RLearner(X, Y, T)","category":"page"},{"location":"guide/metalearners/#Estimate-the-CATE","page":"Metalearners","title":"Estimate the CATE","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can estimate the CATE for all the models by passing them to estimatecausaleffect!.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"estimate_causal_effect!(s_learner)\nestimate_causal_effect!(t_learner)\nestimate_causal_effect!(x_learner)\nestimate_causal_effect!(r_learner)","category":"page"},{"location":"guide/metalearners/#Get-a-Summary","page":"Metalearners","title":"Get a Summary","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can get a summary of the models that includes p0values and standard errors for the  average treatment effect by passing the models to the summarize method.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"Calling the summarize methodd returns a dictionary with the estimator's task (regression or  classification), the quantity of interest being estimated (CATE), whether the model  uses an L2 penalty, the activation function used in the model's outcome predictors, whether  the data is temporal, the validation metric used for cross validation to find the best  number of neurons, the number of neurons used in the ELMs used by the estimator, the number  of neurons used in the ELM used to learn a mapping from number of neurons to validation  loss during cross validation, the causal effect, standard error, and p-value for the ATE.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"summarize(s_learner)\nsummarize(t_learner)\nsummarize(x_learner)\nsummarize(r_learner)","category":"page"},{"location":"guide/metalearners/#Step-4:-Validate-the-Model","page":"Metalearners","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we assume there were multiple levels of treatments  and find them by binning the dependent vairable for treated observations using Jenks breaks.  The optimal number of breaks between 2 and num_treatments is found using the elbow method.  Using these hypothesized treatment assignemnts, this method compares the MSE of linear  regressions using the observed and hypothesized treatments. If the counterfactual  consistency assumption holds then the difference between the MSE with hypothesized  treatments and the observed treatments should be positive because the hypothesized  treatments should not provide useful information. If it is negative, that indicates there  was more useful information provided by the hypothesized treatments than the observed  treatments or that there is an unobserved confounder. Next, this methods tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  probability of treatment. If the matrix is empty, none of the observations have an estimated  zero probability of treatment, which implies the positivity assumption is satisfied.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"tip: Tip\nOne can also specify the maxium number of possible treatments to consider for the causal  consistency assumption and the minimum and maximum probabilities of treatment for the  positivity assumption with the num_treatments, min, and max keyword arguments.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"danger: Danger\nObtaining correct estimates is dependent on meeting the assumptions for interrupted time  series estimation. If the assumptions are not met then any estimates may be biased and  lead to incorrect conclusions.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"note: Note\nFor a thorough review of casual inference assumptions see:Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and \nFrancis, 2024.For more information on the E-value test see:VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research: \nintroducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"validate(s_learner)\nvalidate(t_learner)\nvalidate(x_learner)\nvalidate(r_learner)","category":"page"}]
}
