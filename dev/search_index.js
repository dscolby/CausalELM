var documenterSearchIndex = {"docs":
[{"location":"guide/gcomputation/#G-Computation","page":"G-computation","title":"G-Computation","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"In some cases, we may want to know the causal effect of a treatment that varies and is  confounded over time. For example, a doctor might want to know the effect of a treatment  given at multiple times whose status depends on the health of the patient at a given time.  One way to get an unbiased estimate of the causal effect is to use G-computation. The basic  steps for using G-computation in CausalELM are below.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"For a good overview of G-Computation see:     Chatton, Arthur, Florent Le Borgne, Clémence Leyrat, Florence Gillaizeau, Chloé      Rousseau, Laetitia Barbin, David Laplaud, Maxime Léger, Bruno Giraudeau, and Yohann      Foucher. \"G-computation, propensity score-based methods, and targeted maximum likelihood      estimator for causal inference with different covariates sets: a comparative simulation      study.\" Scientific reports 10, no. 1 (2020): 9219.","category":"page"},{"location":"guide/gcomputation/#Step-1:-Initialize-a-Model","page":"G-computation","title":"Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"The GComputation method takes three arguments: an array of covariates, a vector of  outcomes, and a vector of treatment statuses.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Create some data with a binary treatment\nX, Y, T =  rand(1000, 5), rand(1000), [rand()<0.4 for i in 1:1000]\n\ng_computer = GComputation(X, Y, T)","category":"page"},{"location":"guide/gcomputation/#Step-2:-Estimate-the-Causal-Effect","page":"G-computation","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"To estimate the causal effect, we pass the model above to estimatecausaleffect!.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Note that we could also estimate the ATT by setting quantity_of_interest=\"ATT\"\nestimate_causal_effect!(g_computer)","category":"page"},{"location":"guide/gcomputation/#Step-3:-Get-a-Summary","page":"G-computation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"We get a summary of the model that includes a p-value and standard error estimated via  asymptotic randomization inference by passing our model to the summarize method.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"summarize(g_computer)","category":"page"},{"location":"guide/gcomputation/#Step-4:-Validate-the-Model","page":"G-computation","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we assume there were multiple levels of treatments  and find them by binning the dependent vairable for treated observations using Jenks breaks.  The optimal number of breaks between 2 and num_treatments is found using the elbow method.  Using these hypothesized treatment assignemnts, this method compares the MSE of linear  regressions using the observed and hypothesized treatments. If the counterfactual  consistency assumption holds then the difference between the MSE with hypothesized  treatments and the observed treatments should be positive because the hypothesized  treatments should not provide useful information. If it is negative, that indicates there  was more useful information provided by the hypothesized treatments than the observed  treatments or that there is an unobserved confounder. Next, this methods tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  probability of treatment. If the matrix is empty, none of the observations have an estimated  zero probability of treatment, which implies the positivity assumption is satisfied.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"For a thorough review of casual inference assumptions see:     Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and      Francis, 2024. ","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"For more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"validate(g_computer)","category":"page"},{"location":"contributing/#Contributing","page":"Contributing","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All contributions are welcome. To ensure contributions align with the existing code base and  are not duplicated, please follow the guidelines below.","category":"page"},{"location":"contributing/#Reporting-a-Bug","page":"Contributing","title":"Reporting a Bug","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"To report a bug, open an issue on the CausalELM.jl GitHub page. Please include all relevant  information, such as what methods were called, the operating system used, the verion/s of  CausalELM used, the verion/s of Julia used, any tracebacks or error codes, and any other  information that would be helpful for debugging.","category":"page"},{"location":"contributing/#Requesting-New-Features","page":"Contributing","title":"Requesting New Features","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before requesting a new feature, please check the issues page on GitHub to make sure someone else did not already request the same feature. If this is not the case, then please open an issue that explains what function or method you would like to be added and how you believe  it should behave.","category":"page"},{"location":"contributing/#Contributing-Code","page":"Contributing","title":"Contributing Code","text":"","category":"section"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"Before submitting a pull request, please open an issue explaining what the proposed code is and why you want to add it. When submitting a pull request, please reference the relevant issue/s. Please also ensure your code follows the guidelines below.","category":"page"},{"location":"contributing/","page":"Contributing","title":"Contributing","text":"All abstract structs, structs, functions, methods, macros, and constants have docstrings    that follow the same format as the other docstrings. These functions should also be    included in the relevant section of the API Manual.\nThere are no repeated code blocks. If there are repeated codeblocks, then they should be    in a separate function.\nMethods should generally include types and be type stable. If there is a strong reason    to deviate from this point, there should be a comment in the code explaining why.\nMinimize use of new constants and macros. If they must be included, the reason for their    inclusion should be obvious or included in the docstring.\nAvoid using global variables and constants.\nCode should take advantage of Julia's macros for performance. Use @inbounds, @view,    @fastmath, and @simd when possible.\nWhen appending to an array in a loop, preallocate the array and update its values by    index.\nAvoid long functions and decompose them into smaller functions or methods.\nUse self-explanatory names for variables, methods, structs, constants, and macros.","category":"page"},{"location":"reference/crossval/#Cross-Valdiation","page":"Cross Validation","title":"Cross Valdiation","text":"","category":"section"},{"location":"reference/crossval/","page":"Cross Validation","title":"Cross Validation","text":"Methods to find the optimal number of neurons via cross validation","category":"page"},{"location":"reference/crossval/","page":"Cross Validation","title":"Cross Validation","text":"CausalELM.CrossValidation\nCausalELM.CrossValidation.recode\nCausalELM.CrossValidation.generate_folds\nCausalELM.CrossValidation.validate_fold\nCausalELM.CrossValidation.cross_validate\nCausalELM.CrossValidation.best_size\nCausalELM.CrossValidation.shuffle_data","category":"page"},{"location":"reference/metrics/#Validation-Metrics","page":"Validation Metrics","title":"Validation Metrics","text":"","category":"section"},{"location":"reference/metrics/","page":"Validation Metrics","title":"Validation Metrics","text":"Validation metrics used in cross validation of CausalELM estimators","category":"page"},{"location":"reference/metrics/","page":"Validation Metrics","title":"Validation Metrics","text":"CausalELM.Metrics\nCausalELM.Metrics.mse\nCausalELM.Metrics.mae\nCausalELM.Metrics.confusion_matrix\nCausalELM.Metrics.accuracy\nCausalELM.Metrics.precision\nCausalELM.Metrics.recall\nCausalELM.Metrics.F1","category":"page"},{"location":"reference/metalearners/#CATE-Estimation","page":"CATE Estimation","title":"CATE Estimation","text":"","category":"section"},{"location":"reference/metalearners/","page":"CATE Estimation","title":"CATE Estimation","text":"Stucts and methods to estimate the CATE from observational data","category":"page"},{"location":"reference/metalearners/","page":"CATE Estimation","title":"CATE Estimation","text":"CausalELM.Metalearners\nCausalELM.Metalearners.Metalearner\nCausalELM.Metalearners.SLearner\nCausalELM.Metalearners.TLearner\nCausalELM.Metalearners.XLearner\nCausalELM.Metalearners.estimate_causal_effect!\nCausalELM.Metalearners.stage1!\nCausalELM.Metalearners.stage2!","category":"page"},{"location":"reference/inference/#Inference-and-Summarization","page":"Inference and Summarization","title":"Inference and Summarization","text":"","category":"section"},{"location":"reference/inference/","page":"Inference and Summarization","title":"Inference and Summarization","text":"Methods for summarization and inference of estimators in the CausalELM package","category":"page"},{"location":"reference/inference/","page":"Inference and Summarization","title":"Inference and Summarization","text":"CausalELM.Inference\nCausalELM.Inference.summarize\nCausalELM.Inference.quantities_of_interest\nCausalELM.Inference.generate_null_distribution","category":"page"},{"location":"guide/doublemachinelearning/#Double-Machine-Learning","page":"Double Machine Learning","title":"Double Machine Learning","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"Doubly robust estimation estimates separate models for the treatment and outcome variables  and weights the outcome estimates by the treatment estimates. This allows one to model more  complex, nonlinear relationships between the treatment and outcome variables. Additonally,  double machine learning is doubly robust, which meants that only one of the models has to be  specified correctly to produce an unbiased estimate of the causal effect. This  implementation also uses cross fitting to avoid regularization bias. The main steps for  using doubly robust estimation in CausalELM are below.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"For more information see:     Chernozhukov, Victor, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen,      Whitney Newey, and James Robins. \"Double/debiased machine learning for treatment and      structural parameters.\" (2018): C1-C68.","category":"page"},{"location":"guide/doublemachinelearning/##-Step-1:-Initialize-a-Model","page":"Double Machine Learning","title":"# Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"The DoubleMachineLearning constructor takes four arguments, an array of covariates for the  outcome model, an array of covariates for the treatment model, a vector of outcomes, and a  vector of treatment statuses.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# Create some data with a binary treatment\nX, Xₚ, Y, T =  rand(100, 5), rand(100, 4), rand(100), [rand()<0.4 for i in 1:100]\n\ndml = DoubleMachineLearning(X, Xₚ, Y, T)","category":"page"},{"location":"guide/doublemachinelearning/#Step-2:-Estimate-the-Causal-Effect","page":"Double Machine Learning","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"To estimate the causal effect, we call estimatecausaleffect! on the model above.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"# we could also estimate the ATT by passing quantity_of_interest=\"ATT\"\nestimate_causal_effect!(dml)","category":"page"},{"location":"guide/doublemachinelearning/#Get-a-Summary","page":"Double Machine Learning","title":"Get a Summary","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"We can get a summary that includes a p-value and standard error estimated via asymptotic  randomization inference by passing our model to the summarize method.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"summarize(dml)","category":"page"},{"location":"guide/doublemachinelearning/#Step-4:-Validate-the-Model","page":"Double Machine Learning","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we assume there were multiple levels of treatments  and find them by binning the dependent vairable for treated observations using Jenks breaks.  The optimal number of breaks between 2 and num_treatments is found using the elbow method.  Using these hypothesized treatment assignemnts, this method compares the MSE of linear  regressions using the observed and hypothesized treatments. If the counterfactual  consistency assumption holds then the difference between the MSE with hypothesized  treatments and the observed treatments should be positive because the hypothesized  treatments should not provide useful information. If it is negative, that indicates there  was more useful information provided by the hypothesized treatments than the observed  treatments or that there is an unobserved confounder. Next, this methods tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  probability of treatment. If the matrix is empty, none of the observations have an estimated  zero probability of treatment, which implies the positivity assumption is satisfied.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"For a thorough review of casual inference assumptions see:     Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and      Francis, 2024. ","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"For more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/doublemachinelearning/","page":"Double Machine Learning","title":"Double Machine Learning","text":"validate(g_computer)","category":"page"},{"location":"reference/activations/#Activation-Functions","page":"Activation Functions","title":"Activation Functions","text":"","category":"section"},{"location":"reference/activations/","page":"Activation Functions","title":"Activation Functions","text":"Activation functions for CausalELM estimators","category":"page"},{"location":"reference/activations/","page":"Activation Functions","title":"Activation Functions","text":"CausalELM.ActivationFunctions\nCausalELM.ActivationFunctions.binary_step\nCausalELM.ActivationFunctions.σ\nCausalELM.ActivationFunctions.tanh\nCausalELM.ActivationFunctions.relu\nCausalELM.ActivationFunctions.leaky_relu\nCausalELM.ActivationFunctions.swish\nCausalELM.ActivationFunctions.softmax\nCausalELM.ActivationFunctions.softplus\nCausalELM.ActivationFunctions.gelu\nCausalELM.ActivationFunctions.gaussian\nCausalELM.ActivationFunctions.hard_tanh\nCausalELM.ActivationFunctions.elish\nCausalELM.ActivationFunctions.fourier","category":"page"},{"location":"reference/base/#Base-Models","page":"Base Models","title":"Base Models","text":"","category":"section"},{"location":"reference/base/","page":"Base Models","title":"Base Models","text":"Extreme learning machines and L2 regularized extreme learning machines for CausalELM estimators","category":"page"},{"location":"reference/base/","page":"Base Models","title":"Base Models","text":"CausalELM.Models\nCausalELM.Models.ExtremeLearningMachine\nCausalELM.Models.ExtremeLearner\nCausalELM.Models.RegularizedExtremeLearner\nCausalELM.Models.fit!\nCausalELM.Models.predict\nCausalELM.Models.predict_counterfactual!\nCausalELM.Models.placebo_test\nCausalELM.Models.ridge_constant\nCausalELM.Models.set_weights_biases","category":"page"},{"location":"reference/estimation/#ATE/ATT/ITT-Estimation","page":"ATE/ATT/ITE Estimation","title":"ATE/ATT/ITT Estimation","text":"","category":"section"},{"location":"reference/estimation/","page":"ATE/ATT/ITE Estimation","title":"ATE/ATT/ITE Estimation","text":"Structs and methods to estimate the ATE, ATT, ITT, and change over time from observational research designs.","category":"page"},{"location":"reference/estimation/","page":"ATE/ATT/ITE Estimation","title":"ATE/ATT/ITE Estimation","text":"CausalELM.Estimators\nCausalELM.Estimators.CausalEstimator\nCausalELM.Estimators.InterruptedTimeSeries\nCausalELM.Estimators.GComputation\nCausalELM.Estimators.DoubleMachineLearning\nCausalELM.Estimators.estimate_causal_effect!\nCausalELM.Estimators.first_stage!\nCausalELM.Estimators.ate!\nCausalELM.Estimators.predict_propensity_score\nCausalELM.Estimators.predict_control_outcomes\nCausalELM.Estimators.predict_treatment_outcomes\nCausalELM.Estimators.crossfitting_sets\nCausalELM.Estimators.moving_average","category":"page"},{"location":"reference/validation/#Model-Validation","page":"Model Validation","title":"Model Validation","text":"","category":"section"},{"location":"reference/validation/","page":"Model Validation","title":"Model Validation","text":"Methods to validate causal modeling assumptions of an estimated model","category":"page"},{"location":"reference/validation/","page":"Model Validation","title":"Model Validation","text":"CausalELM.ModelValidation\nCausalELM.ModelValidation.validate\nCausalELM.ModelValidation.covariate_independence\nCausalELM.ModelValidation.omitted_predictor\nCausalELM.ModelValidation.sup_wald\nCausalELM.ModelValidation.p_val\nCausalELM.ModelValidation.counterfactual_consistency\nCausalELM.ModelValidation.exchangeability\nCausalELM.ModelValidation.e_value\nCausalELM.ModelValidation.positivity\nCausalELM.ModelValidation.sums_of_squares\nCausalELM.ModelValidation.class_pointers\nCausalELM.ModelValidation.backtrack_to_find_breaks\nCausalELM.ModelValidation.variance\nCausalELM.ModelValidation.best_splits\nCausalELM.ModelValidation.group_by_class\nCausalELM.ModelValidation.jenks_breaks\nCausalELM.ModelValidation.fake_treatments\nCausalELM.ModelValidation.sdam\nCausalELM.ModelValidation.scdm\nCausalELM.ModelValidation.gvf","category":"page"},{"location":"reference/api/#CausalELM","page":"CausalELM","title":"CausalELM","text":"","category":"section"},{"location":"reference/api/","page":"CausalELM","title":"CausalELM","text":"CausalELM","category":"page"},{"location":"reference/api/#CausalELM","page":"CausalELM","title":"CausalELM","text":"Macros, functions, and structs for applying Extreme Learning Machines to causal inference tasks where the counterfactual is unavailable or biased and must be predicted. Provides  macros for event study designs, parametric G-computation, doubly robust estimation, and  metalearners. Additionally, these tasks can be performed with or without L2 penalization and will automatically choose the best number of neurons and L2 penalty. \n\nFor more details on Extreme Learning Machines see:     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\n\n\n\n\n","category":"module"},{"location":"guide/its/#Interrupted-Time-Series-Analysis","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Analysis","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Sometimes we want to know how an outcome variable for a single unit changed after an event  or intervention. For example, if regulators announce sanctions against company A, we might  want to know how the price of stock A changed after the announcement. Since we do not know what the price of Company A's stock would have been if the santions were not announced, we need some way to predict those values. An interrupted time series analysis does this by  using some covariates that are related to the oucome variable but not related to whether the  event happened to predict what would have happened. The estimated effects are the  differences between the predicted post-event counterfactual outcomes and the observed  post-event outcomes, which can also be aggregated to mean or cumulative effects.  Estimating an interrupted time series design in CausalELM consists of three steps.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"For a deeper dive see:     Bernal, James Lopez, Steven Cummins, and Antonio Gasparrini. \"Interrupted time series      regression for the evaluation of public health interventions: a tutorial.\" International      journal of epidemiology 46, no. 1 (2017): 348-355.","category":"page"},{"location":"guide/its/#Step-1:-Initialize-an-interrupted-time-series-estimator","page":"Interrupted Time Series Estimation","title":"Step 1: Initialize an interrupted time series estimator","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"The InterruptedTimeSeries method takes four agruments: an array of pre-event covariates, a  vector of pre-event outcomes, an array of post-event covariates, and a vector of post-event  outcomes.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"# Generate some data to use\nX₀, Y₀, X₁, Y₁ =  rand(1000, 5), rand(1000), rand(100, 5), rand(100)\n\nits = InterruptedTimeSeries(X₀, Y₀, X₁, Y₁)","category":"page"},{"location":"guide/its/#Step-2:-Estimate-the-Treatment-Effect","page":"Interrupted Time Series Estimation","title":"Step 2: Estimate the Treatment Effect","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"Estimating the treatment effect only requires one argument: an InterruptedTimeSeries struct.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"# We can also estimate the ATT by passing quantity_of_interest=\"ATT\"\nestimate_causal_effect!(its)","category":"page"},{"location":"guide/its/#Step-3:-Get-a-Summary","page":"Interrupted Time Series Estimation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"We can get a summary of the model, including a p-value and statndard via asymptotic  randomization inference, by pasing the model to the summarize method.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"summarize(its)","category":"page"},{"location":"guide/its/#Step-4:-Validate-the-Model","page":"Interrupted Time Series Estimation","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"For an interrupted time series design to work well we need to be able to get an unbiased  prediction of the counterfactual outcomes. If the event or intervention effected the  covariates we are using to predict the counterfactual outcomes, then we will not be able to  get unbiased predictions. We can verify this by conducting a Chow Test on the covariates. An ITS design also assumes that any observed effect is due to the hypothesized intervention,  rather than any simultaneous interventions, anticipation of the intervention, or any  intervention that ocurred after the hypothesized intervention. We can use a Wald supremum  test to see if the hypothesized intervention ocurred where there is the largest structural  break in the outcome or if there was a larger, statistically significant break in the  outcome that could confound an ITS analysis. The covariates in an ITS analysis should be  good predictors of the outcome. If this is the case, then adding irrelevant predictors  should not have much of a change on the results of the analysis. We can conduct all these  tests in one line of code.","category":"page"},{"location":"guide/its/","page":"Interrupted Time Series Estimation","title":"Interrupted Time Series Estimation","text":"validate(its)","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"<div style=\"width:100%; height:15px;;\n        border-radius:6px;text-align:center;\n        color:#1e1e20\">\n    <a class=\"github-button\" href=\"https://github.com/dscolby/CausalELM.jl\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star dscolby/CausalELM.jl on GitHub\" style=\"margin:auto\">Star</a>\n    <script async defer src=\"https://buttons.github.io/buttons.js\"></script>\n</div>","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CurrentModule = CausalELM","category":"page"},{"location":"#Overview","page":"CausalELM","title":"Overview","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM enables Estimation of causal quantities of interest in research designs where a  counterfactual must be predicted and compared to the observed outcomes. More specifically,  CausalELM provides a simple API to execute interupted time series analysis, G-Computation,  and double machine learning as well as estimation of the CATE via S-Learning, T-Learning,  and X-Learning. Once a causal model has beeen estimated, CausalELM's summarize method  provides basic information about the model as well as a p-value and standard error estimated  with approximate randomization inference. One can then validate causal modeling assumptions  for any model with a single call to the validate method. In all of these implementations,  CausalELM predicts the counterfactuals using an Extreme Learning Machine that includes an L2  penalty by default. In this context, ELMs strike a good balance between prediction accuracy,  generalization, ease of implementation, speed, and interpretability. ","category":"page"},{"location":"#Features","page":"CausalELM","title":"Features","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Simple interface enables estimating causal effects in only a few lines of code\nAnalytically derived L2 penalty reduces cross validation time and multicollinearity\nFast automatic cross validation works with longitudinal, panel, and time series data\nIncludes 13 activation functions and allows user-defined activation functions\nSingle interface for continous, binary, and categorical outcome variables\nEstimation of p-values and standard errors via asymptotic randomization inference\nNo dependencies outside of the Julia standard library\nValidate causal modeling assumptiions with one line of code","category":"page"},{"location":"#What's-New?","page":"CausalELM","title":"What's New?","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"All functions and methods converted to snake case\nRandomization inference for interrupted time series randomizes all indices\nImplemented validate method to probe assumptions for all estimators and metalearners\nReimplemented cross validation for temporal data\nFixed issue related to recoding variables to calculate validation metrics for cross validation","category":"page"},{"location":"#Comparison-with-Other-Packages","page":"CausalELM","title":"Comparison with Other Packages","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Other packages, mainly EconML, DoWhy, and CausalML, have similar funcitonality. Beides being  written in Julia rather than Python, the main differences between CausalELM and these  libraries are:","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM uses extreme learning machines rather than tree-based or deep learners\nCausalELM performs cross validation during training\nCausalELM performs inference via asymptotic randomization inference rather than    bootstrapping\nCausalELM does not require you to instantiate a model and pass it into a separate class    or struct for training\nCausalELM creates train/test splits automatically\nCausalELM does not have external dependencies: all the functions it uses are in the    Julia standard library","category":"page"},{"location":"#Installation","page":"CausalELM","title":"Installation","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM requires Julia version 1.7 or greater and can be installed from the REPL as shown  below. ","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"using Pkg \nPkg.add(\"CausalELM\")","category":"page"},{"location":"guide/metalearners/#Metalearners","page":"Metalearners","title":"Metalearners","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"Instead of knowing the average cuasal effect, we might want to know which units benefit and  which units lose by being exposed to a treatment. For example, a cash transfer program might  motivate some people to work harder and incentivize others to work less. Thus, we might want  to know how the cash transfer program affects individuals instead of it average affect on  the population. To do so, we can use metalearners. Depending on the scenario, we may want to  use an S-learner, a T-learner, or an X-learner. The basic steps to use all three  metalearners are below.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"For a deeper dive on metalearners see:     Künzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for      estimating heterogeneous treatment effects using machine learning.\" Proceedings of the      national academy of sciences 116, no. 10 (2019): 4156-4165.","category":"page"},{"location":"guide/metalearners/#Initialize-a-Metalearner","page":"Metalearners","title":"Initialize a Metalearner","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"S-learners, T-learners, and X-learners all take three arguments: an array of covariates, a  vector of outcomes, and a vector of treatment statuses.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"# Generate data to use\nX, Y, T =  rand(1000, 5), rand(1000), [rand()<0.4 for i in 1:1000]\n\ns_learner = SLearner(X, Y, T)\nt_learner = TLearner(X, Y, T)\nx_learner = XLearner(X, Y, T)","category":"page"},{"location":"guide/metalearners/#Estimate-the-CATE","page":"Metalearners","title":"Estimate the CATE","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can estimate the CATE for all the models by passing them to estimatecausaleffect!.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"estimate_causal_effect!(s_learner)\nestimate_causal_effect!(t_learner)\nestimate_causal_effect!(x_learner)","category":"page"},{"location":"guide/metalearners/#Get-a-Summary","page":"Metalearners","title":"Get a Summary","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can get a summary of the models that includes p0values and standard errors for the  average treatment effect by passing the models to the summarize method.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"summarize(s_learner)\nsummarize(t_learner)\nsummarize(x_learner)","category":"page"},{"location":"guide/metalearners/#Step-4:-Validate-the-Model","page":"Metalearners","title":"Step 4: Validate the Model","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can validate the model by examining the plausibility that the main assumptions of causal  inference, counterfactual consistency, exchangeability, and positivity, hold. It should be  noted that consistency and exchangeability are not directly testable, so instead, these  tests do not provide definitive evidence of a violation of these assumptions. To probe the  counterfactual consistency assumption, we assume there were multiple levels of treatments  and find them by binning the dependent vairable for treated observations using Jenks breaks.  The optimal number of breaks between 2 and num_treatments is found using the elbow method.  Using these hypothesized treatment assignemnts, this method compares the MSE of linear  regressions using the observed and hypothesized treatments. If the counterfactual  consistency assumption holds then the difference between the MSE with hypothesized  treatments and the observed treatments should be positive because the hypothesized  treatments should not provide useful information. If it is negative, that indicates there  was more useful information provided by the hypothesized treatments than the observed  treatments or that there is an unobserved confounder. Next, this methods tests the model's  sensitivity to a violation of the exchangeability assumption by calculating the E-value,  which is the minimum strength of association, on the risk ratio scale, that an unobserved  confounder would need to have with the treatment and outcome variable to fully explain away  the estimated effect. Thus, higher E-values imply the model is more robust to a violation of  the exchangeability assumption. Finally, this method tests the positivity assumption by  estimating propensity scores. Rows in the matrix are levels of covariates that have a zero  probability of treatment. If the matrix is empty, none of the observations have an estimated  zero probability of treatment, which implies the positivity assumption is satisfied.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"For a thorough review of casual inference assumptions see:     Hernan, Miguel A., and James M. Robins. Causal inference what if. Boca Raton: Taylor and      Francis, 2024. ","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"For more information on the E-value test see:     VanderWeele, Tyler J., and Peng Ding. \"Sensitivity analysis in observational research:      introducing the E-value.\" Annals of internal medicine 167, no. 4 (2017): 268-274.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"validate(s_learner)\nvalidate(t_learner)\nvalidate(x_learner)","category":"page"}]
}
