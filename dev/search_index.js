var documenterSearchIndex = {"docs":
[{"location":"guide/gcomputation/#G-Computation","page":"G-computation","title":"G-Computation","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"In some cases, we may want to know the causal effect of a treatment that varies and is  confounded over time. For example, a doctor might want to know the effect of a treatment  given at multiple times whose status depends on the health of the patient. One way to get an  unbiased estimate of the causal effect is to use G-computation. The basic steps for using  G-computation in CausalELM are below.","category":"page"},{"location":"guide/gcomputation/#Generate-Data","page":"G-computation","title":"Generate Data","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Create some data with a binary treatment\nX, Y, T =  rand(1000, 5), rand(1000), [rand()<0.4 for i in 1:1000]","category":"page"},{"location":"guide/gcomputation/#Step-1:-Initialize-a-Model","page":"G-computation","title":"Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"The GComputation method takes three arguments: an array of covariates, a vector of  outcomes, and a vector of treatment statuses.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"m1 = GComputation(X, Y, T)","category":"page"},{"location":"guide/gcomputation/#Step-2:-Estimate-the-Causal-Effect","page":"G-computation","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"To estimate the causal effect, we pass the model above to estimatecausaleffect!.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"# Note that we could also estimate the ATT by setting quantity_of_interest=\"ATT\"\nestimatecausaleffect!(m1)","category":"page"},{"location":"guide/gcomputation/#Step-3:-Get-a-Summary","page":"G-computation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"We get a summary of the model that includes a p-value and standard error estimated via  asymptotic randomization inference by passing our model to the summarize method.","category":"page"},{"location":"guide/gcomputation/","page":"G-computation","title":"G-computation","text":"summarize(m1)","category":"page"},{"location":"contributing/code/#Contributing-Code","page":"Contributing Code","title":"Contributing Code","text":"","category":"section"},{"location":"contributing/code/","page":"Contributing Code","title":"Contributing Code","text":"Before submitting a pull request, please open an issue explaining what the proposed code is and why you want to add it. When submitting a pull request, please reference the relevant issue(s). Please also ensure your code follows the following guidelines.","category":"page"},{"location":"contributing/code/","page":"Contributing Code","title":"Contributing Code","text":"All abstract structs, structs, functions, methods, macros, and constants have docstrings    that follow the same format as the other docstrings. These functions should also be included    in the relevant section of the API Manual.\nThere are no repeated code blocks. If there are repeated codeblocks, then they should be    in a separate function.\nMethods should generally include types and be type stable.If there is a strong reason to    deviate from this point, there should be a comment in the code explaining why.\nMinimize use of new constants and macros. If they must be included, the reason for their    includsion should be obvious or included in the docstring.\nWhen possible and relevant, code should call the @fastmath and @inbounds macros.\nUse self-explanatory names for variables, methods, structs, constants, and macros.","category":"page"},{"location":"reference/average/#ATE/ATT/ITT-Estimation","page":"ATE/ATT/ITT Estimation","title":"ATE/ATT/ITT Estimation","text":"","category":"section"},{"location":"reference/average/","page":"ATE/ATT/ITT Estimation","title":"ATE/ATT/ITT Estimation","text":"Structs and methods to estimate the ATE, ATT, ITT, and abnormal returns from observational research designs.","category":"page"},{"location":"reference/average/","page":"ATE/ATT/ITT Estimation","title":"ATE/ATT/ITT Estimation","text":"CausalELM.Estimators\nCausalELM.Estimators.CausalEstimator\nCausalELM.Estimators.EventStudy\nCausalELM.Estimators.GComputation\nCausalELM.Estimators.DoublyRobust\nCausalELM.Estimators.estimatecausaleffect!","category":"page"},{"location":"reference/average/#CausalELM.Estimators","page":"ATE/ATT/ITT Estimation","title":"CausalELM.Estimators","text":"Estimate causal effects with event study designs, G-computation, and doubly robust  estiamtion using Extreme Learning machines.\n\n\n\n\n\n","category":"module"},{"location":"reference/average/#CausalELM.Estimators.CausalEstimator","page":"ATE/ATT/ITT Estimation","title":"CausalELM.Estimators.CausalEstimator","text":"Abstract type for GComputation and DoublyRobust\n\n\n\n\n\n","category":"type"},{"location":"reference/average/#CausalELM.Estimators.EventStudy","page":"ATE/ATT/ITT Estimation","title":"CausalELM.Estimators.EventStudy","text":"Container for the results of an event study\n\n\n\n\n\n","category":"type"},{"location":"reference/average/#CausalELM.Estimators.GComputation","page":"ATE/ATT/ITT Estimation","title":"CausalELM.Estimators.GComputation","text":"Container for the results of G-Computation\n\n\n\n\n\n","category":"type"},{"location":"reference/average/#CausalELM.Estimators.DoublyRobust","page":"ATE/ATT/ITT Estimation","title":"CausalELM.Estimators.DoublyRobust","text":"Container for the results of doubly robust estimation\n\n\n\n\n\n","category":"type"},{"location":"reference/average/#CausalELM.estimatecausaleffect!","page":"ATE/ATT/ITT Estimation","title":"CausalELM.estimatecausaleffect!","text":"estimatecausaleffect!(study)\n\nEstimate the abnormal returns in an event study.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = EventStudy(X₀, Y₀, X₁, Y₁)\njulia> estimatecausaleffect!(m1)\n0.25714308\n\n\n\n\n\nestimatecausaleffect!(g)\n\nEstimate a causal effect of interest using G-Computation.\n\nIf treatents are administered at multiple time periods, the effect will be estimated as the average difference between the outcome of being treated in all periods and being treated in no periods. For example, given that individuals 1, 2, ..., i ∈ I recieved either a treatment or a placebo in p  different periods, the model would estimate the average treatment effect as  E[Yᵢ|T₁=1, T₂=1, ... Tₚ=1, Xₚ] - E[Yᵢ|T₁=0, T₂=0, ... Tₚ=0, Xₚ].\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = GComputation(X, Y, T)\njulia> estimatecausaleffect!(m1)\n0.31067439\n\n\n\n\n\nestimatecausaleffect!(DRE)\n\nEstimate a causal effect of interest using doubly robust estimation.\n\nUnlike other estimators, this method does not support time series or panel data. This method also  does not work as well with smaller datasets because it estimates separate outcome models for the  treatment and control groups.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = DoublyRobust(X, Y, T)\njulia> estimatecausaleffect!(m1)\n0.31067439\n\n\n\n\n\nestimatecausaleffect!(s)\n\nEstimate the CATE using an S-Learner.\n\nFor an overview of meatlearning, including S-Learners see:\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = SLearner(X, Y, T)\njulia> estimatecausaleffect!(m1)\n[0.20729633391630697, 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630697, 0.20729633391630697, 0.20729633391630703, \n0.20729633391630697, 0.20729633391630697  …  0.20729633391630703, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630703, 0.20729633391630697, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697]\n\n\n\n\n\nestimatecausaleffect!(t)\n\nEstimate the CATE using a T-Learner.\n\nFor an overview of meatlearning, including T-Learners see:\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = TLearner(X, Y, T)\njulia> estimatecausaleffect!(m1)\n[0.0493951571746305, 0.049395157174630444, 0.0493951571746305, 0.049395157174630444, \n0.04939515717463039, 0.04939515717463039, 0.04939515717463039, 0.04939515717463039, \n0.049395157174630444, 0.04939515717463061  …  0.0493951571746305, 0.04939515717463039, \n0.0493951571746305, 0.04939515717463039, 0.0493951571746305, 0.04939515717463039, \n0.04939515717463039, 0.049395157174630444, 0.04939515717463039, 0.049395157174630444]\n\n\n\n\n\nestimatecausaleffect!(x)\n\nEstimate the CATE using an X-Learner.\n\nFor an overview of meatlearning, including X-Learners see:\n\nKünzel, Sören R., Jasjeet S. Sekhon, Peter J. Bickel, and Bin Yu. \"Metalearners for \nestimating heterogeneous treatment effects using machine learning.\" Proceedings of the \nnational academy of sciences 116, no. 10 (2019): 4156-4165.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = XLearner(X, Y, T)\njulia> estimatecausaleffect!(m1)\n[-0.025012644892878473, -0.024634294305967294, -0.022144246680543364, -0.023983138957276127, \n-0.024756239357838557, -0.019409519377053822, -0.02312807640357356, -0.016967113188439076, \n-0.020188871831409317, -0.02546526148141366  …  -0.019811641136866287, \n-0.020780821058711863, -0.013588359417922776, -0.020438648396328824, -0.016169487825519843, \n-0.024031422484491572, -0.01884713946778991, -0.021163590874553318, -0.014607310062509895, \n-0.022449034332142046]\n\n\n\n\n\n","category":"function"},{"location":"reference/cate/#CATE-Estimation","page":"CATE Estimation","title":"CATE Estimation","text":"","category":"section"},{"location":"reference/cate/","page":"CATE Estimation","title":"CATE Estimation","text":"Structs and methods to estimate the ITE/CATE via metalearning","category":"page"},{"location":"reference/cate/","page":"CATE Estimation","title":"CATE Estimation","text":"CausalELM.Metalearners\nCausalELM.Metalearners.Metalearner\nCausalELM.Metalearners.SLearner\nCausalELM.Metalearners.TLearner\nCausalELM.Metalearners.XLearner\nCausalELM.Metalearners.SLearner.estimatecausaleffect!\nCausalELM.Metalearners.TLearner.estimatecausaleffect!\nCausalELM.Metalearners.XLearner.estimatecausaleffect!","category":"page"},{"location":"reference/cate/#CausalELM.Metalearners","page":"CATE Estimation","title":"CausalELM.Metalearners","text":"Metalearners to estimate the conditional average treatment effect (CATE).\n\n\n\n\n\n","category":"module"},{"location":"reference/cate/#CausalELM.Metalearners.Metalearner","page":"CATE Estimation","title":"CausalELM.Metalearners.Metalearner","text":"Abstract type for metalearners\n\n\n\n\n\n","category":"type"},{"location":"reference/cate/#CausalELM.Metalearners.SLearner","page":"CATE Estimation","title":"CausalELM.Metalearners.SLearner","text":"S-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"reference/cate/#CausalELM.Metalearners.TLearner","page":"CATE Estimation","title":"CausalELM.Metalearners.TLearner","text":"T-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"reference/cate/#CausalELM.Metalearners.XLearner","page":"CATE Estimation","title":"CausalELM.Metalearners.XLearner","text":"X-Learner for CATE estimation.\n\n\n\n\n\n","category":"type"},{"location":"reference/crossval/#Cross-Valdiation","page":"Cross Validation","title":"Cross Valdiation","text":"","category":"section"},{"location":"reference/crossval/","page":"Cross Validation","title":"Cross Validation","text":"Methods to find the optimal number of neurons via cross validation","category":"page"},{"location":"reference/crossval/","page":"Cross Validation","title":"Cross Validation","text":"CausalELM.CrossValidation\nCausalELM.CrossValidation.recode\nCausalELM.CrossValidation.traintest\nCausalELM.CrossValidation.validate\nCausalELM.CrossValidation.crossvalidate\nCausalELM.CrossValidation.bestsize","category":"page"},{"location":"reference/crossval/#CausalELM.CrossValidation","page":"Cross Validation","title":"CausalELM.CrossValidation","text":"Methods to perform cross validation and find the optimum number of neurons.\n\nTo reduce computation time, the number of neurons is optimized by using cross validation to estimate the validation error on a small subset of the range of possible numbers of  neurons. Then, an Extreme Learning Machine is trained to predict validation loss from  the given cross validation sets. Finally, the number of neurons is selected that has the  smallest predicted loss or the highest classification metric.\n\n\n\n\n\n","category":"module"},{"location":"reference/crossval/#CausalELM.CrossValidation.recode","page":"Cross Validation","title":"CausalELM.CrossValidation.recode","text":"recode(ŷ)\n\nRound predicted values to their predicted class for classification tasks.\n\nIf the smallest predicted label is 0, all labels are shifted up 1; if the smallest  label is -1, all labels are shifted up 2. Also labels cannot be smaller than -1.\n\nExamples\n\njulia> recode([-0.7, 0.2, 1.1])\n3-element Vector{Float64}\n1\n2\n3\njulia> recode([0.1, 0.2, 0.3])\n3-element Vector{Float64}\n1\n1\n1\njulia> recode([1.1, 1.51, 1.8])\n3-element Vector{Float64}\n1\n2\n2\n\n\n\n\n\n","category":"function"},{"location":"reference/crossval/#CausalELM.CrossValidation.traintest","page":"Cross Validation","title":"CausalELM.CrossValidation.traintest","text":"traintest(X, Y, folds)\n\nCreate a train-test split.\n\nIf an iteration is specified, the train test split will be treated as time series/panel data.\n\nExamples\n\njulia> xtrain, ytrain, xtest, ytest = traintest(zeros(20, 2), zeros(20), 5)\n\n\n\n\n\ntraintest(X, Y, folds, iteration)\n\nCreate a rolling train-test split for time series/panel data.\n\nAn iteration should not be specified for non-time series/panel data.\n\nExamples\n\njulia> xtrain, ytrain, xtest, ytest = traintest(zeros(20, 2), zeros(20), 5, 1)\n\n\n\n\n\n","category":"function"},{"location":"reference/crossval/#CausalELM.CrossValidation.validate","page":"Cross Validation","title":"CausalELM.CrossValidation.validate","text":"validate(X, Y, nodes, metric, iteration...; activation, regularized, folds)\n\nCalculate a validation metric for a single fold in k-fold cross validation.\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> validate(x, y, 5, accuracy, 3)\n0.0\n\n\n\n\n\n","category":"function"},{"location":"reference/crossval/#CausalELM.CrossValidation.crossvalidate","page":"Cross Validation","title":"CausalELM.CrossValidation.crossvalidate","text":"crossvalidate(X, Y, neurons, metric, activation, regularized, folds)\n\nCalculate a validation metric for k folds using a single set of hyperparameters.\n\nExamples\n\njulia> x = rand(100, 5); y = Float64.(rand(100) .> 0.5)\njulia> crossvalidate(x, y, 5, accuracy)\n0.0257841765251021\n\n\n\n\n\n","category":"function"},{"location":"reference/crossval/#CausalELM.CrossValidation.bestsize","page":"Cross Validation","title":"CausalELM.CrossValidation.bestsize","text":"bestsize(X, Y, metric, task, activation, min_neurons, max_neurons, regularized, folds, temporal, \n    iterations, approximator_neurons)\n\nCompute the best number of neurons for an Extreme Learning Machine.\n\nThe procedure tests networks with numbers of neurons in a sequence whose length is given  by iterations on the interval [minneurons, maxneurons]. Then, it uses the networks  sizes and validation errors from the sequence to predict the validation error or metric  for every network size between minneurons and maxneurons using the function  approximation ability of an Extreme Learning Machine. Finally, it returns the network  size with the best predicted validation error or metric.\n\nExamples\n\njulia> bestsize(rand(100, 5), rand(100), mse, \"regression\")\n11\n\n\n\n\n\n","category":"function"},{"location":"reference/metrics/#Validation-Metrics","page":"Validation Metrics","title":"Validation Metrics","text":"","category":"section"},{"location":"reference/metrics/","page":"Validation Metrics","title":"Validation Metrics","text":"Validation metrics used in cross validation of CausalELM estimators","category":"page"},{"location":"reference/metrics/","page":"Validation Metrics","title":"Validation Metrics","text":"CausalELM.Metrics\nCausalELM.Metrics.mse\nCausalELM.Metrics.mae\nCausalELM.Metrics.confusionmatrix\nCausalELM.Metrics.accuracy\nCausalELM.Metrics.precision\nCausalELM.Metrics.recall\nCausalELM.Metrics.F1","category":"page"},{"location":"reference/metrics/#CausalELM.Metrics","page":"Validation Metrics","title":"CausalELM.Metrics","text":"Metrics to evaluate the performance of an Extreme learning machine for regression and classification tasks.\n\n\n\n\n\n","category":"module"},{"location":"reference/metrics/#CausalELM.Metrics.mse","page":"Validation Metrics","title":"CausalELM.Metrics.mse","text":"mse(y, ŷ)\n\nCalculate the mean squared error\n\nSee also mae.\n\nExamples\n\njulia> mse([0.0, 0.0, 0.0], [0.0, 0.0, 0.0])\n0\njulia> mse([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n4\n\n\n\n\n\n","category":"function"},{"location":"reference/metrics/#CausalELM.Metrics.mae","page":"Validation Metrics","title":"CausalELM.Metrics.mae","text":"mae(y, ŷ)\n\nCalculate the mean absolute error\n\nSee also mse.\n\nExamples\n\njulia> mae([-1.0, -1.0, -1.0], [1.0, 1.0, 1.0])\n2\njulia> mae([1.0, 1.0, 1.0], [2.0, 2.0, 2.0])\n1\n\n\n\n\n\n","category":"function"},{"location":"reference/metrics/#CausalELM.Metrics.confusionmatrix","page":"Validation Metrics","title":"CausalELM.Metrics.confusionmatrix","text":"confusionmatrix(y, ŷ)\n\nGenerate a confusion matrix\n\nExamples\n\njulia> confusionmatrix([1, 1, 1, 1, 0], [1, 1, 1, 1, 0])\n2×2 Matrix{Int64}:\n 1  0\n 0 4\njulia> confusionmatrix([1, 1, 1, 1, 0, 2], [1, 1, 1, 1, 0, 2])\n3×3 Matrix{Int64}:\n 1  0 0\n 0 4 0\n 0 0 1\n\n\n\n\n\n","category":"function"},{"location":"reference/metrics/#CausalELM.Metrics.accuracy","page":"Validation Metrics","title":"CausalELM.Metrics.accuracy","text":"accuracy(y, ŷ)\n\nCalculate the accuracy for a classification task\n\nExamples\n\njulia> accuracy([1, 1, 1, 1], [0, 1, 1, 0])\n0.5\njulia> accuracy([1, 2, 3, 4], [1, 1, 1, 1])\n0.25\n\n\n\n\n\n","category":"function"},{"location":"reference/metrics/#CausalELM.Metrics.precision","page":"Validation Metrics","title":"CausalELM.Metrics.precision","text":"precision(y, ŷ)\n\nCalculate the precision for a classification task\n\nSee also recall.\n\nExamples\n\njulia> precision([0, 1, 0, 0], [0, 1, 1, 0])\n0.5\njulia> precision([0, 1, 0, 0], [0, 1, 0, 0])\n1\n\n\n\n\n\n","category":"function"},{"location":"reference/metrics/#CausalELM.Metrics.recall","page":"Validation Metrics","title":"CausalELM.Metrics.recall","text":"recall(y, ŷ)\n\nCalculate the recall for a classification task\n\nSee also precision.\n\nExamples\n\njulia> recall([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.5\njulia> recall([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])\n1\n\n\n\n\n\n","category":"function"},{"location":"reference/metrics/#CausalELM.Metrics.F1","page":"Validation Metrics","title":"CausalELM.Metrics.F1","text":"F1(y, ŷ)\n\nCalculate the F1 score for a classification task\n\nExamples\n\njulia> F1([1, 2, 1, 3, 0], [2, 2, 2, 3, 1])\n0.4\njulia> F1([1, 2, 1, 3, 2], [2, 2, 2, 3, 1])\n0.47058823529411764\n\n\n\n\n\n","category":"function"},{"location":"reference/inference/#Inference-and-Summarization","page":"Inference and Summarization","title":"Inference and Summarization","text":"","category":"section"},{"location":"reference/inference/","page":"Inference and Summarization","title":"Inference and Summarization","text":"Methods for summarization and inference of estimators in the CausalELM package","category":"page"},{"location":"reference/inference/","page":"Inference and Summarization","title":"Inference and Summarization","text":"CausalELM.Inference\nCausalELM.Inference.summarize\nCausalELM.Inference.quantitiesofinterest\nCausalELM.Inference.generatenulldistribution","category":"page"},{"location":"reference/inference/#CausalELM.Inference","page":"Inference and Summarization","title":"CausalELM.Inference","text":"Methods for summarization and inference from estimators and metalearners.\n\n\n\n\n\n","category":"module"},{"location":"reference/inference/#CausalELM.summarize","page":"Inference and Summarization","title":"CausalELM.summarize","text":"summarize(study, mean_effect)\n\nReturn a summary from an event study.\n\nExamples\n\njulia> X₀, Y₀, X₁, Y₁ =  rand(100, 5), rand(100), rand(10, 5), rand(10)\njulia> m1 = EventStudy(X₀, Y₀, X₁, Y₁)\njulia> estimatetreatmenteffect!(m1)\n[0.25714308]\njulia> summarize(m1)\n{\"Task\" => \"Regression\", \"Regularized\" => true, \"Activation Function\" => relu, \n\"Validation Metric\" => \"mse\",\"Number of Neurons\" => 2, \n\"Number of Neurons in Approximator\" => 10, \"β\" => [0.25714308], \n\"Causal Effect\" => -3.9101138, \"Standard Error\" => 1.903434356, \"p-value\" = 0.00123356}\n\n\n\n\n\nsummarize(g)\n\nReturn a summary from a G-Computation estimator.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = GComputation(X, Y, T)\njulia> estimatetreatmenteffect!(m1)\n[0.3100468253]\njulia> summarize(m1)\n{\"Task\" => \"Regression\", \"Quantity of Interest\" => \"ATE\", Regularized\" => \"true\", \n\"Activation Function\" => \"relu\", \"Time Series/Panel Data\" => \"false\", \n\"Validation Metric\" => \"mse\",\"Number of Neurons\" => \"5\", \n\"Number of Neurons in Approximator\" => \"10\", \"β\" => \"[0.3100468253]\",\n\"Causal Effect: 0.00589761, \"Standard Error\" => 5.12900734, \"p-value\" => 0.479011245} \n\n\n\n\n\nsummarize(dre, n)\n\nReturn a summary from a doubly robust estimator.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = DoublyRobust(X, X, Y, T)\njulia> estimatetreatmenteffect!(m1)\n[0.5804032956]\njulia> summarize(m1)\n{\"Task\" => \"Regression\", \"Quantity of Interest\" => \"ATE\", Regularized\" => \"true\", \n\"Activation Function\" => \"relu\", \"Validation Metric\" => \"mse\", \"Number of Neurons\" => \"5\", \n\"Number of Neurons in Approximator\" => \"10\", \"Causal Effect\" = 0.5804032956, \n\"Standard Error\" => 2.129400324, \"p-value\" => 0.0008342356}\n\n\n\n\n\nsummarize(m, n)\n\nReturn a summary from a metalearner.\n\nExamples\n\njulia> X, Y, T =  rand(100, 5), rand(100), [rand()<0.4 for i in 1:100]\njulia> m1 = SLearner(X, Y, T)\njulia> estimatecate!(m1)\n[0.20729633391630697, 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630697, 0.20729633391630697, 0.20729633391630703, \n0.20729633391630697, 0.20729633391630697  …  0.20729633391630703, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630703, 0.20729633391630697, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697]\njulia> summarise(m1)\n{\"Task\" => \"Regression\", Regularized\" => \"true\", \"Activation Function\" => \"relu\", \n\"Time Series/Panel Data\" => \"false\", \"Validation Metric\" => \"mse\", \n\"Number of Neurons\" => \"5\", \"Number of Neurons in Approximator\" => \"10\", \n\"β\" => \"[0.3100468253]\", \"Causal Effect: [0.20729633391630697, 0.20729633391630697, \n0.20729633391630692, 0.20729633391630697, 0.20729633391630697, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630703, 0.20729633391630697, 0.20729633391630697  …  \n0.20729633391630703, 0.20729633391630697, 0.20729633391630692, 0.20729633391630703, \n0.20729633391630697, 0.20729633391630697, 0.20729633391630692, 0.20729633391630697, \n0.20729633391630697, 0.20729633391630697], \"Standard Error\" => 5.3121435085, \n\"p-value\" => 0.0632454855}\n\n\n\n\n\n","category":"function"},{"location":"reference/inference/#CausalELM.Inference.quantitiesofinterest","page":"Inference and Summarization","title":"CausalELM.Inference.quantitiesofinterest","text":"quantitiesofinterest(model, n)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from the generated distribution.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x, y, t = rand(100, 5), rand(1:100, 100, 1), [rand()<0.4 for i in 1:100]\njulia> g_computer = GComputation(x, y, t)\njulia> estimatecausaleffect!(g_computer)\njulia> quantitiesofinterest(g_computer, 1000)\n(0.114, 6.953133617011371)\n\n\n\n\n\nquantitiesofinterest(model, nsplits)\n\nGenerate a p-value and standard error through randomization inference\n\nThis method generates a null distribution of treatment effects by reestimating treatment  effects from permutations of the treatment vector and estimates a p-value and standard from  the generated distribution. Randomization for event studies is done by creating time splits  at even intervals and reestimating the causal effect.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:     https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> event_study = EventStudy(x₀, y₀, x₁, y₁)\njulia> estimatecausaleffect!(event_study)\njulia> quantitiesofinterest(event_study, 10)\n(0.0, 0.07703275541001667)\n\n\n\n\n\n","category":"function"},{"location":"reference/inference/#CausalELM.Inference.generatenulldistribution","page":"Inference and Summarization","title":"CausalELM.Inference.generatenulldistribution","text":"generatenulldistribution(e, n)\n\nGenerate a null distribution for the treatment effect of G-computation, doubly robust  estimation, or metalearning.\n\nThis method estimates the same model that is provided using random permutations of the  treatment assignment to generate a vector of estimated effects under different treatment regimes. When e is a metalearner the null statistic is the difference is the ATE.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nExamples\n\njulia> x, y, t = rand(100, 5), rand(1:100, 100, 1), [rand()<0.4 for i in 1:100]\njulia> g_computer = GComputation(x, y, t)\njulia> estimatecausaleffect!(g_computer)\njulia> generatenulldistribution(g_computer, 500)\n[0.016297180690693656, 0.0635928694685571, 0.20004144093635673, 0.505893866040335, \n0.5130594630907543, 0.5432486130493388, 0.6181727442724846, 0.61838399963459, \n0.7038981488009489, 0.7043407710415689  …  21.909186142780246, 21.960498059428854, \n21.988553083790023, 22.285403459215363, 22.613625375395973, 23.382102081355548, \n23.52056245175936, 24.739658523175912, 25.30523686137909, 28.07474553316176]\n\n\n\n\n\ngeneratenulldistribution(e, n, mean_effect)\n\nGenerate a null distribution for the treatment effect in an event study design. By default,  this method generates a null distribution of mean differences. To generate a null  distribution of cummulative differences, set the mean_effect argument to false.\n\nInstead of randomizing the assignment of units to the treamtent or control group, this  method generates the null distribution by reestimating the event study with the intervention set to n splits at even intervals within the total study duration.\n\nNote that lowering the number of iterations increases the probability of failing to reject the null hypothesis.\n\nFor a primer on randomization inference see:      https://www.mattblackwell.org/files/teaching/s05-fisher.pdf\n\nExamples\n\njulia> x₀, y₀, x₁, y₁ = rand(1:100, 100, 5), rand(100), rand(10, 5), rand(10)\njulia> event_study = EventStudy(x₀, y₀, x₁, y₁)\njulia> estimatecausaleffect!(event_study)\njulia> generatenulldistribution(event_study, 10)\n[-0.5012456678829079, -0.33790650529972194, -0.2534340182760628, -0.21030239864895905, \n-0.11672915615117885, -0.08149441936166794, -0.0685134758182695, -0.06217013151235991, \n-0.05905529159312335, -0.04927743270606937]\n\n\n\n\n\n","category":"function"},{"location":"contributing/features/#Requesting-New-Features","page":"Requesting New Features","title":"Requesting New Features","text":"","category":"section"},{"location":"contributing/features/","page":"Requesting New Features","title":"Requesting New Features","text":"Before requesting a new feature, please check the issues page on GitHub to make sure someone else did not already request the same feature. If this is not the case, then please open an issue that explains what function or method you would like to be added and how you beleive  it should behave.","category":"page"},{"location":"guide/event_study/#Event-Study-Estimation","page":"Event Study Estimation","title":"Event Study Estimation","text":"","category":"section"},{"location":"guide/event_study/","page":"Event Study Estimation","title":"Event Study Estimation","text":"Sometimes we want to know how an outcome variable for a single unit changed after an event  or intervention. For example, if regulators announce sanctions against company A, we might  want to know how the price of stock A changed after the announcement. Since we do not know what the price of Company A's stock would have been if the santions were not announced, we need some way to predict those values. An event study does this by using some covariates  that are related to the oucome variable but not related to whether the event happened to  predict what would have happened. The abnormal returns are just the difference or mean  difference between the predicted post-event outcomes and the actual post-event outcomes.  Estimating an event study in CausalELM consists of three steps.","category":"page"},{"location":"guide/event_study/#Generate-Data","page":"Event Study Estimation","title":"Generate Data","text":"","category":"section"},{"location":"guide/event_study/","page":"Event Study Estimation","title":"Event Study Estimation","text":"X₀, Y₀, X₁, Y₁ =  rand(1000, 5), rand(1000), rand(100, 5), rand(100)","category":"page"},{"location":"guide/event_study/#Step-1:-Initialize-an-event-study-estimator","page":"Event Study Estimation","title":"Step 1: Initialize an event study estimator","text":"","category":"section"},{"location":"guide/event_study/","page":"Event Study Estimation","title":"Event Study Estimation","text":"The EventStudy method takes four agruments: an array of pre-event covariates, a vector of  pre-event outcomes, an array of post-event covariates, and a vector of post-event outcomes.","category":"page"},{"location":"guide/event_study/","page":"Event Study Estimation","title":"Event Study Estimation","text":"m1 = EventStudy(X₀, Y₀, X₁, Y₁)","category":"page"},{"location":"guide/event_study/#Step-2:-Estimate-the-Treatment-Effect","page":"Event Study Estimation","title":"Step 2: Estimate the Treatment Effect","text":"","category":"section"},{"location":"guide/event_study/","page":"Event Study Estimation","title":"Event Study Estimation","text":"Estimating the treatment effect only requires one argument: an EventStudy struct.","category":"page"},{"location":"guide/event_study/","page":"Event Study Estimation","title":"Event Study Estimation","text":"# We can also estimate the ATT by passing quantity_of_interest=\"ATT\"\nestimatecausaleffect!(m1)","category":"page"},{"location":"guide/event_study/#Step-3:-Get-a-Summary","page":"Event Study Estimation","title":"Step 3: Get a Summary","text":"","category":"section"},{"location":"guide/event_study/","page":"Event Study Estimation","title":"Event Study Estimation","text":"We can get a summary of the model, including a p-value and statndard via asymptotic  randomization inference, by pasing the model to the summarize method.","category":"page"},{"location":"guide/event_study/","page":"Event Study Estimation","title":"Event Study Estimation","text":"summarize(m1)","category":"page"},{"location":"reference/activations/#Activation-Functions","page":"Activation Functions","title":"Activation Functions","text":"","category":"section"},{"location":"reference/activations/","page":"Activation Functions","title":"Activation Functions","text":"Activation functions for CausalELM estimators","category":"page"},{"location":"reference/activations/","page":"Activation Functions","title":"Activation Functions","text":"CausalELM.ActivationFunctions\nCausalELM.ActivationFunctions.binarystep\nCausalELM.ActivationFunctions.σ\nCausalELM.ActivationFunctions.tanh\nCausalELM.ActivationFunctions.relu\nCausalELM.ActivationFunctions.leakyrelu\nCausalELM.ActivationFunctions.swish\nCausalELM.ActivationFunctions.softmax\nCausalELM.ActivationFunctions.softplus\nCausalELM.ActivationFunctions.gelu\nCausalELM.ActivationFunctions.gaussian\nCausalELM.ActivationFunctions.hardtanh\nCausalELM.ActivationFunctions.elish\nCausalELM.ActivationFunctions.fourier","category":"page"},{"location":"reference/activations/#CausalELM.ActivationFunctions","page":"Activation Functions","title":"CausalELM.ActivationFunctions","text":"Activation functions for Extreme Learning machines\n\n\n\n\n\n","category":"module"},{"location":"reference/activations/#CausalELM.ActivationFunctions.binarystep","page":"Activation Functions","title":"CausalELM.ActivationFunctions.binarystep","text":"binarystep(x)\n\nApply the binary step activation function to a real number.\n\nExamples\n\njulia> binarystep(1)\n1\n\n\n\n\n\nbinarystep(x)\n\nApply the binary step activation function to an array.\n\nExamples\n\njulia> binarystep([-1000, 100, 1, 0, -0.001, -3])\n[0, 1, 1, 1, 0, 0]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.σ","page":"Activation Functions","title":"CausalELM.ActivationFunctions.σ","text":"σ(x)\n\nApply the sigmoid activation function to a real number.\n\nExamples\n\njulia> σ(1)\n0.7310585786300049\n\n\n\n\n\nσ(x)\n\nApply the sigmoid activation function to an array.\n\nExamples\n\njulia> σ([1, 0])\n[0.7310585786300049, 0.5]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.tanh","page":"Activation Functions","title":"CausalELM.ActivationFunctions.tanh","text":"tanh(x)\n\nApply the tanh activation function to an array.\n\nThis is just a vectorized version of Base.tanh\n\nExamples\n\njulia> tanh([1, 0])\n[0.7615941559557649, 0.0]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.relu","page":"Activation Functions","title":"CausalELM.ActivationFunctions.relu","text":"relu(x)\n\nApply the ReLU activation function to a real number.\n\nExamples\n\njulia> relu(1)\n1\n\n\n\n\n\nrelu(x)\n\nApply the ReLU activation function to an array.\n\nExamples\n\njulia> relu([1, 0, -1])\n[1, 0, 0]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.leakyrelu","page":"Activation Functions","title":"CausalELM.ActivationFunctions.leakyrelu","text":"leakyrelu(x)\n\nApply the leaky ReLU activation function to a real number.\n\nExamples\n\njulia> leakyrelu(1)\n1\n\n\n\n\n\nleakyrelu(x)\n\nApply the leaky ReLU activation function to an array.\n\nExamples\n\njulia> leakyrelu([-0.01, 0, 1])\n[1, 0, 0]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.swish","page":"Activation Functions","title":"CausalELM.ActivationFunctions.swish","text":"swish(x)\n\nApply the swish activation function to a real number.\n\nExamples\n\njulia> swish(1)\n0.7310585786300049\n\n\n\n\n\nswish(x)\n\nApply the swish activation function to an array.\n\nExamples\n\njulia> swish([1, 0, -1])\n[0.7310585786300049, 0, -0.2689414213699951]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.softmax","page":"Activation Functions","title":"CausalELM.ActivationFunctions.softmax","text":"softmax(x)\n\nApply the softmax activation function to a real number.\n\nFor numbers that have large absolute values this function may become numerically unstable.\n\nExamples\n\njulia> softmax(1)\n2.718281828459045\n\n\n\n\n\nsoftmax(x)\n\nApply the softmax activation function to an array.\n\nFor numbers that have large absolute values this function might be numerically unstable.\n\nExamples\n\njulia> softmax([1, -1])\n[2.718281828459045, -0.36787944117144233]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.softplus","page":"Activation Functions","title":"CausalELM.ActivationFunctions.softplus","text":"softplus(x)\n\nApply the softplus activation function to a real number.\n\nExamples\n\njulia> softplus(1)\n1.3132616875182228\n\n\n\n\n\nsoftplus(x)\n\nApply the softplus activation function to an array.\n\nExamples\n\njulia> softplus([1, -1])\n[1.3132616875182228, 0.31326168751822286]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.gelu","page":"Activation Functions","title":"CausalELM.ActivationFunctions.gelu","text":"gelu(x)\n\nApply the GeLU activation function to a real number.\n\nExamples\n\njulia> gelu(1)\n0.8411919906082768\n\n\n\n\n\ngelu(x)\n\nApply the GeLU activation function to an array.\n\nExamples\n\njulia> gelu([-1, 0, 1])\n[-0.15880800939172324, 0, 0.8411919906082768]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.gaussian","page":"Activation Functions","title":"CausalELM.ActivationFunctions.gaussian","text":"gaussian(x)\n\nApply the gaussian activation function to a real number.\n\nExamples\n\njulia> gaussian(1)\n0.11443511435028261\n\n\n\n\n\ngaussian(x)\n\nApply the gaussian activation function to an array.\n\nExamples\n\njulia> gaussian([1, -1])\n[0.36787944117144233, 0.36787944117144233]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.hardtanh","page":"Activation Functions","title":"CausalELM.ActivationFunctions.hardtanh","text":"hardtanh(x)\n\nApply the hardtanh activation function to a real number.\n\nExamples\n\njulia> hardtanh(-2)\n-1\n\n\n\n\n\nhardtanh(x)\n\nApply the hardtanh activation function to an array.\n\nExamples\n\njulia> hardtanh([-2, 0, 2])\n[-1, 0, 1]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.elish","page":"Activation Functions","title":"CausalELM.ActivationFunctions.elish","text":"elish(x)\n\nApply the ELiSH activation function to a real number.\n\nExamples\n\njulia> elish(1)\n0.7310585786300049\n\n\n\n\n\nelish(x)\n\nApply the ELiSH activation function to an array.\n\nExamples\n\njulia> elish([-1, 1])\n[-0.17000340156854793, 0.7310585786300049]\n\n\n\n\n\n","category":"function"},{"location":"reference/activations/#CausalELM.ActivationFunctions.fourier","page":"Activation Functions","title":"CausalELM.ActivationFunctions.fourier","text":"fourrier(x)\n\nApply the Fourier activation function to a real number.\n\nExamples\n\njulia> fourier(1)\n0.8414709848078965\n\n\n\n\n\nfourrier(x)\n\nApply the Fourier activation function to an array.\n\nExamples\n\njulia> fourier([-1, 1])\n[-0.8414709848078965, 0.8414709848078965]\n\n\n\n\n\n","category":"function"},{"location":"reference/base/#Base-Models","page":"Base Models","title":"Base Models","text":"","category":"section"},{"location":"reference/base/","page":"Base Models","title":"Base Models","text":"Extreme learning machines and L2 regularized extreme learning machines for CausalELM estimators","category":"page"},{"location":"reference/base/","page":"Base Models","title":"Base Models","text":"CausalELM.Models\nCausalELM.Models.ExtremeLearningMachine\nCausalELM.Models.ExtremeLearner\nCausalELM.Models.RegularizedExtremeLearner\nCausalELM.Models.fit!\nCausalELM.Models.predict\nCausalELM.Models.predictcounterfactual!\nCausalELM.Models.placebotest","category":"page"},{"location":"reference/base/#CausalELM.Models","page":"Base Models","title":"CausalELM.Models","text":"Base models to perform extreme learning with and without L2 penalization.\n\nFor details on Extreme learning machines see;     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nFor details on Extreme learning machines with an L2 penalty see:     Li, Guoqiang, and Peifeng Niu. \"An enhanced extreme learning machine based on ridge      regression for regression.\" Neural Computing and Applications 22, no. 3 (2013):      803-810.\n\n\n\n\n\n","category":"module"},{"location":"reference/base/#CausalELM.Models.ExtremeLearningMachine","page":"Base Models","title":"CausalELM.Models.ExtremeLearningMachine","text":"Abstract type that includes vanilla and L2 regularized Extreme Learning Machines\n\n\n\n\n\n","category":"type"},{"location":"reference/base/#CausalELM.Models.ExtremeLearner","page":"Base Models","title":"CausalELM.Models.ExtremeLearner","text":"Struct to hold data for an Extreme Learning machine\n\n\n\n\n\n","category":"type"},{"location":"reference/base/#CausalELM.Models.RegularizedExtremeLearner","page":"Base Models","title":"CausalELM.Models.RegularizedExtremeLearner","text":"Struct to hold data for a regularized Extreme Learning Machine\n\n\n\n\n\n","category":"type"},{"location":"reference/base/#CausalELM.Models.fit!","page":"Base Models","title":"CausalELM.Models.fit!","text":"fit!(model)\n\nMake predictions with an ExtremeLearner.\n\nFor more details see:      Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit!(m1)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]\n\n\n\n\n\nfit!(model)\n\nFit a Regularized Extreme Learner.\n\nFor more details see:      Li, Guoqiang, and Peifeng Niu. \"An enhanced extreme learning machine based on ridge      regression for regression.\" Neural Computing and Applications 22, no. 3 (2013):      803-810.\n\nExamples julia-repl julia> m1 = RegularizedExtremeLearner(x, y, 10, σ)  Regularized Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit!(m1)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]\n\n\n\n\n\n","category":"function"},{"location":"reference/base/#CausalELM.Models.predict","page":"Base Models","title":"CausalELM.Models.predict","text":"predict(model, X)\n\nUse an ExtremeLearningMachine to make predictions.\n\nFor more details see:      Huang G-B, Zhu Q-Y, Siew C. Extreme learning machine: theory and applications.      Neurocomputing. 2006;70:489–501. https://doi.org/10.1016/j.neucom.2005.12.126\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit(m1, sigmoid)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]  julia> predict(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])  [9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978]\n\n\n\n\n\n","category":"function"},{"location":"reference/base/#CausalELM.Models.predictcounterfactual!","page":"Base Models","title":"CausalELM.Models.predictcounterfactual!","text":"predictcounterfactual(model, X)\n\nUse an ExtremeLearningMachine to predict the counterfactual.\n\nThis should be run with the observed covariates. To use synthtic data for what-if      scenarios use predict.\n\nSee also predict.\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit(m1, sigmoid)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]  julia> predictcounterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])  [9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978]\n\n\n\n\n\n","category":"function"},{"location":"reference/base/#CausalELM.Models.placebotest","page":"Base Models","title":"CausalELM.Models.placebotest","text":"placebotest(model)\n\nConduct a placebo test.\n\nThis method makes predictions for the post-event or post-treatment period using data  in the pre-event or pre-treatment period and the post-event or post-treament. If there is a statistically significant difference between these predictions the study design may be flawed. Due to the multitude of significance tests for time series data, this function returns the predictions but does not test for statistical significance.\n\nExamples julia-repl julia> m1 = ExtremeLearner(x, y, 10, σ)  Extreme Learning Machine with 10 hidden neurons  julia> f1 = fit(m1, sigmoid)  [-4.403356409043448, -5.577616954029608, -2.1732800642523595, 0.9669137012255704,   -3.6474913410560013, -4.206228346376102, -7.575391282978456, 4.528774205936467,   -2.4741301876094655, 40.642730531608635, -11.058942121275233]  julia> predictcounterfactual(m1, [1.0 1.0; 0.0 1.0; 0.0 0.0; 1.0 0.0])  [9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978]  julia> placebotest(m1)  ([9.811656638113011e-16, 0.9999999999999962, -9.020553785284482e-17, 0.9999999999999978],  [0.5, 0.4, 0.3, 0.2])\n\n\n\n\n\n","category":"function"},{"location":"contributing/bug/#Reporting-a-Bug","page":"Reporting a Bug","title":"Reporting a Bug","text":"","category":"section"},{"location":"contributing/bug/","page":"Reporting a Bug","title":"Reporting a Bug","text":"To report a bug, open an issue on the CausalELM.jl GitHub page. Please include all relevant  infomration, such as what methods were called, the operating system used, the verions of  CausalELM used, the verion or Julia used, any tracebakcs or error codes, and any other  relevant information.","category":"page"},{"location":"guide/doublyrobust/#Doubly-Robust-Estimation","page":"Doubly Robust Estimation","title":"Doubly Robust Estimation","text":"","category":"section"},{"location":"guide/doublyrobust/","page":"Doubly Robust Estimation","title":"Doubly Robust Estimation","text":"Doubly robust estimation estimates separate models for the treatment and outcome variables  and weights the outcome estimates by the treatment estimates. The advantage of doing this is that only one of the models has to be specified correctly to produce an unbiased estimate of  the causal effect. The main steps for using doubly robust estimation in CausalELM are below.","category":"page"},{"location":"guide/doublyrobust/#Generate-Data","page":"Doubly Robust Estimation","title":"Generate Data","text":"","category":"section"},{"location":"guide/doublyrobust/","page":"Doubly Robust Estimation","title":"Doubly Robust Estimation","text":"# Create some data with a binary treatment\nX, Xₚ, Y, T =  rand(100, 5), rand(100, 4), rand(100), [rand()<0.4 for i in 1:100]","category":"page"},{"location":"guide/doublyrobust/##-Step-1:-Initialize-a-Model","page":"Doubly Robust Estimation","title":"# Step 1: Initialize a Model","text":"","category":"section"},{"location":"guide/doublyrobust/","page":"Doubly Robust Estimation","title":"Doubly Robust Estimation","text":"The DoublyRobust constructor takes four arguments, an array of covariates for the outcome  model, an array of covariates for the treatment model, a vector of outcomes, and a vector of  treatment statuses.","category":"page"},{"location":"guide/doublyrobust/","page":"Doubly Robust Estimation","title":"Doubly Robust Estimation","text":"m1 = DoublyRobust(X, Xₚ, Y, T)","category":"page"},{"location":"guide/doublyrobust/#Step-2:-Estimate-the-Causal-Effect","page":"Doubly Robust Estimation","title":"Step 2: Estimate the Causal Effect","text":"","category":"section"},{"location":"guide/doublyrobust/","page":"Doubly Robust Estimation","title":"Doubly Robust Estimation","text":"To estimate the causal effect, we call estimatecausaleffect! on the model above.","category":"page"},{"location":"guide/doublyrobust/","page":"Doubly Robust Estimation","title":"Doubly Robust Estimation","text":"# we could also estimate the ATT by passing quantity_of_interest=\"ATT\"\nestimatecausaleffect!(m1)","category":"page"},{"location":"guide/doublyrobust/#Get-a-Summary","page":"Doubly Robust Estimation","title":"Get a Summary","text":"","category":"section"},{"location":"guide/doublyrobust/","page":"Doubly Robust Estimation","title":"Doubly Robust Estimation","text":"We can get a summary that includes a p-value and standard error estimated via asymptotic  randomization inference by passing our model to the summarize method.","category":"page"},{"location":"guide/doublyrobust/","page":"Doubly Robust Estimation","title":"Doubly Robust Estimation","text":"summarize(m1)","category":"page"},{"location":"reference/api/#CausalELM","page":"CausalELM","title":"CausalELM","text":"","category":"section"},{"location":"reference/api/","page":"CausalELM","title":"CausalELM","text":"CausalELM","category":"page"},{"location":"reference/api/#CausalELM","page":"CausalELM","title":"CausalELM","text":"Macros, functions, and structs for applying Extreme Learning Machines to causal inference tasks where the counterfactual is unavailable or biased and must be predicted. Provides  macros for event study designs, parametric G-computation, doubly robust machine learning, and  metalearners. Additionally, these tasks can be performed with or without L2 penalization and will automatically choose the best number of neurons and L2 penalty. \n\nFor more details on Extreme Learning Machines see:     Huang, Guang-Bin, Qin-Yu Zhu, and Chee-Kheong Siew. \"Extreme learning machine: theory      and applications.\" Neurocomputing 70, no. 1-3 (2006): 489-501.\n\n\n\n\n\n","category":"module"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CurrentModule = CausalELM","category":"page"},{"location":"#Overview","page":"CausalELM","title":"Overview","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM enables Estimation of causal quantities of interest in research designs where a  counterfactual must be predicted and compared to the observed outcomes. More specifically,  CausalELM provides structs and methods to execute event study designs (interupted time  series analysis), G-Computation, and doubly robust estimation as well as estimation of the  CATE via S-Learning, T-Learning, and X-Learning. Once a causal model has beeen estimated,  CausalELM's summarize method provides basic information about the model as well as a p-value  and standard error estimated with approximate randomization inference. In all of these  implementations, CausalELM predicts the counterfactuals using an Extreme Learning Machine  that includes an L2 penalty by default. In this context, ELMs strike a good balance between  prediction accuracy, generalization, ease of implementation, speed, and interpretability. ","category":"page"},{"location":"#Features","page":"CausalELM","title":"Features","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Simple interface enables estimating causal effects in only a few lines of code\nAnalytically derived L2 penalty reduces cross validation time and multicollinearity\nFast automatic cross validation works with longitudinal, panel, and time series data\nIncludes 13 activation functions and allows user-defined activation functions\nSingle interface for continous, binary, and categorical outcome variables\nEstimation of p-values and standard errors via asymptotic randomization inference\nNo dependencies outside of the Julia standard library","category":"page"},{"location":"#Comparison-with-Other-Packages","page":"CausalELM","title":"Comparison with Other Packages","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"Other packages, mainly EconML, DoWhy, and CausalML, have similar funcitonality. Beides being  written in Julia rather than Python, the main differences between CausalELM and these  libraries are:","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM uses extreme learning machines rather than tree-based or deep learners\nCausalELM performs cross validation during training\nCausalELM performs inference via asymptotic randomization inference rather than    bootstrapping\nCausalELM does not require you to instantiate a model and pass it into a separate class    or struct for training\nCausalELM creates train/test splits automatically\nCausalELM does not have external dependencies: all the functions it uses are in the    Julia standard library","category":"page"},{"location":"#Installation","page":"CausalELM","title":"Installation","text":"","category":"section"},{"location":"","page":"CausalELM","title":"CausalELM","text":"CausalELM reuires Julia version 1.7 or greater and can be installed from the REPL as shown  below.","category":"page"},{"location":"","page":"CausalELM","title":"CausalELM","text":"using Pkg\nPkg.add(\"CausalELM\")","category":"page"},{"location":"guide/metalearners/#Metalearners","page":"Metalearners","title":"Metalearners","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"Instead of knowing the average cuasal effect, we might want to know which units benefit and  which units lose by being exposed to a treatment. For example, a cash transfer program might  motivate some people to work harder and incentivize others to work less. Thus, we might want  to know how the cash transfer program affects individuals instead of it average affect on  the population. To do so, we can use metalearners. Depending on the scenario, we may want to  use an S-learner, a T-learner, or an X-learner. The basic steps to use all three  metalearners are below.","category":"page"},{"location":"guide/metalearners/#Generate-Some-data","page":"Metalearners","title":"Generate Some data","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"X, Y, T =  rand(1000, 5), rand(1000), [rand()<0.4 for i in 1:1000]","category":"page"},{"location":"guide/metalearners/#Initialize-a-Metalearner","page":"Metalearners","title":"Initialize a Metalearner","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"S-learners, T-learners, and X-learners all take three arguments: an array of covariates, a  vector of outcomes, and a vector of treatment statuses.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"m1 = SLearner(X, Y, T)\nm2 = TLearner(X, Y, T)\nm3 = XLearner(X, Y, T)","category":"page"},{"location":"guide/metalearners/#Estimate-the-CATE","page":"Metalearners","title":"Estimate the CATE","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can estimate the CATE for all the models by passing them to estimatecausaleffect!.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"estimatecausaleffect!(m1)\nestimatecausaleffect!(m2)\nestimatecausaleffect!(m3)","category":"page"},{"location":"guide/metalearners/#Get-a-Summary","page":"Metalearners","title":"Get a Summary","text":"","category":"section"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"We can get a summary of the models that includes p0values and standard errors for the  average treatment effect by passing the models to the summarize method.","category":"page"},{"location":"guide/metalearners/","page":"Metalearners","title":"Metalearners","text":"summarize(m1)","category":"page"}]
}
